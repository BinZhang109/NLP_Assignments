{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/aymericdamien/TensorFlow-Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hell World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Simple hello world using TensorFlow\n",
    "\n",
    "# Create a Constant op\n",
    "# The op is added as a node to the default graph.\n",
    "#\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "\n",
    "hello = tf.constant('Hello, TensorFlow!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tf session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "# Run graph\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic constant operations\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 2 b: 3\n",
      "Addition with constants: 5\n",
      "Multiplication with constants: 6\n"
     ]
    }
   ],
   "source": [
    "# Launch the default graph.\n",
    "with tf.Session() as sess:\n",
    "    print(\"a: %i\" % sess.run(a), \"b: %i\" % sess.run(b))\n",
    "    print(\"Addition with constants: %i\" % sess.run(a+b))\n",
    "    print(\"Multiplication with constants: %i\" % sess.run(a*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Operations with variable as graph input\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Variable op. (define as input when running session)\n",
    "# tf Graph input\n",
    "a = tf.placeholder(tf.int16)\n",
    "b = tf.placeholder(tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some operations\n",
    "add = tf.add(a, b)\n",
    "mul = tf.multiply(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition with variables: 5\n",
      "Multiplication with variables: 6\n"
     ]
    }
   ],
   "source": [
    "# Launch the default graph.\n",
    "with tf.Session() as sess:\n",
    "    # Run every operation with variable input\n",
    "    print(\"Addition with variables: %i\" % sess.run(add, feed_dict={a: 2, b: 3}))\n",
    "    print(\"Multiplication with variables: %i\" % sess.run(mul, feed_dict={a: 2, b: 3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# More in details:\n",
    "# Matrix Multiplication from TensorFlow official tutorial\n",
    "\n",
    "# Create a Constant op that produces a 1x2 matrix.  The op is\n",
    "# added as a node to the default graph.\n",
    "#\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "matrix1 = tf.constant([[3., 3.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another Constant that produces a 2x1 matrix.\n",
    "matrix2 = tf.constant([[2.],[2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n",
    "# The returned value, 'product', represents the result of the matrix\n",
    "# multiplication.\n",
    "product = tf.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.]]\n"
     ]
    }
   ],
   "source": [
    "# To run the matmul op we call the session 'run()' method, passing 'product'\n",
    "# which represents the output of the matmul op.  This indicates to the call\n",
    "# that we want to get the output of the matmul op back.\n",
    "#\n",
    "# All inputs needed by the op are run automatically by the session.  They\n",
    "# typically are run in parallel.\n",
    "#\n",
    "# The call 'run(product)' thus causes the execution of threes ops in the\n",
    "# graph: the two constants and matmul.\n",
    "#\n",
    "# The output of the op is returned in 'result' as a numpy `ndarray` object.\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(product)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Eager API basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Eager mode...\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set Eager API\n",
    "print(\"Setting Eager mode...\")\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define constant tensors\n",
      "a = 2\n",
      "b = 3\n"
     ]
    }
   ],
   "source": [
    "# Define constant tensors\n",
    "print(\"Define constant tensors\")\n",
    "a = tf.constant(2)\n",
    "print(\"a = %i\" % a)\n",
    "b = tf.constant(3)\n",
    "print(\"b = %i\" % b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running operations, without tf.Session\n",
      "a + b = 5\n",
      "a * b = 6\n"
     ]
    }
   ],
   "source": [
    "# Run the operation without the need for tf.Session\n",
    "print(\"Running operations, without tf.Session\")\n",
    "c = a + b\n",
    "print(\"a + b = %i\" % c)\n",
    "d = a * b\n",
    "print(\"a * b = %i\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixing operations with Tensors and Numpy Arrays\n",
      "Tensor:\n",
      " a = tf.Tensor(\n",
      "[[2. 1.]\n",
      " [1. 0.]], shape=(2, 2), dtype=float32)\n",
      "NumpyArray:\n",
      " b = [[3. 0.]\n",
      " [5. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Full compatibility with Numpy\n",
    "print(\"Mixing operations with Tensors and Numpy Arrays\")\n",
    "\n",
    "# Define constant tensors\n",
    "a = tf.constant([[2., 1.],\n",
    "                 [1., 0.]], dtype=tf.float32)\n",
    "print(\"Tensor:\\n a = %s\" % a)\n",
    "b = np.array([[3., 0.],\n",
    "              [5., 1.]], dtype=np.float32)\n",
    "print(\"NumpyArray:\\n b = %s\" % b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running operations, without tf.Session\n",
      "a + b = tf.Tensor(\n",
      "[[5. 1.]\n",
      " [6. 1.]], shape=(2, 2), dtype=float32)\n",
      "a * b = tf.Tensor(\n",
      "[[11.  1.]\n",
      " [ 3.  0.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Run the operation without the need for tf.Session\n",
    "print(\"Running operations, without tf.Session\")\n",
    "\n",
    "c = a + b\n",
    "print(\"a + b = %s\" % c)\n",
    "\n",
    "d = tf.matmul(a, b)\n",
    "print(\"a * b = %s\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterate through Tensor 'a':\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Iterate through Tensor 'a':\")\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(a.shape[1]):\n",
    "        print(a[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "rng = numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1000\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(rng.randn(), name=\"weight\")\n",
    "b = tf.Variable(rng.randn(), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a linear model\n",
    "pred = tf.add(tf.multiply(X, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ryan Wu\\Anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Mean squared error\n",
    "cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n",
    "# Gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= 0.106208548 W= 0.34528488 b= 0.113070354\n",
      "Epoch: 0100 cost= 0.102828495 W= 0.33959362 b= 0.15401246\n",
      "Epoch: 0150 cost= 0.099838980 W= 0.33424088 b= 0.19251956\n",
      "Epoch: 0200 cost= 0.097194940 W= 0.3292066 b= 0.22873661\n",
      "Epoch: 0250 cost= 0.094856493 W= 0.3244716 b= 0.26279962\n",
      "Epoch: 0300 cost= 0.092788309 W= 0.32001814 b= 0.29483697\n",
      "Epoch: 0350 cost= 0.090959176 W= 0.31582972 b= 0.32496876\n",
      "Epoch: 0400 cost= 0.089341559 W= 0.31189027 b= 0.35330805\n",
      "Epoch: 0450 cost= 0.087910898 W= 0.3081852 b= 0.37996247\n",
      "Epoch: 0500 cost= 0.086645663 W= 0.30470034 b= 0.4050318\n",
      "Epoch: 0550 cost= 0.085526802 W= 0.30142298 b= 0.4286092\n",
      "Epoch: 0600 cost= 0.084537305 W= 0.29834047 b= 0.45078474\n",
      "Epoch: 0650 cost= 0.083662257 W= 0.29544124 b= 0.47164178\n",
      "Epoch: 0700 cost= 0.082888469 W= 0.2927144 b= 0.49125788\n",
      "Epoch: 0750 cost= 0.082204200 W= 0.29014996 b= 0.5097074\n",
      "Epoch: 0800 cost= 0.081599116 W= 0.28773782 b= 0.5270596\n",
      "Epoch: 0850 cost= 0.081064075 W= 0.28546917 b= 0.54338\n",
      "Epoch: 0900 cost= 0.080590963 W= 0.2833355 b= 0.55873\n",
      "Epoch: 0950 cost= 0.080172613 W= 0.2813285 b= 0.57316774\n",
      "Epoch: 1000 cost= 0.079802744 W= 0.27944106 b= 0.5867459\n",
      "Optimization Finished!\n",
      "Training cost= 0.079802744 W= 0.27944106 b= 0.5867459 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (x, y) in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n",
    "                \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
    "    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n",
    "\n",
    "    #Graphic display\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression (eager api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set Eager API\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "train_X = [3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n",
    "           7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1]\n",
    "train_Y = [1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53, 1.221,\n",
    "           2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3]\n",
    "n_samples = len(train_X)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "display_step = 100\n",
    "num_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight and Bias\n",
    "W = tfe.Variable(np.random.randn())\n",
    "b = tfe.Variable(np.random.randn())\n",
    "\n",
    "# Linear regression (Wx + b)\n",
    "def linear_regression(inputs):\n",
    "    return inputs * W + b\n",
    "\n",
    "# Mean square error\n",
    "def mean_square_fn(model_fn, inputs, labels):\n",
    "    return tf.reduce_sum(tf.pow(model_fn(inputs) - labels, 2)) / (2 * n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# Compute gradients\n",
    "grad = tfe.implicit_gradients(mean_square_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial cost= 22.056007385 W= -0.6179227 b= -0.094497114\n",
      "WARNING:tensorflow:From C:\\Users\\Ryan Wu\\Anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch: 0001 cost= 6.711404800 W= -0.17721999 b= -0.031399384\n",
      "Epoch: 0100 cost= 0.104648262 W= 0.3460262 b= 0.12960956\n",
      "Epoch: 0200 cost= 0.098670289 W= 0.33523065 b= 0.20614527\n",
      "Epoch: 0300 cost= 0.093981504 W= 0.32566977 b= 0.27392757\n",
      "Epoch: 0400 cost= 0.090303950 W= 0.31720236 b= 0.33395764\n",
      "Epoch: 0500 cost= 0.087419450 W= 0.30970338 b= 0.3871221\n",
      "Epoch: 0600 cost= 0.085157029 W= 0.30306205 b= 0.43420607\n",
      "Epoch: 0700 cost= 0.083382547 W= 0.2971803 b= 0.47590506\n",
      "Epoch: 0800 cost= 0.081990749 W= 0.29197127 b= 0.5128348\n",
      "Epoch: 0900 cost= 0.080899082 W= 0.28735802 b= 0.54554075\n",
      "Epoch: 1000 cost= 0.080042869 W= 0.28327236 b= 0.57450616\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initial cost, before optimizing\n",
    "print(\"Initial cost= {:.9f}\".format(\n",
    "    mean_square_fn(linear_regression, train_X, train_Y)),\n",
    "    \"W=\", W.numpy(), \"b=\", b.numpy())\n",
    "\n",
    "# Training\n",
    "for step in range(num_steps):\n",
    "\n",
    "    optimizer.apply_gradients(grad(linear_regression, train_X, train_Y))\n",
    "\n",
    "    if (step + 1) % display_step == 0 or step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (step + 1), \"cost=\",\n",
    "              \"{:.9f}\".format(mean_square_fn(linear_regression, train_X, train_Y)),\n",
    "              \"W=\", W.numpy(), \"b=\", b.numpy())\n",
    "\n",
    "# Graphic display\n",
    "plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "plt.plot(train_X, np.array(W * train_X + b), label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tf.argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we limit mnist data\n",
    "Xtr, Ytr = mnist.train.next_batch(5000) #5000 for training (nn candidates)\n",
    "Xte, Yte = mnist.test.next_batch(200) #200 for testing\n",
    "\n",
    "# tf Graph Input\n",
    "xtr = tf.placeholder(\"float\", [None, 784])\n",
    "# tf.placeholder(dtype, shape=None, name=None)\n",
    "# 784是图片的像素28 * 28\n",
    "xte = tf.placeholder(\"float\", [784])\n",
    "\n",
    "# Nearest Neighbor calculation using L1 Distance\n",
    "# Calculate L1 Distance\n",
    "# 按行求和\n",
    "distance = tf.reduce_sum(tf.abs(tf.add(xtr, tf.negative(xte))), reduction_indices=1)\n",
    "# Prediction: Get min distance index (Nearest neighbor)\n",
    "pred = tf.argmin(distance, 0)# 返回矩阵横列或者纵列的最小值的坐标，取决于第二个参数 \n",
    "\n",
    "accuracy = 0.\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0 Prediction: 8 True Class: 8\n",
      "Test 1 Prediction: 7 True Class: 7\n",
      "Test 2 Prediction: 0 True Class: 0\n",
      "Test 3 Prediction: 0 True Class: 0\n",
      "Test 4 Prediction: 0 True Class: 0\n",
      "Test 5 Prediction: 3 True Class: 3\n",
      "Test 6 Prediction: 0 True Class: 8\n",
      "Test 7 Prediction: 7 True Class: 7\n",
      "Test 8 Prediction: 7 True Class: 7\n",
      "Test 9 Prediction: 7 True Class: 7\n",
      "Test 10 Prediction: 0 True Class: 0\n",
      "Test 11 Prediction: 0 True Class: 0\n",
      "Test 12 Prediction: 2 True Class: 2\n",
      "Test 13 Prediction: 4 True Class: 4\n",
      "Test 14 Prediction: 8 True Class: 8\n",
      "Test 15 Prediction: 4 True Class: 4\n",
      "Test 16 Prediction: 5 True Class: 5\n",
      "Test 17 Prediction: 9 True Class: 4\n",
      "Test 18 Prediction: 6 True Class: 6\n",
      "Test 19 Prediction: 7 True Class: 7\n",
      "Test 20 Prediction: 6 True Class: 6\n",
      "Test 21 Prediction: 1 True Class: 1\n",
      "Test 22 Prediction: 0 True Class: 0\n",
      "Test 23 Prediction: 8 True Class: 8\n",
      "Test 24 Prediction: 9 True Class: 9\n",
      "Test 25 Prediction: 5 True Class: 5\n",
      "Test 26 Prediction: 0 True Class: 0\n",
      "Test 27 Prediction: 4 True Class: 4\n",
      "Test 28 Prediction: 2 True Class: 2\n",
      "Test 29 Prediction: 1 True Class: 3\n",
      "Test 30 Prediction: 0 True Class: 0\n",
      "Test 31 Prediction: 6 True Class: 6\n",
      "Test 32 Prediction: 9 True Class: 9\n",
      "Test 33 Prediction: 1 True Class: 1\n",
      "Test 34 Prediction: 7 True Class: 7\n",
      "Test 35 Prediction: 2 True Class: 2\n",
      "Test 36 Prediction: 1 True Class: 1\n",
      "Test 37 Prediction: 5 True Class: 5\n",
      "Test 38 Prediction: 3 True Class: 3\n",
      "Test 39 Prediction: 4 True Class: 4\n",
      "Test 40 Prediction: 3 True Class: 3\n",
      "Test 41 Prediction: 9 True Class: 9\n",
      "Test 42 Prediction: 6 True Class: 6\n",
      "Test 43 Prediction: 5 True Class: 5\n",
      "Test 44 Prediction: 5 True Class: 5\n",
      "Test 45 Prediction: 3 True Class: 8\n",
      "Test 46 Prediction: 9 True Class: 9\n",
      "Test 47 Prediction: 8 True Class: 8\n",
      "Test 48 Prediction: 4 True Class: 8\n",
      "Test 49 Prediction: 1 True Class: 1\n",
      "Test 50 Prediction: 8 True Class: 8\n",
      "Test 51 Prediction: 0 True Class: 0\n",
      "Test 52 Prediction: 8 True Class: 8\n",
      "Test 53 Prediction: 4 True Class: 4\n",
      "Test 54 Prediction: 9 True Class: 9\n",
      "Test 55 Prediction: 0 True Class: 0\n",
      "Test 56 Prediction: 4 True Class: 4\n",
      "Test 57 Prediction: 7 True Class: 7\n",
      "Test 58 Prediction: 3 True Class: 3\n",
      "Test 59 Prediction: 5 True Class: 5\n",
      "Test 60 Prediction: 6 True Class: 6\n",
      "Test 61 Prediction: 5 True Class: 5\n",
      "Test 62 Prediction: 3 True Class: 3\n",
      "Test 63 Prediction: 7 True Class: 7\n",
      "Test 64 Prediction: 9 True Class: 5\n",
      "Test 65 Prediction: 5 True Class: 8\n",
      "Test 66 Prediction: 3 True Class: 3\n",
      "Test 67 Prediction: 7 True Class: 7\n",
      "Test 68 Prediction: 2 True Class: 2\n",
      "Test 69 Prediction: 7 True Class: 7\n",
      "Test 70 Prediction: 8 True Class: 8\n",
      "Test 71 Prediction: 5 True Class: 5\n",
      "Test 72 Prediction: 3 True Class: 3\n",
      "Test 73 Prediction: 2 True Class: 3\n",
      "Test 74 Prediction: 1 True Class: 1\n",
      "Test 75 Prediction: 1 True Class: 1\n",
      "Test 76 Prediction: 2 True Class: 2\n",
      "Test 77 Prediction: 7 True Class: 7\n",
      "Test 78 Prediction: 8 True Class: 8\n",
      "Test 79 Prediction: 8 True Class: 8\n",
      "Test 80 Prediction: 0 True Class: 0\n",
      "Test 81 Prediction: 7 True Class: 7\n",
      "Test 82 Prediction: 0 True Class: 0\n",
      "Test 83 Prediction: 7 True Class: 7\n",
      "Test 84 Prediction: 9 True Class: 8\n",
      "Test 85 Prediction: 4 True Class: 4\n",
      "Test 86 Prediction: 9 True Class: 9\n",
      "Test 87 Prediction: 0 True Class: 0\n",
      "Test 88 Prediction: 9 True Class: 9\n",
      "Test 89 Prediction: 2 True Class: 2\n",
      "Test 90 Prediction: 9 True Class: 9\n",
      "Test 91 Prediction: 9 True Class: 9\n",
      "Test 92 Prediction: 2 True Class: 2\n",
      "Test 93 Prediction: 2 True Class: 2\n",
      "Test 94 Prediction: 1 True Class: 1\n",
      "Test 95 Prediction: 3 True Class: 3\n",
      "Test 96 Prediction: 8 True Class: 8\n",
      "Test 97 Prediction: 0 True Class: 0\n",
      "Test 98 Prediction: 6 True Class: 6\n",
      "Test 99 Prediction: 5 True Class: 5\n",
      "Test 100 Prediction: 3 True Class: 5\n",
      "Test 101 Prediction: 4 True Class: 4\n",
      "Test 102 Prediction: 0 True Class: 0\n",
      "Test 103 Prediction: 3 True Class: 3\n",
      "Test 104 Prediction: 6 True Class: 6\n",
      "Test 105 Prediction: 3 True Class: 2\n",
      "Test 106 Prediction: 7 True Class: 7\n",
      "Test 107 Prediction: 9 True Class: 9\n",
      "Test 108 Prediction: 7 True Class: 7\n",
      "Test 109 Prediction: 9 True Class: 9\n",
      "Test 110 Prediction: 2 True Class: 2\n",
      "Test 111 Prediction: 1 True Class: 1\n",
      "Test 112 Prediction: 0 True Class: 0\n",
      "Test 113 Prediction: 1 True Class: 1\n",
      "Test 114 Prediction: 5 True Class: 5\n",
      "Test 115 Prediction: 8 True Class: 8\n",
      "Test 116 Prediction: 3 True Class: 3\n",
      "Test 117 Prediction: 0 True Class: 0\n",
      "Test 118 Prediction: 7 True Class: 7\n",
      "Test 119 Prediction: 1 True Class: 1\n",
      "Test 120 Prediction: 1 True Class: 1\n",
      "Test 121 Prediction: 9 True Class: 9\n",
      "Test 122 Prediction: 8 True Class: 8\n",
      "Test 123 Prediction: 2 True Class: 2\n",
      "Test 124 Prediction: 0 True Class: 0\n",
      "Test 125 Prediction: 0 True Class: 0\n",
      "Test 126 Prediction: 8 True Class: 8\n",
      "Test 127 Prediction: 2 True Class: 2\n",
      "Test 128 Prediction: 7 True Class: 7\n",
      "Test 129 Prediction: 6 True Class: 6\n",
      "Test 130 Prediction: 8 True Class: 8\n",
      "Test 131 Prediction: 6 True Class: 6\n",
      "Test 132 Prediction: 5 True Class: 5\n",
      "Test 133 Prediction: 2 True Class: 2\n",
      "Test 134 Prediction: 0 True Class: 0\n",
      "Test 135 Prediction: 1 True Class: 1\n",
      "Test 136 Prediction: 7 True Class: 7\n",
      "Test 137 Prediction: 8 True Class: 8\n",
      "Test 138 Prediction: 0 True Class: 0\n",
      "Test 139 Prediction: 1 True Class: 3\n",
      "Test 140 Prediction: 7 True Class: 7\n",
      "Test 141 Prediction: 3 True Class: 3\n",
      "Test 142 Prediction: 1 True Class: 1\n",
      "Test 143 Prediction: 3 True Class: 3\n",
      "Test 144 Prediction: 4 True Class: 4\n",
      "Test 145 Prediction: 3 True Class: 3\n",
      "Test 146 Prediction: 2 True Class: 2\n",
      "Test 147 Prediction: 8 True Class: 8\n",
      "Test 148 Prediction: 2 True Class: 2\n",
      "Test 149 Prediction: 6 True Class: 6\n",
      "Test 150 Prediction: 2 True Class: 2\n",
      "Test 151 Prediction: 1 True Class: 1\n",
      "Test 152 Prediction: 4 True Class: 4\n",
      "Test 153 Prediction: 5 True Class: 5\n",
      "Test 154 Prediction: 2 True Class: 2\n",
      "Test 155 Prediction: 7 True Class: 7\n",
      "Test 156 Prediction: 4 True Class: 4\n",
      "Test 157 Prediction: 2 True Class: 2\n",
      "Test 158 Prediction: 5 True Class: 5\n",
      "Test 159 Prediction: 2 True Class: 2\n",
      "Test 160 Prediction: 0 True Class: 0\n",
      "Test 161 Prediction: 0 True Class: 0\n",
      "Test 162 Prediction: 1 True Class: 1\n",
      "Test 163 Prediction: 4 True Class: 9\n",
      "Test 164 Prediction: 1 True Class: 1\n",
      "Test 165 Prediction: 2 True Class: 2\n",
      "Test 166 Prediction: 6 True Class: 6\n",
      "Test 167 Prediction: 2 True Class: 1\n",
      "Test 168 Prediction: 2 True Class: 2\n",
      "Test 169 Prediction: 2 True Class: 2\n",
      "Test 170 Prediction: 2 True Class: 2\n",
      "Test 171 Prediction: 1 True Class: 1\n",
      "Test 172 Prediction: 9 True Class: 9\n",
      "Test 173 Prediction: 0 True Class: 0\n",
      "Test 174 Prediction: 3 True Class: 3\n",
      "Test 175 Prediction: 7 True Class: 7\n",
      "Test 176 Prediction: 3 True Class: 3\n",
      "Test 177 Prediction: 9 True Class: 9\n",
      "Test 178 Prediction: 0 True Class: 0\n",
      "Test 179 Prediction: 2 True Class: 2\n",
      "Test 180 Prediction: 3 True Class: 3\n",
      "Test 181 Prediction: 2 True Class: 2\n",
      "Test 182 Prediction: 0 True Class: 0\n",
      "Test 183 Prediction: 2 True Class: 2\n",
      "Test 184 Prediction: 8 True Class: 8\n",
      "Test 185 Prediction: 2 True Class: 2\n",
      "Test 186 Prediction: 5 True Class: 5\n",
      "Test 187 Prediction: 8 True Class: 8\n",
      "Test 188 Prediction: 8 True Class: 8\n",
      "Test 189 Prediction: 4 True Class: 4\n",
      "Test 190 Prediction: 3 True Class: 3\n",
      "Test 191 Prediction: 3 True Class: 3\n",
      "Test 192 Prediction: 2 True Class: 2\n",
      "Test 193 Prediction: 4 True Class: 4\n",
      "Test 194 Prediction: 0 True Class: 0\n",
      "Test 195 Prediction: 8 True Class: 8\n",
      "Test 196 Prediction: 0 True Class: 0\n",
      "Test 197 Prediction: 8 True Class: 8\n",
      "Test 198 Prediction: 5 True Class: 5\n",
      "Test 199 Prediction: 6 True Class: 6\n",
      "Done!\n",
      "Accuracy: 0.9300000000000007\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # loop over test data\n",
    "    for i in range(len(Xte)):\n",
    "        # Get nearest neighbor\n",
    "        nn_index = sess.run(pred, feed_dict={xtr: Xtr, xte: Xte[i, :]})\n",
    "        # Get nearest neighbor class label and compare it to its true label\n",
    "        print(\"Test\", i, \"Prediction:\", np.argmax(Ytr[nn_index]), \\\n",
    "            \"True Class:\", np.argmax(Yte[i]))\n",
    "        # Calculate accuracy\n",
    "        if np.argmax(Ytr[nn_index]) == np.argmax(Yte[i]):\n",
    "            accuracy += 1./len(Xte)\n",
    "    print(\"Done!\")\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.factorization import KMeans\n",
    "\n",
    "# Ignore all GPUs, tf random forest does not benefit from it.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0909 19:59:43.088595  7632 deprecation.py:323] From <ipython-input-5-2c0b2b4ed230>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0909 19:59:43.089621  7632 deprecation.py:323] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0909 19:59:43.090589  7632 deprecation.py:323] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0909 19:59:57.435160  7632 deprecation.py:323] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0909 20:00:00.650134  7632 deprecation.py:323] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0909 20:00:00.655082  7632 deprecation.py:323] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0909 20:00:08.630361  7632 deprecation.py:323] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "full_data_x = mnist.train.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_steps = 50 # Total steps to train\n",
    "batch_size = 1024 # The number of samples per batch\n",
    "k = 50 # The number of clusters\n",
    "num_classes = 10 # The 10 digits\n",
    "num_features = 784 # Each image is 28x28 pixels\n",
    "\n",
    "# Input images\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "# Labels (for assigning a label to a centroid and testing)\n",
    "Y = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
    "\n",
    "# K-Means Parameters\n",
    "kmeans = KMeans(inputs=X, num_clusters=k, distance_metric='cosine',\n",
    "                use_mini_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build KMeans graph\n",
    "(all_scores, cluster_idx, scores, cluster_centers_initialized, \n",
    "init_op,train_op) = kmeans.training_graph()\n",
    "cluster_idx = cluster_idx[0] # fix for cluster_idx being a tuple\n",
    "avg_distance = tf.reduce_mean(scores)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init_vars = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Avg Distance: 0.314557\n",
      "Step 10, Avg Distance: 0.198181\n",
      "Step 20, Avg Distance: 0.196513\n",
      "Step 30, Avg Distance: 0.195821\n",
      "Step 40, Avg Distance: 0.195415\n",
      "Step 50, Avg Distance: 0.195135\n"
     ]
    }
   ],
   "source": [
    "# Start TensorFlow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init_vars, feed_dict={X: full_data_x})\n",
    "sess.run(init_op, feed_dict={X: full_data_x})\n",
    "\n",
    "# Training\n",
    "for i in range(1, num_steps + 1):\n",
    "    _, d, idx = sess.run([train_op, avg_distance, cluster_idx],\n",
    "                         feed_dict={X: full_data_x})\n",
    "    if i % 10 == 0 or i == 1:\n",
    "        print(\"Step %i, Avg Distance: %f\" % (i, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8074\n"
     ]
    }
   ],
   "source": [
    "# Assign a label to each centroid\n",
    "# Count total number of labels per centroid, using the label of each training\n",
    "# sample to their closest centroid (given by 'idx')\n",
    "counts = np.zeros(shape=(k, num_classes))\n",
    "for i in range(len(idx)):\n",
    "    counts[idx[i]] += mnist.train.labels[i]\n",
    "# Assign the most frequent label to the centroid\n",
    "labels_map = [np.argmax(c) for c in counts]\n",
    "labels_map = tf.convert_to_tensor(labels_map)\n",
    "\n",
    "# Evaluation ops\n",
    "# Lookup: centroid_id -> label\n",
    "cluster_label = tf.nn.embedding_lookup(labels_map, cluster_idx)\n",
    "# Compute accuracy\n",
    "correct_prediction = tf.equal(cluster_label, tf.cast(tf.argmax(Y, 1), tf.int32))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Test Model\n",
    "test_x, test_y = mnist.test.images, mnist.test.labels\n",
    "print(\"Test Accuracy:\", sess.run(accuracy_op, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import resources\n",
    "from tensorflow.contrib.tensor_forest.python import tensor_forest\n",
    "\n",
    "# Ignore all GPUs, tf random forest does not benefit from it.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "num_steps = 500 # Total steps to train\n",
    "batch_size = 1024 # The number of samples per batch\n",
    "num_classes = 10 # The 10 digits\n",
    "num_features = 784 # Each image is 28x28 pixels\n",
    "num_trees = 10\n",
    "max_nodes = 1000\n",
    "\n",
    "# Input and Target data\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "# For random forest, labels must be integers (the class id)\n",
    "Y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "# Random Forest Parameters\n",
    "hparams = tensor_forest.ForestHParams(num_classes=num_classes,\n",
    "                                      num_features=num_features,\n",
    "                                      num_trees=num_trees,\n",
    "                                      max_nodes=max_nodes).fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0909 22:18:37.631386  7632 deprecation.py:506] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0909 22:18:37.841825  7632 deprecation.py:323] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\contrib\\tensor_forest\\python\\tensor_forest.py:529: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "# Build the Random Forest\n",
    "forest_graph = tensor_forest.RandomForestGraphs(hparams)\n",
    "# Get training graph and loss\n",
    "train_op = forest_graph.training_graph(X, Y)\n",
    "loss_op = forest_graph.training_loss(X, Y)\n",
    "\n",
    "# Measure the accuracy\n",
    "infer_op, _, _ = forest_graph.inference_graph(X)\n",
    "correct_prediction = tf.equal(tf.argmax(infer_op, 1), tf.cast(Y, tf.int64))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value) and forest resources\n",
    "init_vars = tf.group(tf.global_variables_initializer(),\n",
    "    resources.initialize_resources(resources.shared_resources()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0909 22:20:13.536843  7632 deprecation.py:323] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss: -1.200000, Acc: 0.408203\n",
      "Step 50, Loss: -256.600006, Acc: 0.889648\n",
      "Step 100, Loss: -541.000000, Acc: 0.929688\n",
      "Step 150, Loss: -832.599976, Acc: 0.916992\n",
      "Step 200, Loss: -1001.000000, Acc: 0.926758\n",
      "Step 250, Loss: -1001.000000, Acc: 0.924805\n",
      "Step 300, Loss: -1001.000000, Acc: 0.940430\n",
      "Step 350, Loss: -1001.000000, Acc: 0.931641\n",
      "Step 400, Loss: -1001.000000, Acc: 0.930664\n",
      "Step 450, Loss: -1001.000000, Acc: 0.928711\n",
      "Step 500, Loss: -1001.000000, Acc: 0.923828\n",
      "Test Accuracy: 0.9265\n"
     ]
    }
   ],
   "source": [
    "# Start TensorFlow session\n",
    "sess = tf.train.MonitoredSession()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init_vars)\n",
    "\n",
    "# Training\n",
    "for i in range(1, num_steps + 1):\n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    _, l = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})\n",
    "    if i % 50 == 0 or i == 1:\n",
    "        acc = sess.run(accuracy_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        print('Step %i, Loss: %f, Acc: %f' % (i, l, acc))\n",
    "\n",
    "# Test Model\n",
    "test_x, test_y = mnist.test.images, mnist.test.labels\n",
    "print(\"Test Accuracy:\", sess.run(accuracy_op, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Decision Tree(GBDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeClassifier\n",
    "from tensorflow.contrib.boosted_trees.proto import learner_pb2 as gbdt_learner\n",
    "\n",
    "# Ignore all GPUs (current TF GBDT does not support GPU).\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "# Set verbosity to display errors only (Remove this line for showing warnings)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False,\n",
    "                                  source_url='http://yann.lecun.com/exdb/mnist/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 4096 # The number of samples per batch\n",
    "num_classes = 10 # The 10 digits\n",
    "num_features = 784 # Each image is 28x28 pixels\n",
    "max_steps = 10000\n",
    "\n",
    "# GBDT Parameters\n",
    "learning_rate = 0.1\n",
    "l1_regul = 0.\n",
    "l2_regul = 1.\n",
    "examples_per_layer = 1000\n",
    "num_trees = 10\n",
    "max_depth = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill GBDT parameters into the config proto\n",
    "learner_config = gbdt_learner.LearnerConfig()\n",
    "learner_config.learning_rate_tuner.fixed.learning_rate = learning_rate\n",
    "learner_config.regularization.l1 = l1_regul\n",
    "learner_config.regularization.l2 = l2_regul / examples_per_layer\n",
    "learner_config.constraints.max_tree_depth = max_depth\n",
    "growing_mode = gbdt_learner.LearnerConfig.LAYER_BY_LAYER\n",
    "learner_config.growing_mode = growing_mode\n",
    "run_config = tf.contrib.learn.RunConfig(save_checkpoints_secs=300)\n",
    "learner_config.multi_class_strategy = (\n",
    "    gbdt_learner.LearnerConfig.DIAGONAL_HESSIAN)\n",
    "\n",
    "# Create a TensorFlor GBDT Estimator\n",
    "gbdt_model = GradientBoostedDecisionTreeClassifier(\n",
    "    model_dir=None, # No save directory specified\n",
    "    learner_config=learner_config,\n",
    "    n_classes=num_classes,\n",
    "    examples_per_layer=examples_per_layer,\n",
    "    num_trees=num_trees,\n",
    "    center_bias=False,\n",
    "    config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0909 22:39:38.871496  7632 deprecation.py:323] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0909 22:39:38.873490  7632 deprecation.py:323] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0909 22:39:38.883465  7632 deprecation.py:323] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "I0909 22:39:39.968562  7632 gbdt_batch.py:437] Active Feature Columns: ['images_0', 'images_1', 'images_2', 'images_3', 'images_4', 'images_5', 'images_6', 'images_7', 'images_8', 'images_9', 'images_10', 'images_11', 'images_12', 'images_13', 'images_14', 'images_15', 'images_16', 'images_17', 'images_18', 'images_19', 'images_20', 'images_21', 'images_22', 'images_23', 'images_24', 'images_25', 'images_26', 'images_27', 'images_28', 'images_29', 'images_30', 'images_31', 'images_32', 'images_33', 'images_34', 'images_35', 'images_36', 'images_37', 'images_38', 'images_39', 'images_40', 'images_41', 'images_42', 'images_43', 'images_44', 'images_45', 'images_46', 'images_47', 'images_48', 'images_49', 'images_50', 'images_51', 'images_52', 'images_53', 'images_54', 'images_55', 'images_56', 'images_57', 'images_58', 'images_59', 'images_60', 'images_61', 'images_62', 'images_63', 'images_64', 'images_65', 'images_66', 'images_67', 'images_68', 'images_69', 'images_70', 'images_71', 'images_72', 'images_73', 'images_74', 'images_75', 'images_76', 'images_77', 'images_78', 'images_79', 'images_80', 'images_81', 'images_82', 'images_83', 'images_84', 'images_85', 'images_86', 'images_87', 'images_88', 'images_89', 'images_90', 'images_91', 'images_92', 'images_93', 'images_94', 'images_95', 'images_96', 'images_97', 'images_98', 'images_99', 'images_100', 'images_101', 'images_102', 'images_103', 'images_104', 'images_105', 'images_106', 'images_107', 'images_108', 'images_109', 'images_110', 'images_111', 'images_112', 'images_113', 'images_114', 'images_115', 'images_116', 'images_117', 'images_118', 'images_119', 'images_120', 'images_121', 'images_122', 'images_123', 'images_124', 'images_125', 'images_126', 'images_127', 'images_128', 'images_129', 'images_130', 'images_131', 'images_132', 'images_133', 'images_134', 'images_135', 'images_136', 'images_137', 'images_138', 'images_139', 'images_140', 'images_141', 'images_142', 'images_143', 'images_144', 'images_145', 'images_146', 'images_147', 'images_148', 'images_149', 'images_150', 'images_151', 'images_152', 'images_153', 'images_154', 'images_155', 'images_156', 'images_157', 'images_158', 'images_159', 'images_160', 'images_161', 'images_162', 'images_163', 'images_164', 'images_165', 'images_166', 'images_167', 'images_168', 'images_169', 'images_170', 'images_171', 'images_172', 'images_173', 'images_174', 'images_175', 'images_176', 'images_177', 'images_178', 'images_179', 'images_180', 'images_181', 'images_182', 'images_183', 'images_184', 'images_185', 'images_186', 'images_187', 'images_188', 'images_189', 'images_190', 'images_191', 'images_192', 'images_193', 'images_194', 'images_195', 'images_196', 'images_197', 'images_198', 'images_199', 'images_200', 'images_201', 'images_202', 'images_203', 'images_204', 'images_205', 'images_206', 'images_207', 'images_208', 'images_209', 'images_210', 'images_211', 'images_212', 'images_213', 'images_214', 'images_215', 'images_216', 'images_217', 'images_218', 'images_219', 'images_220', 'images_221', 'images_222', 'images_223', 'images_224', 'images_225', 'images_226', 'images_227', 'images_228', 'images_229', 'images_230', 'images_231', 'images_232', 'images_233', 'images_234', 'images_235', 'images_236', 'images_237', 'images_238', 'images_239', 'images_240', 'images_241', 'images_242', 'images_243', 'images_244', 'images_245', 'images_246', 'images_247', 'images_248', 'images_249', 'images_250', 'images_251', 'images_252', 'images_253', 'images_254', 'images_255', 'images_256', 'images_257', 'images_258', 'images_259', 'images_260', 'images_261', 'images_262', 'images_263', 'images_264', 'images_265', 'images_266', 'images_267', 'images_268', 'images_269', 'images_270', 'images_271', 'images_272', 'images_273', 'images_274', 'images_275', 'images_276', 'images_277', 'images_278', 'images_279', 'images_280', 'images_281', 'images_282', 'images_283', 'images_284', 'images_285', 'images_286', 'images_287', 'images_288', 'images_289', 'images_290', 'images_291', 'images_292', 'images_293', 'images_294', 'images_295', 'images_296', 'images_297', 'images_298', 'images_299', 'images_300', 'images_301', 'images_302', 'images_303', 'images_304', 'images_305', 'images_306', 'images_307', 'images_308', 'images_309', 'images_310', 'images_311', 'images_312', 'images_313', 'images_314', 'images_315', 'images_316', 'images_317', 'images_318', 'images_319', 'images_320', 'images_321', 'images_322', 'images_323', 'images_324', 'images_325', 'images_326', 'images_327', 'images_328', 'images_329', 'images_330', 'images_331', 'images_332', 'images_333', 'images_334', 'images_335', 'images_336', 'images_337', 'images_338', 'images_339', 'images_340', 'images_341', 'images_342', 'images_343', 'images_344', 'images_345', 'images_346', 'images_347', 'images_348', 'images_349', 'images_350', 'images_351', 'images_352', 'images_353', 'images_354', 'images_355', 'images_356', 'images_357', 'images_358', 'images_359', 'images_360', 'images_361', 'images_362', 'images_363', 'images_364', 'images_365', 'images_366', 'images_367', 'images_368', 'images_369', 'images_370', 'images_371', 'images_372', 'images_373', 'images_374', 'images_375', 'images_376', 'images_377', 'images_378', 'images_379', 'images_380', 'images_381', 'images_382', 'images_383', 'images_384', 'images_385', 'images_386', 'images_387', 'images_388', 'images_389', 'images_390', 'images_391', 'images_392', 'images_393', 'images_394', 'images_395', 'images_396', 'images_397', 'images_398', 'images_399', 'images_400', 'images_401', 'images_402', 'images_403', 'images_404', 'images_405', 'images_406', 'images_407', 'images_408', 'images_409', 'images_410', 'images_411', 'images_412', 'images_413', 'images_414', 'images_415', 'images_416', 'images_417', 'images_418', 'images_419', 'images_420', 'images_421', 'images_422', 'images_423', 'images_424', 'images_425', 'images_426', 'images_427', 'images_428', 'images_429', 'images_430', 'images_431', 'images_432', 'images_433', 'images_434', 'images_435', 'images_436', 'images_437', 'images_438', 'images_439', 'images_440', 'images_441', 'images_442', 'images_443', 'images_444', 'images_445', 'images_446', 'images_447', 'images_448', 'images_449', 'images_450', 'images_451', 'images_452', 'images_453', 'images_454', 'images_455', 'images_456', 'images_457', 'images_458', 'images_459', 'images_460', 'images_461', 'images_462', 'images_463', 'images_464', 'images_465', 'images_466', 'images_467', 'images_468', 'images_469', 'images_470', 'images_471', 'images_472', 'images_473', 'images_474', 'images_475', 'images_476', 'images_477', 'images_478', 'images_479', 'images_480', 'images_481', 'images_482', 'images_483', 'images_484', 'images_485', 'images_486', 'images_487', 'images_488', 'images_489', 'images_490', 'images_491', 'images_492', 'images_493', 'images_494', 'images_495', 'images_496', 'images_497', 'images_498', 'images_499', 'images_500', 'images_501', 'images_502', 'images_503', 'images_504', 'images_505', 'images_506', 'images_507', 'images_508', 'images_509', 'images_510', 'images_511', 'images_512', 'images_513', 'images_514', 'images_515', 'images_516', 'images_517', 'images_518', 'images_519', 'images_520', 'images_521', 'images_522', 'images_523', 'images_524', 'images_525', 'images_526', 'images_527', 'images_528', 'images_529', 'images_530', 'images_531', 'images_532', 'images_533', 'images_534', 'images_535', 'images_536', 'images_537', 'images_538', 'images_539', 'images_540', 'images_541', 'images_542', 'images_543', 'images_544', 'images_545', 'images_546', 'images_547', 'images_548', 'images_549', 'images_550', 'images_551', 'images_552', 'images_553', 'images_554', 'images_555', 'images_556', 'images_557', 'images_558', 'images_559', 'images_560', 'images_561', 'images_562', 'images_563', 'images_564', 'images_565', 'images_566', 'images_567', 'images_568', 'images_569', 'images_570', 'images_571', 'images_572', 'images_573', 'images_574', 'images_575', 'images_576', 'images_577', 'images_578', 'images_579', 'images_580', 'images_581', 'images_582', 'images_583', 'images_584', 'images_585', 'images_586', 'images_587', 'images_588', 'images_589', 'images_590', 'images_591', 'images_592', 'images_593', 'images_594', 'images_595', 'images_596', 'images_597', 'images_598', 'images_599', 'images_600', 'images_601', 'images_602', 'images_603', 'images_604', 'images_605', 'images_606', 'images_607', 'images_608', 'images_609', 'images_610', 'images_611', 'images_612', 'images_613', 'images_614', 'images_615', 'images_616', 'images_617', 'images_618', 'images_619', 'images_620', 'images_621', 'images_622', 'images_623', 'images_624', 'images_625', 'images_626', 'images_627', 'images_628', 'images_629', 'images_630', 'images_631', 'images_632', 'images_633', 'images_634', 'images_635', 'images_636', 'images_637', 'images_638', 'images_639', 'images_640', 'images_641', 'images_642', 'images_643', 'images_644', 'images_645', 'images_646', 'images_647', 'images_648', 'images_649', 'images_650', 'images_651', 'images_652', 'images_653', 'images_654', 'images_655', 'images_656', 'images_657', 'images_658', 'images_659', 'images_660', 'images_661', 'images_662', 'images_663', 'images_664', 'images_665', 'images_666', 'images_667', 'images_668', 'images_669', 'images_670', 'images_671', 'images_672', 'images_673', 'images_674', 'images_675', 'images_676', 'images_677', 'images_678', 'images_679', 'images_680', 'images_681', 'images_682', 'images_683', 'images_684', 'images_685', 'images_686', 'images_687', 'images_688', 'images_689', 'images_690', 'images_691', 'images_692', 'images_693', 'images_694', 'images_695', 'images_696', 'images_697', 'images_698', 'images_699', 'images_700', 'images_701', 'images_702', 'images_703', 'images_704', 'images_705', 'images_706', 'images_707', 'images_708', 'images_709', 'images_710', 'images_711', 'images_712', 'images_713', 'images_714', 'images_715', 'images_716', 'images_717', 'images_718', 'images_719', 'images_720', 'images_721', 'images_722', 'images_723', 'images_724', 'images_725', 'images_726', 'images_727', 'images_728', 'images_729', 'images_730', 'images_731', 'images_732', 'images_733', 'images_734', 'images_735', 'images_736', 'images_737', 'images_738', 'images_739', 'images_740', 'images_741', 'images_742', 'images_743', 'images_744', 'images_745', 'images_746', 'images_747', 'images_748', 'images_749', 'images_750', 'images_751', 'images_752', 'images_753', 'images_754', 'images_755', 'images_756', 'images_757', 'images_758', 'images_759', 'images_760', 'images_761', 'images_762', 'images_763', 'images_764', 'images_765', 'images_766', 'images_767', 'images_768', 'images_769', 'images_770', 'images_771', 'images_772', 'images_773', 'images_774', 'images_775', 'images_776', 'images_777', 'images_778', 'images_779', 'images_780', 'images_781', 'images_782', 'images_783']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0909 22:39:39.969560  7632 gbdt_batch.py:438] Learner config: num_classes: 10\n",
      "regularization {\n",
      "  l2: 0.0010000000474974513\n",
      "}\n",
      "constraints {\n",
      "  max_tree_depth: 16\n",
      "}\n",
      "learning_rate_tuner {\n",
      "  fixed {\n",
      "    learning_rate: 0.10000000149011612\n",
      "  }\n",
      "}\n",
      "pruning_mode: POST_PRUNE\n",
      "growing_mode: LAYER_BY_LAYER\n",
      "multi_class_strategy: DIAGONAL_HESSIAN\n",
      "\n",
      "W0909 22:39:56.790055  7632 deprecation.py:323] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:678: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n",
      "I0909 22:39:56.791061  7632 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "W0909 22:39:59.231551  7632 meta_graph.py:449] Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "I0909 22:40:04.212332  7632 monitored_session.py:240] Graph was finalized.\n",
      "I0909 22:40:07.800759  7632 session_manager.py:500] Running local_init_op.\n",
      "I0909 22:40:08.170770  7632 session_manager.py:502] Done running local_init_op.\n",
      "W0909 22:40:09.320778  7632 deprecation.py:323] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0909 22:40:22.988695  7632 meta_graph.py:449] Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "I0909 22:40:24.557026  7632 basic_session_run_hooks.py:606] Saving checkpoints for 0 into C:\\Users\\MR5E8F~1.WU\\AppData\\Local\\Temp\\tmppf1qujhw\\model.ckpt.\n",
      "W0909 22:40:27.350595  7632 meta_graph.py:449] Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "I0909 22:40:50.256408  7632 basic_session_run_hooks.py:262] loss = 2.3025992, step = 1\n",
      "I0909 22:45:29.138280  7632 basic_session_run_hooks.py:606] Saving checkpoints for 81 into C:\\Users\\MR5E8F~1.WU\\AppData\\Local\\Temp\\tmppf1qujhw\\model.ckpt.\n",
      "W0909 22:45:30.336069  7632 meta_graph.py:449] Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "I0909 22:46:30.419008  7632 basic_session_run_hooks.py:692] global_step/sec: 0.293976\n",
      "I0909 22:46:30.421002  7632 basic_session_run_hooks.py:260] loss = 0.3019178, step = 101 (340.164 sec)\n",
      "I0909 22:50:11.313240  7632 trainer_hooks.py:189] Requesting stop since we have reached 10 trees.\n",
      "I0909 22:50:11.317228  7632 basic_session_run_hooks.py:606] Saving checkpoints for 161 into C:\\Users\\MR5E8F~1.WU\\AppData\\Local\\Temp\\tmppf1qujhw\\model.ckpt.\n",
      "W0909 22:50:13.103505  7632 meta_graph.py:449] Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "I0909 22:50:15.179462  7632 estimator.py:525] Loss for final step: 0.22175708.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostedDecisionTreeClassifier(params={'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x000001BCD1E42278>, 'feature_columns': None, 'learner_config': num_classes: 10\n",
       "regularization {\n",
       "  l2: 0.0010000000474974513\n",
       "}\n",
       "constraints {\n",
       "  max_tree_depth: 16\n",
       "}\n",
       "learning_rate_tuner {\n",
       "  fixed {\n",
       "    learning_rate: 0.10000000149011612\n",
       "  }\n",
       "}\n",
       "pruning_mode: POST_PRUNE\n",
       "growing_mode: LAYER_BY_LAYER\n",
       "multi_class_strategy: DIAGONAL_HESSIAN\n",
       ", 'num_trees': 10, 'weight_column_name': None, 'examples_per_layer': 1000, 'center_bias': False, 'logits_modifier_function': None, 'use_core_libs': False, 'output_leaf_index': False, 'override_global_step_value': None, 'num_quantiles': 100})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display TF info logs\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.train.images}, y=mnist.train.labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "\n",
    "# Train the Model\n",
    "gbdt_model.fit(input_fn=input_fn, max_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0909 22:50:16.224694  7632 gbdt_batch.py:437] Active Feature Columns: ['images_0', 'images_1', 'images_2', 'images_3', 'images_4', 'images_5', 'images_6', 'images_7', 'images_8', 'images_9', 'images_10', 'images_11', 'images_12', 'images_13', 'images_14', 'images_15', 'images_16', 'images_17', 'images_18', 'images_19', 'images_20', 'images_21', 'images_22', 'images_23', 'images_24', 'images_25', 'images_26', 'images_27', 'images_28', 'images_29', 'images_30', 'images_31', 'images_32', 'images_33', 'images_34', 'images_35', 'images_36', 'images_37', 'images_38', 'images_39', 'images_40', 'images_41', 'images_42', 'images_43', 'images_44', 'images_45', 'images_46', 'images_47', 'images_48', 'images_49', 'images_50', 'images_51', 'images_52', 'images_53', 'images_54', 'images_55', 'images_56', 'images_57', 'images_58', 'images_59', 'images_60', 'images_61', 'images_62', 'images_63', 'images_64', 'images_65', 'images_66', 'images_67', 'images_68', 'images_69', 'images_70', 'images_71', 'images_72', 'images_73', 'images_74', 'images_75', 'images_76', 'images_77', 'images_78', 'images_79', 'images_80', 'images_81', 'images_82', 'images_83', 'images_84', 'images_85', 'images_86', 'images_87', 'images_88', 'images_89', 'images_90', 'images_91', 'images_92', 'images_93', 'images_94', 'images_95', 'images_96', 'images_97', 'images_98', 'images_99', 'images_100', 'images_101', 'images_102', 'images_103', 'images_104', 'images_105', 'images_106', 'images_107', 'images_108', 'images_109', 'images_110', 'images_111', 'images_112', 'images_113', 'images_114', 'images_115', 'images_116', 'images_117', 'images_118', 'images_119', 'images_120', 'images_121', 'images_122', 'images_123', 'images_124', 'images_125', 'images_126', 'images_127', 'images_128', 'images_129', 'images_130', 'images_131', 'images_132', 'images_133', 'images_134', 'images_135', 'images_136', 'images_137', 'images_138', 'images_139', 'images_140', 'images_141', 'images_142', 'images_143', 'images_144', 'images_145', 'images_146', 'images_147', 'images_148', 'images_149', 'images_150', 'images_151', 'images_152', 'images_153', 'images_154', 'images_155', 'images_156', 'images_157', 'images_158', 'images_159', 'images_160', 'images_161', 'images_162', 'images_163', 'images_164', 'images_165', 'images_166', 'images_167', 'images_168', 'images_169', 'images_170', 'images_171', 'images_172', 'images_173', 'images_174', 'images_175', 'images_176', 'images_177', 'images_178', 'images_179', 'images_180', 'images_181', 'images_182', 'images_183', 'images_184', 'images_185', 'images_186', 'images_187', 'images_188', 'images_189', 'images_190', 'images_191', 'images_192', 'images_193', 'images_194', 'images_195', 'images_196', 'images_197', 'images_198', 'images_199', 'images_200', 'images_201', 'images_202', 'images_203', 'images_204', 'images_205', 'images_206', 'images_207', 'images_208', 'images_209', 'images_210', 'images_211', 'images_212', 'images_213', 'images_214', 'images_215', 'images_216', 'images_217', 'images_218', 'images_219', 'images_220', 'images_221', 'images_222', 'images_223', 'images_224', 'images_225', 'images_226', 'images_227', 'images_228', 'images_229', 'images_230', 'images_231', 'images_232', 'images_233', 'images_234', 'images_235', 'images_236', 'images_237', 'images_238', 'images_239', 'images_240', 'images_241', 'images_242', 'images_243', 'images_244', 'images_245', 'images_246', 'images_247', 'images_248', 'images_249', 'images_250', 'images_251', 'images_252', 'images_253', 'images_254', 'images_255', 'images_256', 'images_257', 'images_258', 'images_259', 'images_260', 'images_261', 'images_262', 'images_263', 'images_264', 'images_265', 'images_266', 'images_267', 'images_268', 'images_269', 'images_270', 'images_271', 'images_272', 'images_273', 'images_274', 'images_275', 'images_276', 'images_277', 'images_278', 'images_279', 'images_280', 'images_281', 'images_282', 'images_283', 'images_284', 'images_285', 'images_286', 'images_287', 'images_288', 'images_289', 'images_290', 'images_291', 'images_292', 'images_293', 'images_294', 'images_295', 'images_296', 'images_297', 'images_298', 'images_299', 'images_300', 'images_301', 'images_302', 'images_303', 'images_304', 'images_305', 'images_306', 'images_307', 'images_308', 'images_309', 'images_310', 'images_311', 'images_312', 'images_313', 'images_314', 'images_315', 'images_316', 'images_317', 'images_318', 'images_319', 'images_320', 'images_321', 'images_322', 'images_323', 'images_324', 'images_325', 'images_326', 'images_327', 'images_328', 'images_329', 'images_330', 'images_331', 'images_332', 'images_333', 'images_334', 'images_335', 'images_336', 'images_337', 'images_338', 'images_339', 'images_340', 'images_341', 'images_342', 'images_343', 'images_344', 'images_345', 'images_346', 'images_347', 'images_348', 'images_349', 'images_350', 'images_351', 'images_352', 'images_353', 'images_354', 'images_355', 'images_356', 'images_357', 'images_358', 'images_359', 'images_360', 'images_361', 'images_362', 'images_363', 'images_364', 'images_365', 'images_366', 'images_367', 'images_368', 'images_369', 'images_370', 'images_371', 'images_372', 'images_373', 'images_374', 'images_375', 'images_376', 'images_377', 'images_378', 'images_379', 'images_380', 'images_381', 'images_382', 'images_383', 'images_384', 'images_385', 'images_386', 'images_387', 'images_388', 'images_389', 'images_390', 'images_391', 'images_392', 'images_393', 'images_394', 'images_395', 'images_396', 'images_397', 'images_398', 'images_399', 'images_400', 'images_401', 'images_402', 'images_403', 'images_404', 'images_405', 'images_406', 'images_407', 'images_408', 'images_409', 'images_410', 'images_411', 'images_412', 'images_413', 'images_414', 'images_415', 'images_416', 'images_417', 'images_418', 'images_419', 'images_420', 'images_421', 'images_422', 'images_423', 'images_424', 'images_425', 'images_426', 'images_427', 'images_428', 'images_429', 'images_430', 'images_431', 'images_432', 'images_433', 'images_434', 'images_435', 'images_436', 'images_437', 'images_438', 'images_439', 'images_440', 'images_441', 'images_442', 'images_443', 'images_444', 'images_445', 'images_446', 'images_447', 'images_448', 'images_449', 'images_450', 'images_451', 'images_452', 'images_453', 'images_454', 'images_455', 'images_456', 'images_457', 'images_458', 'images_459', 'images_460', 'images_461', 'images_462', 'images_463', 'images_464', 'images_465', 'images_466', 'images_467', 'images_468', 'images_469', 'images_470', 'images_471', 'images_472', 'images_473', 'images_474', 'images_475', 'images_476', 'images_477', 'images_478', 'images_479', 'images_480', 'images_481', 'images_482', 'images_483', 'images_484', 'images_485', 'images_486', 'images_487', 'images_488', 'images_489', 'images_490', 'images_491', 'images_492', 'images_493', 'images_494', 'images_495', 'images_496', 'images_497', 'images_498', 'images_499', 'images_500', 'images_501', 'images_502', 'images_503', 'images_504', 'images_505', 'images_506', 'images_507', 'images_508', 'images_509', 'images_510', 'images_511', 'images_512', 'images_513', 'images_514', 'images_515', 'images_516', 'images_517', 'images_518', 'images_519', 'images_520', 'images_521', 'images_522', 'images_523', 'images_524', 'images_525', 'images_526', 'images_527', 'images_528', 'images_529', 'images_530', 'images_531', 'images_532', 'images_533', 'images_534', 'images_535', 'images_536', 'images_537', 'images_538', 'images_539', 'images_540', 'images_541', 'images_542', 'images_543', 'images_544', 'images_545', 'images_546', 'images_547', 'images_548', 'images_549', 'images_550', 'images_551', 'images_552', 'images_553', 'images_554', 'images_555', 'images_556', 'images_557', 'images_558', 'images_559', 'images_560', 'images_561', 'images_562', 'images_563', 'images_564', 'images_565', 'images_566', 'images_567', 'images_568', 'images_569', 'images_570', 'images_571', 'images_572', 'images_573', 'images_574', 'images_575', 'images_576', 'images_577', 'images_578', 'images_579', 'images_580', 'images_581', 'images_582', 'images_583', 'images_584', 'images_585', 'images_586', 'images_587', 'images_588', 'images_589', 'images_590', 'images_591', 'images_592', 'images_593', 'images_594', 'images_595', 'images_596', 'images_597', 'images_598', 'images_599', 'images_600', 'images_601', 'images_602', 'images_603', 'images_604', 'images_605', 'images_606', 'images_607', 'images_608', 'images_609', 'images_610', 'images_611', 'images_612', 'images_613', 'images_614', 'images_615', 'images_616', 'images_617', 'images_618', 'images_619', 'images_620', 'images_621', 'images_622', 'images_623', 'images_624', 'images_625', 'images_626', 'images_627', 'images_628', 'images_629', 'images_630', 'images_631', 'images_632', 'images_633', 'images_634', 'images_635', 'images_636', 'images_637', 'images_638', 'images_639', 'images_640', 'images_641', 'images_642', 'images_643', 'images_644', 'images_645', 'images_646', 'images_647', 'images_648', 'images_649', 'images_650', 'images_651', 'images_652', 'images_653', 'images_654', 'images_655', 'images_656', 'images_657', 'images_658', 'images_659', 'images_660', 'images_661', 'images_662', 'images_663', 'images_664', 'images_665', 'images_666', 'images_667', 'images_668', 'images_669', 'images_670', 'images_671', 'images_672', 'images_673', 'images_674', 'images_675', 'images_676', 'images_677', 'images_678', 'images_679', 'images_680', 'images_681', 'images_682', 'images_683', 'images_684', 'images_685', 'images_686', 'images_687', 'images_688', 'images_689', 'images_690', 'images_691', 'images_692', 'images_693', 'images_694', 'images_695', 'images_696', 'images_697', 'images_698', 'images_699', 'images_700', 'images_701', 'images_702', 'images_703', 'images_704', 'images_705', 'images_706', 'images_707', 'images_708', 'images_709', 'images_710', 'images_711', 'images_712', 'images_713', 'images_714', 'images_715', 'images_716', 'images_717', 'images_718', 'images_719', 'images_720', 'images_721', 'images_722', 'images_723', 'images_724', 'images_725', 'images_726', 'images_727', 'images_728', 'images_729', 'images_730', 'images_731', 'images_732', 'images_733', 'images_734', 'images_735', 'images_736', 'images_737', 'images_738', 'images_739', 'images_740', 'images_741', 'images_742', 'images_743', 'images_744', 'images_745', 'images_746', 'images_747', 'images_748', 'images_749', 'images_750', 'images_751', 'images_752', 'images_753', 'images_754', 'images_755', 'images_756', 'images_757', 'images_758', 'images_759', 'images_760', 'images_761', 'images_762', 'images_763', 'images_764', 'images_765', 'images_766', 'images_767', 'images_768', 'images_769', 'images_770', 'images_771', 'images_772', 'images_773', 'images_774', 'images_775', 'images_776', 'images_777', 'images_778', 'images_779', 'images_780', 'images_781', 'images_782', 'images_783']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0909 22:50:16.225691  7632 gbdt_batch.py:438] Learner config: num_classes: 10\n",
      "regularization {\n",
      "  l2: 0.0010000000474974513\n",
      "}\n",
      "constraints {\n",
      "  max_tree_depth: 16\n",
      "}\n",
      "learning_rate_tuner {\n",
      "  fixed {\n",
      "    learning_rate: 0.10000000149011612\n",
      "  }\n",
      "}\n",
      "pruning_mode: POST_PRUNE\n",
      "growing_mode: LAYER_BY_LAYER\n",
      "multi_class_strategy: DIAGONAL_HESSIAN\n",
      "\n",
      "I0909 22:50:16.321434  7632 evaluation.py:255] Starting evaluation at 2019-09-09T22:50:16Z\n",
      "I0909 22:50:16.421168  7632 monitored_session.py:240] Graph was finalized.\n",
      "W0909 22:50:16.422166  7632 deprecation.py:323] From C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "I0909 22:50:16.428186  7632 saver.py:1280] Restoring parameters from C:\\Users\\MR5E8F~1.WU\\AppData\\Local\\Temp\\tmppf1qujhw\\model.ckpt-161\n",
      "I0909 22:50:16.515915  7632 session_manager.py:500] Running local_init_op.\n",
      "I0909 22:50:16.536859  7632 session_manager.py:502] Done running local_init_op.\n",
      "I0909 22:50:17.276421  7632 evaluation.py:275] Finished evaluation at 2019-09-09-22:50:17\n",
      "I0909 22:50:17.277390  7632 estimator.py:347] Saving dict for global step 161: accuracy = 0.9287, global_step = 161, loss = 0.24334426\n",
      "W0909 22:50:17.459919  7632 meta_graph.py:449] Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.9287\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Use the Estimator 'evaluate' method\n",
    "e = gbdt_model.evaluate(input_fn=input_fn)\n",
    "print(\"Testing Accuracy:\", e['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (Word Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "num_steps = 3000000\n",
    "display_step = 10000\n",
    "eval_step = 200000\n",
    "\n",
    "# Evaluation Parameters\n",
    "eval_words = ['five', 'of', 'going', 'hardware', 'american', 'britain']\n",
    "\n",
    "# Word2Vec Parameters\n",
    "embedding_size = 200 # Dimension of the embedding vector\n",
    "max_vocabulary_size = 50000 # Total number of different words in the vocabulary\n",
    "min_occurrence = 10 # Remove all words that does not appears at least n times\n",
    "skip_window = 3 # How many words to consider left and right\n",
    "num_skips = 2 # How many times to reuse an input to generate a label\n",
    "num_sampled = 64 # Number of negative examples to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the dataset... (It may take some time)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Download a small chunk of Wikipedia articles collection\n",
    "url = 'http://mattmahoney.net/dc/text8.zip'\n",
    "data_path = 'text8.zip'\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Downloading the dataset... (It may take some time)\")\n",
    "    filename, _ = urllib.request.urlretrieve(url, data_path)\n",
    "    print(\"Done!\")\n",
    "# Unzip the dataset file. Text has already been processed\n",
    "with zipfile.ZipFile(data_path) as f:\n",
    "    text_words = f.read(f.namelist()[0]).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dictionary and replace rare words with UNK token\n",
    "count = [('UNK', -1)]\n",
    "# Retrieve the most common words\n",
    "count.extend(collections.Counter(text_words).most_common(max_vocabulary_size - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('UNK', -1),\n",
       " (b'the', 1061396),\n",
       " (b'of', 593677),\n",
       " (b'and', 416629),\n",
       " (b'one', 411764),\n",
       " (b'in', 372201),\n",
       " (b'a', 325873),\n",
       " (b'to', 316376),\n",
       " (b'zero', 264975),\n",
       " (b'nine', 250430),\n",
       " (b'two', 192644),\n",
       " (b'is', 183153),\n",
       " (b'as', 131815),\n",
       " (b'eight', 125285),\n",
       " (b'for', 118445),\n",
       " (b's', 116710),\n",
       " (b'five', 115789),\n",
       " (b'three', 114775),\n",
       " (b'was', 112807),\n",
       " (b'by', 111831),\n",
       " (b'that', 109510),\n",
       " (b'four', 108182),\n",
       " (b'six', 102145),\n",
       " (b'seven', 99683),\n",
       " (b'with', 95603),\n",
       " (b'on', 91250),\n",
       " (b'are', 76527),\n",
       " (b'it', 73334),\n",
       " (b'from', 72871),\n",
       " (b'or', 68945),\n",
       " (b'his', 62603),\n",
       " (b'an', 61925),\n",
       " (b'be', 61281),\n",
       " (b'this', 58832),\n",
       " (b'which', 54788),\n",
       " (b'at', 54576),\n",
       " (b'he', 53573),\n",
       " (b'also', 44358),\n",
       " (b'not', 44033),\n",
       " (b'have', 39712),\n",
       " (b'were', 39086),\n",
       " (b'has', 37866),\n",
       " (b'but', 35358),\n",
       " (b'other', 32433),\n",
       " (b'their', 31523),\n",
       " (b'its', 29567),\n",
       " (b'first', 28810),\n",
       " (b'they', 28553),\n",
       " (b'some', 28161),\n",
       " (b'had', 28100),\n",
       " (b'all', 26229),\n",
       " (b'more', 26223),\n",
       " (b'most', 25563),\n",
       " (b'can', 25519),\n",
       " (b'been', 25383),\n",
       " (b'such', 24413),\n",
       " (b'many', 24096),\n",
       " (b'who', 23997),\n",
       " (b'new', 23770),\n",
       " (b'used', 22737),\n",
       " (b'there', 22707),\n",
       " (b'after', 21125),\n",
       " (b'when', 20623),\n",
       " (b'into', 20484),\n",
       " (b'american', 20477),\n",
       " (b'time', 20412),\n",
       " (b'these', 19864),\n",
       " (b'only', 19463),\n",
       " (b'see', 19206),\n",
       " (b'may', 19115),\n",
       " (b'than', 18807),\n",
       " (b'world', 17949),\n",
       " (b'i', 17581),\n",
       " (b'b', 17516),\n",
       " (b'would', 17377),\n",
       " (b'd', 17236),\n",
       " (b'no', 16155),\n",
       " (b'however', 15861),\n",
       " (b'between', 15737),\n",
       " (b'about', 15574),\n",
       " (b'over', 15122),\n",
       " (b'years', 14935),\n",
       " (b'states', 14916),\n",
       " (b'people', 14696),\n",
       " (b'war', 14629),\n",
       " (b'during', 14578),\n",
       " (b'united', 14494),\n",
       " (b'known', 14437),\n",
       " (b'if', 14420),\n",
       " (b'called', 14151),\n",
       " (b'use', 14011),\n",
       " (b'th', 13380),\n",
       " (b'system', 13296),\n",
       " (b'often', 12987),\n",
       " (b'state', 12904),\n",
       " (b'so', 12722),\n",
       " (b'history', 12623),\n",
       " (b'will', 12560),\n",
       " (b'up', 12445),\n",
       " (b'while', 12363),\n",
       " (b'where', 12347),\n",
       " (b'city', 12275),\n",
       " (b'being', 11931),\n",
       " (b'english', 11868),\n",
       " (b'then', 11847),\n",
       " (b'any', 11803),\n",
       " (b'both', 11755),\n",
       " (b'under', 11753),\n",
       " (b'out', 11721),\n",
       " (b'made', 11701),\n",
       " (b'well', 11537),\n",
       " (b'her', 11536),\n",
       " (b'e', 11426),\n",
       " (b'number', 11399),\n",
       " (b'government', 11323),\n",
       " (b'them', 11285),\n",
       " (b'm', 10976),\n",
       " (b'later', 10971),\n",
       " (b'since', 10691),\n",
       " (b'him', 10629),\n",
       " (b'part', 10627),\n",
       " (b'name', 10572),\n",
       " (b'c', 10561),\n",
       " (b'century', 10550),\n",
       " (b'through', 10371),\n",
       " (b'because', 10332),\n",
       " (b'x', 10307),\n",
       " (b'university', 10195),\n",
       " (b'early', 10172),\n",
       " (b'life', 10096),\n",
       " (b'british', 10056),\n",
       " (b'year', 9858),\n",
       " (b'like', 9854),\n",
       " (b'same', 9774),\n",
       " (b'including', 9633),\n",
       " (b'became', 9591),\n",
       " (b'example', 9539),\n",
       " (b'day', 9534),\n",
       " (b'each', 9500),\n",
       " (b'even', 9412),\n",
       " (b'work', 9388),\n",
       " (b'language', 9375),\n",
       " (b'although', 9286),\n",
       " (b'several', 9168),\n",
       " (b'form', 9133),\n",
       " (b'john', 8956),\n",
       " (b'u', 8928),\n",
       " (b'national', 8904),\n",
       " (b'very', 8861),\n",
       " (b'much', 8822),\n",
       " (b'g', 8773),\n",
       " (b'french', 8736),\n",
       " (b'before', 8700),\n",
       " (b'general', 8659),\n",
       " (b'what', 8581),\n",
       " (b't', 8491),\n",
       " (b'against', 8432),\n",
       " (b'n', 8372),\n",
       " (b'high', 8337),\n",
       " (b'links', 8312),\n",
       " (b'could', 8304),\n",
       " (b'based', 8244),\n",
       " (b'those', 8209),\n",
       " (b'now', 8206),\n",
       " (b'second', 8110),\n",
       " (b'de', 8002),\n",
       " (b'music', 7987),\n",
       " (b'another', 7933),\n",
       " (b'large', 7898),\n",
       " (b'she', 7896),\n",
       " (b'f', 7878),\n",
       " (b'external', 7862),\n",
       " (b'german', 7858),\n",
       " (b'different', 7797),\n",
       " (b'modern', 7790),\n",
       " (b'great', 7770),\n",
       " (b'do', 7763),\n",
       " (b'common', 7698),\n",
       " (b'set', 7682),\n",
       " (b'list', 7672),\n",
       " (b'south', 7628),\n",
       " (b'series', 7611),\n",
       " (b'major', 7585),\n",
       " (b'game', 7553),\n",
       " (b'power', 7522),\n",
       " (b'long', 7488),\n",
       " (b'country', 7481),\n",
       " (b'king', 7456),\n",
       " (b'law', 7435),\n",
       " (b'group', 7417),\n",
       " (b'film', 7400),\n",
       " (b'still', 7378),\n",
       " (b'until', 7368),\n",
       " (b'north', 7328),\n",
       " (b'international', 7262),\n",
       " (b'term', 7219),\n",
       " (b'we', 7118),\n",
       " (b'end', 7113),\n",
       " (b'book', 7110),\n",
       " (b'found', 7043),\n",
       " (b'own', 7034),\n",
       " (b'political', 6970),\n",
       " (b'party', 6943),\n",
       " (b'order', 6908),\n",
       " (b'usually', 6872),\n",
       " (b'president', 6865),\n",
       " (b'church', 6786),\n",
       " (b'you', 6690),\n",
       " (b'death', 6684),\n",
       " (b'theory', 6604),\n",
       " (b'area', 6591),\n",
       " (b'around', 6576),\n",
       " (b'include', 6531),\n",
       " (b'god', 6518),\n",
       " (b'ii', 6494),\n",
       " (b'way', 6433),\n",
       " (b'did', 6419),\n",
       " (b'military', 6410),\n",
       " (b'population', 6400),\n",
       " (b'using', 6384),\n",
       " (b'though', 6362),\n",
       " (b'small', 6361),\n",
       " (b'following', 6300),\n",
       " (b'within', 6238),\n",
       " (b'non', 6220),\n",
       " (b'human', 6184),\n",
       " (b'left', 6184),\n",
       " (b'main', 6166),\n",
       " (b'among', 6146),\n",
       " (b'point', 6141),\n",
       " (b'r', 6064),\n",
       " (b'due', 6056),\n",
       " (b'p', 6001),\n",
       " (b'considered', 5985),\n",
       " (b'public', 5976),\n",
       " (b'popular', 5967),\n",
       " (b'computer', 5874),\n",
       " (b'west', 5862),\n",
       " (b'family', 5857),\n",
       " (b'east', 5854),\n",
       " (b'information', 5844),\n",
       " (b'important', 5843),\n",
       " (b'european', 5806),\n",
       " (b'man', 5778),\n",
       " (b'sometimes', 5761),\n",
       " (b'right', 5758),\n",
       " (b'old', 5711),\n",
       " (b'free', 5684),\n",
       " (b'word', 5678),\n",
       " (b'without', 5661),\n",
       " (b'last', 5654),\n",
       " (b'us', 5652),\n",
       " (b'members', 5634),\n",
       " (b'given', 5605),\n",
       " (b'times', 5582),\n",
       " (b'roman', 5468),\n",
       " (b'make', 5450),\n",
       " (b'h', 5404),\n",
       " (b'age', 5350),\n",
       " (b'place', 5345),\n",
       " (b'l', 5343),\n",
       " (b'thus', 5319),\n",
       " (b'science', 5314),\n",
       " (b'case', 5289),\n",
       " (b'become', 5268),\n",
       " (b'systems', 5262),\n",
       " (b'union', 5248),\n",
       " (b'born', 5246),\n",
       " (b'york', 5243),\n",
       " (b'line', 5236),\n",
       " (b'countries', 5231),\n",
       " (b'does', 5221),\n",
       " (b'isbn', 5216),\n",
       " (b'st', 5207),\n",
       " (b'control', 5196),\n",
       " (b'various', 5163),\n",
       " (b'others', 5160),\n",
       " (b'house', 5157),\n",
       " (b'article', 5143),\n",
       " (b'island', 5124),\n",
       " (b'should', 5113),\n",
       " (b'led', 5108),\n",
       " (b'back', 5105),\n",
       " (b'period', 5099),\n",
       " (b'player', 5096),\n",
       " (b'europe', 5094),\n",
       " (b'languages', 5087),\n",
       " (b'central', 5070),\n",
       " (b'water', 5025),\n",
       " (b'few', 5013),\n",
       " (b'western', 5010),\n",
       " (b'home', 5007),\n",
       " (b'began', 5004),\n",
       " (b'generally', 4979),\n",
       " (b'less', 4974),\n",
       " (b'k', 4970),\n",
       " (b'similar', 4939),\n",
       " (b'written', 4916),\n",
       " (b'original', 4910),\n",
       " (b'best', 4902),\n",
       " (b'must', 4898),\n",
       " (b'according', 4884),\n",
       " (b'school', 4872),\n",
       " (b'france', 4813),\n",
       " (b'air', 4802),\n",
       " (b'single', 4801),\n",
       " (b'force', 4776),\n",
       " (b'v', 4762),\n",
       " (b'land', 4755),\n",
       " (b'groups', 4731),\n",
       " (b'down', 4728),\n",
       " (b'how', 4727),\n",
       " (b'works', 4724),\n",
       " (b'development', 4721),\n",
       " (b'official', 4720),\n",
       " (b'support', 4686),\n",
       " (b'england', 4641),\n",
       " (b'j', 4612),\n",
       " (b'rather', 4605),\n",
       " (b'data', 4586),\n",
       " (b'space', 4586),\n",
       " (b'greek', 4577),\n",
       " (b'km', 4574),\n",
       " (b'named', 4551),\n",
       " (b'germany', 4539),\n",
       " (b'just', 4528),\n",
       " (b'games', 4527),\n",
       " (b'said', 4483),\n",
       " (b'version', 4472),\n",
       " (b'late', 4471),\n",
       " (b'earth', 4457),\n",
       " (b'company', 4448),\n",
       " (b'every', 4446),\n",
       " (b'economic', 4435),\n",
       " (b'short', 4433),\n",
       " (b'published', 4430),\n",
       " (b'black', 4429),\n",
       " (b'army', 4418),\n",
       " (b'off', 4414),\n",
       " (b'london', 4399),\n",
       " (b'million', 4386),\n",
       " (b'body', 4383),\n",
       " (b'field', 4352),\n",
       " (b'christian', 4347),\n",
       " (b'either', 4330),\n",
       " (b'social', 4307),\n",
       " (b'empire', 4307),\n",
       " (b'o', 4299),\n",
       " (b'developed', 4270),\n",
       " (b'standard', 4269),\n",
       " (b'court', 4268),\n",
       " (b'service', 4260),\n",
       " (b'kingdom', 4257),\n",
       " (b'along', 4241),\n",
       " (b'college', 4236),\n",
       " (b'republic', 4231),\n",
       " (b'sea', 4212),\n",
       " (b'america', 4202),\n",
       " (b'today', 4198),\n",
       " (b'result', 4193),\n",
       " (b'held', 4178),\n",
       " (b'team', 4176),\n",
       " (b'light', 4172),\n",
       " (b'means', 4165),\n",
       " (b'never', 4127),\n",
       " (b'especially', 4125),\n",
       " (b'third', 4121),\n",
       " (b'further', 4118),\n",
       " (b'character', 4114),\n",
       " (b'forces', 4114),\n",
       " (b'take', 4105),\n",
       " (b'men', 4078),\n",
       " (b'society', 4067),\n",
       " (b'show', 4064),\n",
       " (b'open', 4063),\n",
       " (b'possible', 4051),\n",
       " (b'fact', 4042),\n",
       " (b'battle', 4034),\n",
       " (b'took', 4013),\n",
       " (b'former', 4007),\n",
       " (b'books', 3992),\n",
       " (b'soviet', 3985),\n",
       " (b'river', 3984),\n",
       " (b'children', 3978),\n",
       " (b'having', 3964),\n",
       " (b'good', 3962),\n",
       " (b'local', 3960),\n",
       " (b'current', 3955),\n",
       " (b'son', 3955),\n",
       " (b'process', 3949),\n",
       " (b'natural', 3947),\n",
       " (b'present', 3922),\n",
       " (b'himself', 3919),\n",
       " (b'islands', 3903),\n",
       " (b'total', 3891),\n",
       " (b'near', 3874),\n",
       " (b'white', 3866),\n",
       " (b'days', 3863),\n",
       " (b'person', 3855),\n",
       " (b'itself', 3843),\n",
       " (b'seen', 3831),\n",
       " (b'culture', 3809),\n",
       " (b'little', 3805),\n",
       " (b'above', 3782),\n",
       " (b'software', 3773),\n",
       " (b'largest', 3772),\n",
       " (b'words', 3771),\n",
       " (b'upon', 3768),\n",
       " (b'level', 3767),\n",
       " (b'father', 3762),\n",
       " (b'created', 3760),\n",
       " (b'side', 3760),\n",
       " (b'red', 3755),\n",
       " (b'references', 3747),\n",
       " (b'press', 3745),\n",
       " (b'full', 3734),\n",
       " (b'region', 3726),\n",
       " (b'almost', 3717),\n",
       " (b'image', 3714),\n",
       " (b'al', 3714),\n",
       " (b'famous', 3708),\n",
       " (b'play', 3702),\n",
       " (b'came', 3698),\n",
       " (b'role', 3691),\n",
       " (b'once', 3686),\n",
       " (b'certain', 3675),\n",
       " (b'league', 3663),\n",
       " (b'jewish', 3662),\n",
       " (b'james', 3653),\n",
       " (b'january', 3650),\n",
       " (b'site', 3639),\n",
       " (b'again', 3616),\n",
       " (b'numbers', 3611),\n",
       " (b'art', 3611),\n",
       " (b'member', 3601),\n",
       " (b'areas', 3599),\n",
       " (b'movement', 3595),\n",
       " (b'religious', 3588),\n",
       " (b'type', 3588),\n",
       " (b'march', 3583),\n",
       " (b'community', 3581),\n",
       " (b'story', 3573),\n",
       " (b'played', 3572),\n",
       " (b'production', 3570),\n",
       " (b'released', 3555),\n",
       " (b'center', 3548),\n",
       " (b'rights', 3546),\n",
       " (b'real', 3545),\n",
       " (b'related', 3536),\n",
       " (b'foreign', 3528),\n",
       " (b'low', 3524),\n",
       " (b'ancient', 3523),\n",
       " (b'terms', 3519),\n",
       " (b'view', 3519),\n",
       " (b'source', 3503),\n",
       " (b'act', 3502),\n",
       " (b'minister', 3493),\n",
       " (b'change', 3483),\n",
       " (b'energy', 3464),\n",
       " (b'produced', 3457),\n",
       " (b'research', 3452),\n",
       " (b'actor', 3451),\n",
       " (b'making', 3448),\n",
       " (b'civil', 3443),\n",
       " (b'december', 3443),\n",
       " (b'women', 3442),\n",
       " (b'special', 3441),\n",
       " (b'style', 3438),\n",
       " (b'william', 3437),\n",
       " (b'design', 3437),\n",
       " (b'japanese', 3437),\n",
       " (b'available', 3435),\n",
       " (b'chinese', 3430),\n",
       " (b'forms', 3429),\n",
       " (b'canada', 3428),\n",
       " (b'northern', 3423),\n",
       " (b'died', 3418),\n",
       " (b'class', 3412),\n",
       " (b'living', 3410),\n",
       " (b'next', 3406),\n",
       " (b'particular', 3404),\n",
       " (b'program', 3403),\n",
       " (b'council', 3401),\n",
       " (b'television', 3395),\n",
       " (b'head', 3376),\n",
       " (b'david', 3368),\n",
       " (b'china', 3365),\n",
       " (b'middle', 3363),\n",
       " (b'established', 3360),\n",
       " (b'hand', 3356),\n",
       " (b'bc', 3356),\n",
       " (b'far', 3352),\n",
       " (b'july', 3333),\n",
       " (b'function', 3330),\n",
       " (b'position', 3318),\n",
       " (b'y', 3311),\n",
       " (b'built', 3310),\n",
       " (b'george', 3307),\n",
       " (b'band', 3304),\n",
       " (b'together', 3303),\n",
       " (b'w', 3301),\n",
       " (b'latin', 3290),\n",
       " (b'thought', 3277),\n",
       " (b'eastern', 3276),\n",
       " (b'charles', 3268),\n",
       " (b'parts', 3266),\n",
       " (b'instead', 3259),\n",
       " (b'study', 3248),\n",
       " (b'might', 3242),\n",
       " (b'india', 3242),\n",
       " (b'code', 3240),\n",
       " (b'included', 3213),\n",
       " (b'meaning', 3213),\n",
       " (b'trade', 3207),\n",
       " (b'per', 3206),\n",
       " (b'june', 3195),\n",
       " (b'least', 3185),\n",
       " (b'half', 3176),\n",
       " (b'model', 3141),\n",
       " (b'economy', 3136),\n",
       " (b'prime', 3135),\n",
       " (b'traditional', 3133),\n",
       " (b'always', 3131),\n",
       " (b'capital', 3129),\n",
       " (b'range', 3125),\n",
       " (b'november', 3119),\n",
       " (b'emperor', 3119),\n",
       " (b'young', 3109),\n",
       " (b'anti', 3103),\n",
       " (b'final', 3102),\n",
       " (b'text', 3098),\n",
       " (b'players', 3088),\n",
       " (b'uk', 3079),\n",
       " (b'april', 3069),\n",
       " (b'run', 3063),\n",
       " (b'september', 3055),\n",
       " (b'addition', 3050),\n",
       " (b'radio', 3050),\n",
       " (b'live', 3046),\n",
       " (b'august', 3044),\n",
       " (b'taken', 3043),\n",
       " (b'note', 3043),\n",
       " (b'italian', 3037),\n",
       " (b'lost', 3035),\n",
       " (b'nature', 3027),\n",
       " (b'project', 3022),\n",
       " (b'technology', 3017),\n",
       " (b'spanish', 3011),\n",
       " (b'october', 3009),\n",
       " (b'recent', 3002),\n",
       " (b'rate', 3002),\n",
       " (b'won', 2989),\n",
       " (b'true', 2965),\n",
       " (b'value', 2964),\n",
       " (b'uses', 2963),\n",
       " (b'russian', 2961),\n",
       " (b'est', 2959),\n",
       " (b'wrote', 2935),\n",
       " (b'effect', 2933),\n",
       " (b'album', 2932),\n",
       " (b'southern', 2927),\n",
       " (b'africa', 2923),\n",
       " (b'whose', 2914),\n",
       " (b'top', 2913),\n",
       " (b'historical', 2899),\n",
       " (b'australia', 2895),\n",
       " (b'catholic', 2888),\n",
       " (b'particularly', 2882),\n",
       " (b'self', 2879),\n",
       " (b'structure', 2876),\n",
       " (b'record', 2863),\n",
       " (b'evidence', 2857),\n",
       " (b'rule', 2852),\n",
       " (b'themselves', 2852),\n",
       " (b'influence', 2845),\n",
       " (b'cases', 2843),\n",
       " (b'subject', 2830),\n",
       " (b'referred', 2829),\n",
       " (b'continued', 2822),\n",
       " (b'nations', 2821),\n",
       " (b'below', 2819),\n",
       " (b'rock', 2819),\n",
       " (b'japan', 2817),\n",
       " (b'com', 2810),\n",
       " (b'song', 2809),\n",
       " (b'throughout', 2806),\n",
       " (b'names', 2806),\n",
       " (b'female', 2797),\n",
       " (b'title', 2796),\n",
       " (b'therefore', 2795),\n",
       " (b'our', 2795),\n",
       " (b'office', 2785),\n",
       " (b'star', 2784),\n",
       " (b'paul', 2777),\n",
       " (b'too', 2775),\n",
       " (b'cities', 2773),\n",
       " (b'february', 2771),\n",
       " (b'independent', 2771),\n",
       " (b'author', 2769),\n",
       " (b'problem', 2767),\n",
       " (b'species', 2761),\n",
       " (b'education', 2759),\n",
       " (b'done', 2759),\n",
       " (b'philosophy', 2758),\n",
       " (b'come', 2755),\n",
       " (b'higher', 2753),\n",
       " (b'originally', 2752),\n",
       " (b'market', 2748),\n",
       " (b'town', 2746),\n",
       " (b'my', 2745),\n",
       " (b'season', 2741),\n",
       " (b'love', 2740),\n",
       " (b'strong', 2736),\n",
       " (b'israel', 2735),\n",
       " (b'irish', 2731),\n",
       " (b'writer', 2731),\n",
       " (b'films', 2727),\n",
       " (b'elements', 2723),\n",
       " (b'robert', 2722),\n",
       " (b'whether', 2719),\n",
       " (b'despite', 2716),\n",
       " (b'eventually', 2714),\n",
       " (b'here', 2709),\n",
       " (b'football', 2708),\n",
       " (b'action', 2699),\n",
       " (b'internet', 2697),\n",
       " (b'individual', 2693),\n",
       " (b'sound', 2693),\n",
       " (b'network', 2691),\n",
       " (b'described', 2685),\n",
       " (b'practice', 2681),\n",
       " (b'characters', 2674),\n",
       " (b're', 2664),\n",
       " (b'royal', 2663),\n",
       " (b'la', 2660),\n",
       " (b'events', 2650),\n",
       " (b'formed', 2644),\n",
       " (b'commonly', 2638),\n",
       " (b'base', 2633),\n",
       " (b'received', 2632),\n",
       " (b'african', 2628),\n",
       " (b'problems', 2628),\n",
       " (b'food', 2623),\n",
       " (b'jews', 2620),\n",
       " (b'able', 2619),\n",
       " (b'male', 2615),\n",
       " (b'typically', 2595),\n",
       " (b'mass', 2595),\n",
       " (b'complex', 2593),\n",
       " (b'lower', 2583),\n",
       " (b'includes', 2576),\n",
       " (b'outside', 2574),\n",
       " (b'legal', 2562),\n",
       " (b'complete', 2561),\n",
       " (b'significant', 2554),\n",
       " (b'parliament', 2554),\n",
       " (b'actually', 2553),\n",
       " (b'business', 2542),\n",
       " (b'fiction', 2540),\n",
       " (b'physical', 2539),\n",
       " (b'followed', 2537),\n",
       " (b'deaths', 2530),\n",
       " (b'key', 2527),\n",
       " (b'leader', 2525),\n",
       " (b'widely', 2525),\n",
       " (b'page', 2520),\n",
       " (b'basic', 2519),\n",
       " (b'types', 2518),\n",
       " (b'henry', 2494),\n",
       " (b'elected', 2493),\n",
       " (b'beginning', 2493),\n",
       " (b'fire', 2489),\n",
       " (b'building', 2480),\n",
       " (b'independence', 2478),\n",
       " (b'went', 2475),\n",
       " (b'movie', 2471),\n",
       " (b'aircraft', 2471),\n",
       " (b'ever', 2470),\n",
       " (b'canadian', 2468),\n",
       " (b'material', 2464),\n",
       " (b'births', 2462),\n",
       " (b'video', 2461),\n",
       " (b'news', 2458),\n",
       " (b'future', 2457),\n",
       " (b'scientific', 2456),\n",
       " (b'simply', 2454),\n",
       " (b'go', 2451),\n",
       " (b'defined', 2449),\n",
       " (b'laws', 2444),\n",
       " (b'get', 2440),\n",
       " (b'close', 2439),\n",
       " (b'industry', 2434),\n",
       " (b'specific', 2432),\n",
       " (b'examples', 2431),\n",
       " (b'believe', 2404),\n",
       " (b'services', 2404),\n",
       " (b'idea', 2398),\n",
       " (b'method', 2397),\n",
       " (b'introduced', 2397),\n",
       " (b'points', 2396),\n",
       " (b'return', 2395),\n",
       " (b'cause', 2392),\n",
       " (b'indian', 2389),\n",
       " (b'britain', 2384),\n",
       " (b'features', 2380),\n",
       " (b'majority', 2379),\n",
       " (b'size', 2379),\n",
       " (b'post', 2376),\n",
       " (b'lead', 2375),\n",
       " (b'organization', 2374),\n",
       " (b'cannot', 2373),\n",
       " (b'designed', 2364),\n",
       " (b'ireland', 2362),\n",
       " (b'cross', 2361),\n",
       " (b'classical', 2359),\n",
       " (b'personal', 2357),\n",
       " (b'writing', 2356),\n",
       " (b'concept', 2353),\n",
       " (b'associated', 2349),\n",
       " (b'required', 2349),\n",
       " (b'soon', 2343),\n",
       " (b'changes', 2337),\n",
       " (b'california', 2336),\n",
       " (b'located', 2336),\n",
       " (b'sense', 2331),\n",
       " (b'believed', 2319),\n",
       " (b'away', 2313),\n",
       " (b'started', 2313),\n",
       " (b'co', 2308),\n",
       " (b'religion', 2304),\n",
       " (b'mother', 2303),\n",
       " (b'county', 2300),\n",
       " (b'rules', 2299),\n",
       " (b'studies', 2298),\n",
       " (b'yet', 2296),\n",
       " (b'find', 2290),\n",
       " (b'knowledge', 2289),\n",
       " (b'put', 2288),\n",
       " (b'founded', 2282),\n",
       " (b'policy', 2278),\n",
       " (b'currently', 2275),\n",
       " (b'provide', 2272),\n",
       " (b'working', 2271),\n",
       " (b'media', 2267),\n",
       " (b'election', 2266),\n",
       " (b'australian', 2260),\n",
       " (b'me', 2256),\n",
       " (b'thomas', 2255),\n",
       " (b'allowed', 2252),\n",
       " (b'russia', 2251),\n",
       " (b'earlier', 2249),\n",
       " (b'greater', 2249),\n",
       " (b'limited', 2248),\n",
       " (b'object', 2243),\n",
       " (b'brought', 2241),\n",
       " (b'online', 2236),\n",
       " (b'association', 2231),\n",
       " (b'lord', 2231),\n",
       " (b'mostly', 2227),\n",
       " (b'blue', 2223),\n",
       " (b'constitution', 2222),\n",
       " (b'across', 2222),\n",
       " (b'added', 2218),\n",
       " (b'interest', 2217),\n",
       " (b'things', 2216),\n",
       " (b'relations', 2215),\n",
       " (b'speed', 2206),\n",
       " (b'federal', 2200),\n",
       " (b'singer', 2195),\n",
       " (b'effects', 2194),\n",
       " (b'growth', 2193),\n",
       " (b'sources', 2192),\n",
       " (b'your', 2189),\n",
       " (b'remains', 2183),\n",
       " (b'z', 2182),\n",
       " (b'probably', 2178),\n",
       " (b'gave', 2178),\n",
       " (b'simple', 2177),\n",
       " (b'attack', 2175),\n",
       " (b'longer', 2174),\n",
       " (b'reference', 2171),\n",
       " (b'saint', 2168),\n",
       " (b'success', 2164),\n",
       " (b'killed', 2163),\n",
       " (b'past', 2162),\n",
       " (b'career', 2162),\n",
       " (b'need', 2159),\n",
       " (b'park', 2156),\n",
       " (b'definition', 2147),\n",
       " (b'say', 2139),\n",
       " (b'etc', 2137),\n",
       " (b'give', 2133),\n",
       " (b'peace', 2133),\n",
       " (b'chief', 2130),\n",
       " (b'stories', 2128),\n",
       " (b'security', 2126),\n",
       " (b'wide', 2125),\n",
       " (b'ball', 2124),\n",
       " (b'saw', 2119),\n",
       " (b'machine', 2117),\n",
       " (b'better', 2116),\n",
       " (b'cell', 2115),\n",
       " (b'leading', 2114),\n",
       " (b'becomes', 2111),\n",
       " (b'spain', 2109),\n",
       " (b'larger', 2109),\n",
       " (b'products', 2108),\n",
       " (b'parties', 2108),\n",
       " (b'night', 2108),\n",
       " (b'remained', 2105),\n",
       " (b'prize', 2103),\n",
       " (b'months', 2102),\n",
       " (b'website', 2102),\n",
       " (b'big', 2102),\n",
       " (b'cultural', 2101),\n",
       " (b'money', 2101),\n",
       " (b'help', 2099),\n",
       " (b'territory', 2099),\n",
       " (b'private', 2098),\n",
       " (b'moved', 2098),\n",
       " (b'letter', 2093),\n",
       " (b'wife', 2093),\n",
       " (b'politics', 2090),\n",
       " (b'lines', 2090),\n",
       " (b'largely', 2089),\n",
       " (b'contains', 2086),\n",
       " (b'companies', 2084),\n",
       " (b'lake', 2080),\n",
       " (b'perhaps', 2079),\n",
       " (b'green', 2074),\n",
       " (b'already', 2070),\n",
       " (b'dead', 2069),\n",
       " (b'iii', 2069),\n",
       " (b'library', 2068),\n",
       " (b'separate', 2067),\n",
       " (b'refer', 2066),\n",
       " (b'makes', 2065),\n",
       " (b'appeared', 2064),\n",
       " (b'dutch', 2060),\n",
       " (b'holy', 2059),\n",
       " (b'era', 2057),\n",
       " (b'novel', 2054),\n",
       " (b'successful', 2052),\n",
       " (b'italy', 2050),\n",
       " (b'letters', 2049),\n",
       " (b'results', 2048),\n",
       " (b'matter', 2045),\n",
       " (b'produce', 2042),\n",
       " (b'origin', 2041),\n",
       " (b'claim', 2039),\n",
       " (b'whole', 2034),\n",
       " (b'directly', 2033),\n",
       " (b'attempt', 2033),\n",
       " (b'actress', 2031),\n",
       " (b'surface', 2030),\n",
       " (b'revolution', 2029),\n",
       " (b'highly', 2026),\n",
       " (b'caused', 2025),\n",
       " (b'status', 2024),\n",
       " (b'musical', 2022),\n",
       " (b'richard', 2021),\n",
       " (b'commercial', 2020),\n",
       " (b'division', 2018),\n",
       " (b'color', 2017),\n",
       " (b'health', 2013),\n",
       " (b'coast', 2013),\n",
       " (b'release', 2013),\n",
       " (b'latter', 2012),\n",
       " (b'authority', 2009),\n",
       " (b'treaty', 2007),\n",
       " (b'turn', 2005),\n",
       " (b'michael', 2000),\n",
       " (b'nation', 1995),\n",
       " (b'direct', 1994),\n",
       " (b'asia', 1992),\n",
       " (b'edition', 1990),\n",
       " (b'programming', 1989),\n",
       " (b'playing', 1981),\n",
       " (b'date', 1980),\n",
       " (b'mary', 1977),\n",
       " (b'native', 1977),\n",
       " (b'whom', 1977),\n",
       " (b'married', 1973),\n",
       " (b'towards', 1971),\n",
       " (b'issues', 1967),\n",
       " (b'double', 1966),\n",
       " (b'primary', 1959),\n",
       " (b'basis', 1959),\n",
       " (b'allow', 1958),\n",
       " (b'enough', 1958),\n",
       " (b'memory', 1957),\n",
       " (b'reason', 1954),\n",
       " (b'web', 1952),\n",
       " (b'exist', 1950),\n",
       " (b'provided', 1948),\n",
       " (b'oil', 1945),\n",
       " (b'course', 1945),\n",
       " (b'functions', 1945),\n",
       " (b'alexander', 1944),\n",
       " (b'analysis', 1944),\n",
       " (b'chemical', 1944),\n",
       " (b'mid', 1941),\n",
       " (b'replaced', 1941),\n",
       " (b'queen', 1940),\n",
       " (b'claims', 1938),\n",
       " (b'tv', 1938),\n",
       " (b'sun', 1938),\n",
       " (b'literature', 1936),\n",
       " (b'metal', 1935),\n",
       " (b'amount', 1934),\n",
       " (b'divided', 1930),\n",
       " (b'blood', 1927),\n",
       " (b'likely', 1926),\n",
       " (b'access', 1924),\n",
       " (b'average', 1924),\n",
       " (b'length', 1922),\n",
       " (b'smaller', 1921),\n",
       " (b'medical', 1919),\n",
       " (b'property', 1918),\n",
       " (b'students', 1916),\n",
       " (b'degree', 1915),\n",
       " (b'elections', 1914),\n",
       " (b'club', 1913),\n",
       " (b'claimed', 1912),\n",
       " (b'performance', 1911),\n",
       " (b'director', 1907),\n",
       " (b'digital', 1905),\n",
       " (b'front', 1904),\n",
       " (b'museum', 1904),\n",
       " (b'difficult', 1903),\n",
       " (b'tradition', 1903),\n",
       " (b'nearly', 1902),\n",
       " (b'schools', 1900),\n",
       " (b'washington', 1899),\n",
       " (b'gas', 1897),\n",
       " (b'jesus', 1896),\n",
       " (b'map', 1896),\n",
       " (b'louis', 1895),\n",
       " (b'rome', 1895),\n",
       " (b'unit', 1892),\n",
       " (b'baseball', 1890),\n",
       " (b'mind', 1888),\n",
       " (b'peter', 1887),\n",
       " (b'mark', 1887),\n",
       " (b'collection', 1883),\n",
       " (b'product', 1881),\n",
       " (b'congress', 1880),\n",
       " (b'programs', 1872),\n",
       " (b'changed', 1871),\n",
       " (b'ideas', 1870),\n",
       " (b'moon', 1868),\n",
       " (b'entire', 1866),\n",
       " (b'user', 1865),\n",
       " (b'ground', 1864),\n",
       " (b'records', 1862),\n",
       " (b'frequently', 1861),\n",
       " (b'increase', 1860),\n",
       " (b'highest', 1859),\n",
       " (b'sent', 1858),\n",
       " (b'finally', 1858),\n",
       " (b'board', 1857),\n",
       " (b'don', 1856),\n",
       " (b'notable', 1854),\n",
       " (b'read', 1852),\n",
       " (b'methods', 1852),\n",
       " (b'recently', 1851),\n",
       " (b'bit', 1849),\n",
       " (b'involved', 1843),\n",
       " (b'variety', 1843),\n",
       " (b'call', 1842),\n",
       " (b'democratic', 1841),\n",
       " (b'ten', 1840),\n",
       " (b'served', 1836),\n",
       " (b'minor', 1830),\n",
       " (b'hard', 1828),\n",
       " (b'birth', 1823),\n",
       " (b'objects', 1823),\n",
       " (b'nuclear', 1819),\n",
       " (b'increased', 1819),\n",
       " (b'section', 1813),\n",
       " (b'street', 1812),\n",
       " (b'windows', 1811),\n",
       " (b'relatively', 1810),\n",
       " (b'car', 1809),\n",
       " (b'move', 1808),\n",
       " (b'create', 1807),\n",
       " (b'returned', 1807),\n",
       " (b'bank', 1806),\n",
       " (b'conditions', 1805),\n",
       " (b'operation', 1802),\n",
       " (b'adopted', 1801),\n",
       " (b'relationship', 1801),\n",
       " (b'christ', 1800),\n",
       " (b'hall', 1797),\n",
       " (b'appear', 1794),\n",
       " (b'rest', 1790),\n",
       " (b'child', 1789),\n",
       " (b'element', 1787),\n",
       " (b'appears', 1786),\n",
       " (b'takes', 1783),\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove samples with less than 'min_occurrence' occurrences\n",
    "for i in range(len(count) - 1, -1, -1):\n",
    "    if count[i][1] < min_occurrence:\n",
    "        count.pop(i)\n",
    "    else:\n",
    "        # The collection is ordered, so stop when 'min_occurrence' is reached\n",
    "        break\n",
    "# Compute the vocabulary size\n",
    "vocabulary_size = len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47135"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'anarchism',\n",
       " b'originated',\n",
       " b'as',\n",
       " b'a',\n",
       " b'term',\n",
       " b'of',\n",
       " b'abuse',\n",
       " b'first',\n",
       " b'used',\n",
       " b'against',\n",
       " b'early',\n",
       " b'working',\n",
       " b'class',\n",
       " b'radicals',\n",
       " b'including',\n",
       " b'the',\n",
       " b'diggers',\n",
       " b'of',\n",
       " b'the',\n",
       " b'english',\n",
       " b'revolution',\n",
       " b'and',\n",
       " b'the',\n",
       " b'sans',\n",
       " b'culottes',\n",
       " b'of',\n",
       " b'the',\n",
       " b'french',\n",
       " b'revolution',\n",
       " b'whilst',\n",
       " b'the',\n",
       " b'term',\n",
       " b'is',\n",
       " b'still',\n",
       " b'used',\n",
       " b'in',\n",
       " b'a',\n",
       " b'pejorative',\n",
       " b'way',\n",
       " b'to',\n",
       " b'describe',\n",
       " b'any',\n",
       " b'act',\n",
       " b'that',\n",
       " b'used',\n",
       " b'violent',\n",
       " b'means',\n",
       " b'to',\n",
       " b'destroy',\n",
       " b'the',\n",
       " b'organization',\n",
       " b'of',\n",
       " b'society',\n",
       " b'it',\n",
       " b'has',\n",
       " b'also',\n",
       " b'been',\n",
       " b'taken',\n",
       " b'up',\n",
       " b'as',\n",
       " b'a',\n",
       " b'positive',\n",
       " b'label',\n",
       " b'by',\n",
       " b'self',\n",
       " b'defined',\n",
       " b'anarchists',\n",
       " b'the',\n",
       " b'word',\n",
       " b'anarchism',\n",
       " b'is',\n",
       " b'derived',\n",
       " b'from',\n",
       " b'the',\n",
       " b'greek',\n",
       " b'without',\n",
       " b'archons',\n",
       " b'ruler',\n",
       " b'chief',\n",
       " b'king',\n",
       " b'anarchism',\n",
       " b'as',\n",
       " b'a',\n",
       " b'political',\n",
       " b'philosophy',\n",
       " b'is',\n",
       " b'the',\n",
       " b'belief',\n",
       " b'that',\n",
       " b'rulers',\n",
       " b'are',\n",
       " b'unnecessary',\n",
       " b'and',\n",
       " b'should',\n",
       " b'be',\n",
       " b'abolished',\n",
       " b'although',\n",
       " b'there',\n",
       " b'are',\n",
       " b'differing',\n",
       " b'interpretations',\n",
       " b'of',\n",
       " b'what',\n",
       " b'this',\n",
       " b'means',\n",
       " b'anarchism',\n",
       " b'also',\n",
       " b'refers',\n",
       " b'to',\n",
       " b'related',\n",
       " b'social',\n",
       " b'movements',\n",
       " b'that',\n",
       " b'advocate',\n",
       " b'the',\n",
       " b'elimination',\n",
       " b'of',\n",
       " b'authoritarian',\n",
       " b'institutions',\n",
       " b'particularly',\n",
       " b'the',\n",
       " b'state',\n",
       " b'the',\n",
       " b'word',\n",
       " b'anarchy',\n",
       " b'as',\n",
       " b'most',\n",
       " b'anarchists',\n",
       " b'use',\n",
       " b'it',\n",
       " b'does',\n",
       " b'not',\n",
       " b'imply',\n",
       " b'chaos',\n",
       " b'nihilism',\n",
       " b'or',\n",
       " b'anomie',\n",
       " b'but',\n",
       " b'rather',\n",
       " b'a',\n",
       " b'harmonious',\n",
       " b'anti',\n",
       " b'authoritarian',\n",
       " b'society',\n",
       " b'in',\n",
       " b'place',\n",
       " b'of',\n",
       " b'what',\n",
       " b'are',\n",
       " b'regarded',\n",
       " b'as',\n",
       " b'authoritarian',\n",
       " b'political',\n",
       " b'structures',\n",
       " b'and',\n",
       " b'coercive',\n",
       " b'economic',\n",
       " b'institutions',\n",
       " b'anarchists',\n",
       " b'advocate',\n",
       " b'social',\n",
       " b'relations',\n",
       " b'based',\n",
       " b'upon',\n",
       " b'voluntary',\n",
       " b'association',\n",
       " b'of',\n",
       " b'autonomous',\n",
       " b'individuals',\n",
       " b'mutual',\n",
       " b'aid',\n",
       " b'and',\n",
       " b'self',\n",
       " b'governance',\n",
       " b'while',\n",
       " b'anarchism',\n",
       " b'is',\n",
       " b'most',\n",
       " b'easily',\n",
       " b'defined',\n",
       " b'by',\n",
       " b'what',\n",
       " b'it',\n",
       " b'is',\n",
       " b'against',\n",
       " b'anarchists',\n",
       " b'also',\n",
       " b'offer',\n",
       " b'positive',\n",
       " b'visions',\n",
       " b'of',\n",
       " b'what',\n",
       " b'they',\n",
       " b'believe',\n",
       " b'to',\n",
       " b'be',\n",
       " b'a',\n",
       " b'truly',\n",
       " b'free',\n",
       " b'society',\n",
       " b'however',\n",
       " b'ideas',\n",
       " b'about',\n",
       " b'how',\n",
       " b'an',\n",
       " b'anarchist',\n",
       " b'society',\n",
       " b'might',\n",
       " b'work',\n",
       " b'vary',\n",
       " b'considerably',\n",
       " b'especially',\n",
       " b'with',\n",
       " b'respect',\n",
       " b'to',\n",
       " b'economics',\n",
       " b'there',\n",
       " b'is',\n",
       " b'also',\n",
       " b'disagreement',\n",
       " b'about',\n",
       " b'how',\n",
       " b'a',\n",
       " b'free',\n",
       " b'society',\n",
       " b'might',\n",
       " b'be',\n",
       " b'brought',\n",
       " b'about',\n",
       " b'origins',\n",
       " b'and',\n",
       " b'predecessors',\n",
       " b'kropotkin',\n",
       " b'and',\n",
       " b'others',\n",
       " b'argue',\n",
       " b'that',\n",
       " b'before',\n",
       " b'recorded',\n",
       " b'history',\n",
       " b'human',\n",
       " b'society',\n",
       " b'was',\n",
       " b'organized',\n",
       " b'on',\n",
       " b'anarchist',\n",
       " b'principles',\n",
       " b'most',\n",
       " b'anthropologists',\n",
       " b'follow',\n",
       " b'kropotkin',\n",
       " b'and',\n",
       " b'engels',\n",
       " b'in',\n",
       " b'believing',\n",
       " b'that',\n",
       " b'hunter',\n",
       " b'gatherer',\n",
       " b'bands',\n",
       " b'were',\n",
       " b'egalitarian',\n",
       " b'and',\n",
       " b'lacked',\n",
       " b'division',\n",
       " b'of',\n",
       " b'labour',\n",
       " b'accumulated',\n",
       " b'wealth',\n",
       " b'or',\n",
       " b'decreed',\n",
       " b'law',\n",
       " b'and',\n",
       " b'had',\n",
       " b'equal',\n",
       " b'access',\n",
       " b'to',\n",
       " b'resources',\n",
       " b'william',\n",
       " b'godwin',\n",
       " b'anarchists',\n",
       " b'including',\n",
       " b'the',\n",
       " b'the',\n",
       " b'anarchy',\n",
       " b'organisation',\n",
       " b'and',\n",
       " b'rothbard',\n",
       " b'find',\n",
       " b'anarchist',\n",
       " b'attitudes',\n",
       " b'in',\n",
       " b'taoism',\n",
       " b'from',\n",
       " b'ancient',\n",
       " b'china',\n",
       " b'kropotkin',\n",
       " b'found',\n",
       " b'similar',\n",
       " b'ideas',\n",
       " b'in',\n",
       " b'stoic',\n",
       " b'zeno',\n",
       " b'of',\n",
       " b'citium',\n",
       " b'according',\n",
       " b'to',\n",
       " b'kropotkin',\n",
       " b'zeno',\n",
       " b'repudiated',\n",
       " b'the',\n",
       " b'omnipotence',\n",
       " b'of',\n",
       " b'the',\n",
       " b'state',\n",
       " b'its',\n",
       " b'intervention',\n",
       " b'and',\n",
       " b'regimentation',\n",
       " b'and',\n",
       " b'proclaimed',\n",
       " b'the',\n",
       " b'sovereignty',\n",
       " b'of',\n",
       " b'the',\n",
       " b'moral',\n",
       " b'law',\n",
       " b'of',\n",
       " b'the',\n",
       " b'individual',\n",
       " b'the',\n",
       " b'anabaptists',\n",
       " b'of',\n",
       " b'one',\n",
       " b'six',\n",
       " b'th',\n",
       " b'century',\n",
       " b'europe',\n",
       " b'are',\n",
       " b'sometimes',\n",
       " b'considered',\n",
       " b'to',\n",
       " b'be',\n",
       " b'religious',\n",
       " b'forerunners',\n",
       " b'of',\n",
       " b'modern',\n",
       " b'anarchism',\n",
       " b'bertrand',\n",
       " b'russell',\n",
       " b'in',\n",
       " b'his',\n",
       " b'history',\n",
       " b'of',\n",
       " b'western',\n",
       " b'philosophy',\n",
       " b'writes',\n",
       " b'that',\n",
       " b'the',\n",
       " b'anabaptists',\n",
       " b'repudiated',\n",
       " b'all',\n",
       " b'law',\n",
       " b'since',\n",
       " b'they',\n",
       " b'held',\n",
       " b'that',\n",
       " b'the',\n",
       " b'good',\n",
       " b'man',\n",
       " b'will',\n",
       " b'be',\n",
       " b'guided',\n",
       " b'at',\n",
       " b'every',\n",
       " b'moment',\n",
       " b'by',\n",
       " b'the',\n",
       " b'holy',\n",
       " b'spirit',\n",
       " b'from',\n",
       " b'this',\n",
       " b'premise',\n",
       " b'they',\n",
       " b'arrive',\n",
       " b'at',\n",
       " b'communism',\n",
       " b'the',\n",
       " b'diggers',\n",
       " b'or',\n",
       " b'true',\n",
       " b'levellers',\n",
       " b'were',\n",
       " b'an',\n",
       " b'early',\n",
       " b'communistic',\n",
       " b'movement',\n",
       " b'during',\n",
       " b'the',\n",
       " b'time',\n",
       " b'of',\n",
       " b'the',\n",
       " b'english',\n",
       " b'civil',\n",
       " b'war',\n",
       " b'and',\n",
       " b'are',\n",
       " b'considered',\n",
       " b'by',\n",
       " b'some',\n",
       " b'as',\n",
       " b'forerunners',\n",
       " b'of',\n",
       " b'modern',\n",
       " b'anarchism',\n",
       " b'in',\n",
       " b'the',\n",
       " b'modern',\n",
       " b'era',\n",
       " b'the',\n",
       " b'first',\n",
       " b'to',\n",
       " b'use',\n",
       " b'the',\n",
       " b'term',\n",
       " b'to',\n",
       " b'mean',\n",
       " b'something',\n",
       " b'other',\n",
       " b'than',\n",
       " b'chaos',\n",
       " b'was',\n",
       " b'louis',\n",
       " b'armand',\n",
       " b'baron',\n",
       " b'de',\n",
       " b'lahontan',\n",
       " b'in',\n",
       " b'his',\n",
       " b'nouveaux',\n",
       " b'voyages',\n",
       " b'dans',\n",
       " b'l',\n",
       " b'am',\n",
       " b'rique',\n",
       " b'septentrionale',\n",
       " b'one',\n",
       " b'seven',\n",
       " b'zero',\n",
       " b'three',\n",
       " b'where',\n",
       " b'he',\n",
       " b'described',\n",
       " b'the',\n",
       " b'indigenous',\n",
       " b'american',\n",
       " b'society',\n",
       " b'which',\n",
       " b'had',\n",
       " b'no',\n",
       " b'state',\n",
       " b'laws',\n",
       " b'prisons',\n",
       " b'priests',\n",
       " b'or',\n",
       " b'private',\n",
       " b'property',\n",
       " b'as',\n",
       " b'being',\n",
       " b'in',\n",
       " b'anarchy',\n",
       " b'russell',\n",
       " b'means',\n",
       " b'a',\n",
       " b'libertarian',\n",
       " b'and',\n",
       " b'leader',\n",
       " b'in',\n",
       " b'the',\n",
       " b'american',\n",
       " b'indian',\n",
       " b'movement',\n",
       " b'has',\n",
       " b'repeatedly',\n",
       " b'stated',\n",
       " b'that',\n",
       " b'he',\n",
       " b'is',\n",
       " b'an',\n",
       " b'anarchist',\n",
       " b'and',\n",
       " b'so',\n",
       " b'are',\n",
       " b'all',\n",
       " b'his',\n",
       " b'ancestors',\n",
       " b'in',\n",
       " b'one',\n",
       " b'seven',\n",
       " b'nine',\n",
       " b'three',\n",
       " b'in',\n",
       " b'the',\n",
       " b'thick',\n",
       " b'of',\n",
       " b'the',\n",
       " b'french',\n",
       " b'revolution',\n",
       " b'william',\n",
       " b'godwin',\n",
       " b'published',\n",
       " b'an',\n",
       " b'enquiry',\n",
       " b'concerning',\n",
       " b'political',\n",
       " b'justice',\n",
       " b'although',\n",
       " b'godwin',\n",
       " b'did',\n",
       " b'not',\n",
       " b'use',\n",
       " b'the',\n",
       " b'word',\n",
       " b'anarchism',\n",
       " b'many',\n",
       " b'later',\n",
       " b'anarchists',\n",
       " b'have',\n",
       " b'regarded',\n",
       " b'this',\n",
       " b'book',\n",
       " b'as',\n",
       " b'the',\n",
       " b'first',\n",
       " b'major',\n",
       " b'anarchist',\n",
       " b'text',\n",
       " b'and',\n",
       " b'godwin',\n",
       " b'as',\n",
       " b'the',\n",
       " b'founder',\n",
       " b'of',\n",
       " b'philosophical',\n",
       " b'anarchism',\n",
       " b'but',\n",
       " b'at',\n",
       " b'this',\n",
       " b'point',\n",
       " b'no',\n",
       " b'anarchist',\n",
       " b'movement',\n",
       " b'yet',\n",
       " b'existed',\n",
       " b'and',\n",
       " b'the',\n",
       " b'term',\n",
       " b'anarchiste',\n",
       " b'was',\n",
       " b'known',\n",
       " b'mainly',\n",
       " b'as',\n",
       " b'an',\n",
       " b'insult',\n",
       " b'hurled',\n",
       " b'by',\n",
       " b'the',\n",
       " b'bourgeois',\n",
       " b'girondins',\n",
       " b'at',\n",
       " b'more',\n",
       " b'radical',\n",
       " b'elements',\n",
       " b'in',\n",
       " b'the',\n",
       " b'french',\n",
       " b'revolution',\n",
       " b'the',\n",
       " b'first',\n",
       " b'self',\n",
       " b'labelled',\n",
       " b'anarchist',\n",
       " b'pierre',\n",
       " b'joseph',\n",
       " b'proudhon',\n",
       " b'it',\n",
       " b'is',\n",
       " b'commonly',\n",
       " b'held',\n",
       " b'that',\n",
       " b'it',\n",
       " b'wasn',\n",
       " b't',\n",
       " b'until',\n",
       " b'pierre',\n",
       " b'joseph',\n",
       " b'proudhon',\n",
       " b'published',\n",
       " b'what',\n",
       " b'is',\n",
       " b'property',\n",
       " b'in',\n",
       " b'one',\n",
       " b'eight',\n",
       " b'four',\n",
       " b'zero',\n",
       " b'that',\n",
       " b'the',\n",
       " b'term',\n",
       " b'anarchist',\n",
       " b'was',\n",
       " b'adopted',\n",
       " b'as',\n",
       " b'a',\n",
       " b'self',\n",
       " b'description',\n",
       " b'it',\n",
       " b'is',\n",
       " b'for',\n",
       " b'this',\n",
       " b'reason',\n",
       " b'that',\n",
       " b'some',\n",
       " b'claim',\n",
       " b'proudhon',\n",
       " b'as',\n",
       " b'the',\n",
       " b'founder',\n",
       " b'of',\n",
       " b'modern',\n",
       " b'anarchist',\n",
       " b'theory',\n",
       " b'in',\n",
       " b'what',\n",
       " b'is',\n",
       " b'property',\n",
       " b'proudhon',\n",
       " b'answers',\n",
       " b'with',\n",
       " b'the',\n",
       " b'famous',\n",
       " b'accusation',\n",
       " b'property',\n",
       " b'is',\n",
       " b'theft',\n",
       " b'in',\n",
       " b'this',\n",
       " b'work',\n",
       " b'he',\n",
       " b'opposed',\n",
       " b'the',\n",
       " b'institution',\n",
       " b'of',\n",
       " b'decreed',\n",
       " b'property',\n",
       " b'propri',\n",
       " b't',\n",
       " b'where',\n",
       " b'owners',\n",
       " b'have',\n",
       " b'complete',\n",
       " b'rights',\n",
       " b'to',\n",
       " b'use',\n",
       " b'and',\n",
       " b'abuse',\n",
       " b'their',\n",
       " b'property',\n",
       " b'as',\n",
       " b'they',\n",
       " b'wish',\n",
       " b'such',\n",
       " b'as',\n",
       " b'exploiting',\n",
       " b'workers',\n",
       " b'for',\n",
       " b'profit',\n",
       " b'in',\n",
       " b'its',\n",
       " b'place',\n",
       " b'proudhon',\n",
       " b'supported',\n",
       " b'what',\n",
       " b'he',\n",
       " b'called',\n",
       " b'possession',\n",
       " b'individuals',\n",
       " b'can',\n",
       " b'have',\n",
       " b'limited',\n",
       " b'rights',\n",
       " b'to',\n",
       " b'use',\n",
       " b'resources',\n",
       " b'capital',\n",
       " b'and',\n",
       " b'goods',\n",
       " b'in',\n",
       " b'accordance',\n",
       " b'with',\n",
       " b'principles',\n",
       " b'of',\n",
       " b'equality',\n",
       " b'and',\n",
       " b'justice',\n",
       " b'proudhon',\n",
       " b's',\n",
       " b'vision',\n",
       " b'of',\n",
       " b'anarchy',\n",
       " b'which',\n",
       " b'he',\n",
       " b'called',\n",
       " b'mutualism',\n",
       " b'mutuellisme',\n",
       " b'involved',\n",
       " b'an',\n",
       " b'exchange',\n",
       " b'economy',\n",
       " b'where',\n",
       " b'individuals',\n",
       " b'and',\n",
       " b'groups',\n",
       " b'could',\n",
       " b'trade',\n",
       " b'the',\n",
       " b'products',\n",
       " b'of',\n",
       " b'their',\n",
       " b'labor',\n",
       " b'using',\n",
       " b'labor',\n",
       " b'notes',\n",
       " b'which',\n",
       " b'represented',\n",
       " b'the',\n",
       " b'amount',\n",
       " b'of',\n",
       " b'working',\n",
       " b'time',\n",
       " b'involved',\n",
       " b'in',\n",
       " b'production',\n",
       " b'this',\n",
       " b'would',\n",
       " b'ensure',\n",
       " b'that',\n",
       " b'no',\n",
       " b'one',\n",
       " b'would',\n",
       " b'profit',\n",
       " b'from',\n",
       " b'the',\n",
       " b'labor',\n",
       " b'of',\n",
       " b'others',\n",
       " b'workers',\n",
       " b'could',\n",
       " b'freely',\n",
       " b'join',\n",
       " b'together',\n",
       " b'in',\n",
       " b'co',\n",
       " b'operative',\n",
       " b'workshops',\n",
       " b'an',\n",
       " b'interest',\n",
       " b'free',\n",
       " b'bank',\n",
       " b'would',\n",
       " b'be',\n",
       " b'set',\n",
       " b'up',\n",
       " b'to',\n",
       " b'provide',\n",
       " b'everyone',\n",
       " b'with',\n",
       " b'access',\n",
       " b'to',\n",
       " b'the',\n",
       " b'means',\n",
       " b'of',\n",
       " b'production',\n",
       " b'proudhon',\n",
       " b's',\n",
       " b'ideas',\n",
       " b'were',\n",
       " b'influential',\n",
       " b'within',\n",
       " b'french',\n",
       " b'working',\n",
       " b'class',\n",
       " b'movements',\n",
       " b'and',\n",
       " b'his',\n",
       " b'followers',\n",
       " b'were',\n",
       " b'active',\n",
       " b'in',\n",
       " b'the',\n",
       " b'revolution',\n",
       " b'of',\n",
       " b'one',\n",
       " b'eight',\n",
       " b'four',\n",
       " b'eight',\n",
       " b'in',\n",
       " b'france',\n",
       " b'proudhon',\n",
       " b's',\n",
       " b'philosophy',\n",
       " b'of',\n",
       " b'property',\n",
       " b'is',\n",
       " b'complex',\n",
       " b'it',\n",
       " b'was',\n",
       " b'developed',\n",
       " b'in',\n",
       " b'a',\n",
       " b'number',\n",
       " b'of',\n",
       " b'works',\n",
       " b'over',\n",
       " b'his',\n",
       " b'lifetime',\n",
       " b'and',\n",
       " b'there',\n",
       " b'are',\n",
       " b'differing',\n",
       " b'interpretations',\n",
       " b'of',\n",
       " b'some',\n",
       " b'of',\n",
       " b'his',\n",
       " b'ideas',\n",
       " b'for',\n",
       " b'more',\n",
       " b'detailed',\n",
       " b'discussion',\n",
       " b'see',\n",
       " b'here',\n",
       " b'max',\n",
       " b'stirner',\n",
       " b's',\n",
       " b'egoism',\n",
       " b'in',\n",
       " b'his',\n",
       " b'the',\n",
       " b'ego',\n",
       " b'and',\n",
       " b'its',\n",
       " b'own',\n",
       " b'stirner',\n",
       " b'argued',\n",
       " b'that',\n",
       " b'most',\n",
       " b'commonly',\n",
       " b'accepted',\n",
       " b'social',\n",
       " b'institutions',\n",
       " b'including',\n",
       " b'the',\n",
       " b'notion',\n",
       " b'of',\n",
       " b'state',\n",
       " b'property',\n",
       " b'as',\n",
       " b'a',\n",
       " b'right',\n",
       " b'natural',\n",
       " b'rights',\n",
       " b'in',\n",
       " b'general',\n",
       " b'and',\n",
       " b'the',\n",
       " b'very',\n",
       " b'notion',\n",
       " b'of',\n",
       " b'society',\n",
       " b'were',\n",
       " b'mere',\n",
       " b'illusions',\n",
       " b'or',\n",
       " b'ghosts',\n",
       " b'in',\n",
       " b'the',\n",
       " b'mind',\n",
       " b'saying',\n",
       " b'of',\n",
       " b'society',\n",
       " b'that',\n",
       " b'the',\n",
       " b'individuals',\n",
       " b'are',\n",
       " b'its',\n",
       " b'reality',\n",
       " b'he',\n",
       " b'advocated',\n",
       " b'egoism',\n",
       " b'and',\n",
       " b'a',\n",
       " b'form',\n",
       " b'of',\n",
       " b'amoralism',\n",
       " b'in',\n",
       " b'which',\n",
       " b'individuals',\n",
       " b'would',\n",
       " b'unite',\n",
       " b'in',\n",
       " b'associations',\n",
       " b'of',\n",
       " b'egoists',\n",
       " b'only',\n",
       " b'when',\n",
       " b'it',\n",
       " b'was',\n",
       " b'in',\n",
       " b'their',\n",
       " b'self',\n",
       " b'interest',\n",
       " b'to',\n",
       " b'do',\n",
       " b'so',\n",
       " b'for',\n",
       " b'him',\n",
       " b'property',\n",
       " b'simply',\n",
       " b'comes',\n",
       " b'about',\n",
       " b'through',\n",
       " b'might',\n",
       " b'whoever',\n",
       " b'knows',\n",
       " b'how',\n",
       " b'to',\n",
       " b'take',\n",
       " b'to',\n",
       " b'defend',\n",
       " b'the',\n",
       " b'thing',\n",
       " b'to',\n",
       " b'him',\n",
       " b'belongs',\n",
       " b'property',\n",
       " b'and',\n",
       " b'what',\n",
       " b'i',\n",
       " b'have',\n",
       " b'in',\n",
       " b'my',\n",
       " b'power',\n",
       " b'that',\n",
       " b'is',\n",
       " b'my',\n",
       " b'own',\n",
       " b'so',\n",
       " b'long',\n",
       " b'as',\n",
       " b'i',\n",
       " b'assert',\n",
       " b'myself',\n",
       " b'as',\n",
       " b'holder',\n",
       " b'i',\n",
       " b'am',\n",
       " b'the',\n",
       " b'proprietor',\n",
       " b'of',\n",
       " b'the',\n",
       " b'thing',\n",
       " b'stirner',\n",
       " b'never',\n",
       " b'called',\n",
       " b'himself',\n",
       " b'an',\n",
       " b'anarchist',\n",
       " b'he',\n",
       " b'accepted',\n",
       " b'only',\n",
       " b'the',\n",
       " b'label',\n",
       " b'egoist',\n",
       " b'nevertheless',\n",
       " b'his',\n",
       " b'ideas',\n",
       " b'were',\n",
       " b'influential',\n",
       " b'on',\n",
       " b'many',\n",
       " b'individualistically',\n",
       " b'inclined',\n",
       " b'anarchists',\n",
       " b'although',\n",
       " b'interpretations',\n",
       " b'of',\n",
       " b'his',\n",
       " b'thought',\n",
       " b'are',\n",
       " b'diverse',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count: 17005207\n",
      "Unique words: 253854\n",
      "Vocabulary size: 47135\n",
      "Most common words: [('UNK', 444176), (b'the', 1061396), (b'of', 593677), (b'and', 416629), (b'one', 411764), (b'in', 372201), (b'a', 325873), (b'to', 316376), (b'zero', 264975), (b'nine', 250430)]\n"
     ]
    }
   ],
   "source": [
    "# Assign an id to each word\n",
    "word2id = dict()\n",
    "for i, (word, _)in enumerate(count):\n",
    "    word2id[word] = i\n",
    "\n",
    "data = list()#储存每个词的index\n",
    "unk_count = 0\n",
    "for word in text_words:\n",
    "    # Retrieve a word id, or assign it index 0 ('UNK') if not in dictionary\n",
    "    index = word2id.get(word, 0)\n",
    "    if index == 0:\n",
    "        unk_count += 1\n",
    "    data.append(index)\n",
    "count[0] = ('UNK', unk_count)\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "\n",
    "print(\"Words count:\", len(text_words))\n",
    "print(\"Unique words:\", len(set(text_words)))\n",
    "print(\"Vocabulary size:\", vocabulary_size)\n",
    "print(\"Most common words:\", count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "# Generate training batch for the skip-gram model\n",
    "def next_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    # get window size (words left and right + current one)\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]# 除去中心词\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ryan Wu\\Anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "X = tf.placeholder(tf.int32, shape=[None])\n",
    "# Input label\n",
    "Y = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "\n",
    "# Ensure the following ops & var are assigned on CPU\n",
    "# (some ops are not compatible on GPU)\n",
    "with tf.device('/cpu:0'):\n",
    "    # Create the embedding variable (each row represent a word embedding vector)\n",
    "    embedding = tf.Variable(tf.random_normal([vocabulary_size, embedding_size]))\n",
    "    # Lookup the corresponding embedding vectors for each sample in X\n",
    "    X_embed = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(tf.random_normal([vocabulary_size, embedding_size]))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Compute the average NCE loss for the batch\n",
    "loss_op = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(weights=nce_weights,\n",
    "                   biases=nce_biases,\n",
    "                   labels=Y,\n",
    "                   inputs=X_embed,\n",
    "                   num_sampled=num_sampled,\n",
    "                   num_classes=vocabulary_size))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluation\n",
    "# Compute the cosine similarity between input data embedding and every embedding vectors\n",
    "X_embed_norm = X_embed / tf.sqrt(tf.reduce_sum(tf.square(X_embed)))\n",
    "embedding_norm = embedding / tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True))\n",
    "cosine_sim_op = tf.matmul(X_embed_norm, embedding_norm, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Average Loss= 521.8065\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'surreal', b'amusing', b'jew', b'thoughts', b'variational', b'bronzes', b'superficial', b'natasha',\n",
      "\"of\" nearest neighbors: b'accepts', b'looked', b'chaplin', b'gambler', b'lao', b'renewal', b'sas', b'lauder',\n",
      "\"going\" nearest neighbors: b'rockwell', b'serial', b'menlo', b'khz', b'halides', b'bleach', b'browski', b'here',\n",
      "\"hardware\" nearest neighbors: b'campus', b'concatenation', b'conectiva', b'roger', b'eightfold', b'corrode', b'onion', b'tomino',\n",
      "\"american\" nearest neighbors: b'ride', b'bantu', b'abolishment', b'jdk', b'antiparticle', b'agreement', b'numerical', b'elway',\n",
      "\"britain\" nearest neighbors: b'fundy', b'jamison', b'fading', b'sco', b'fon', b'define', b'equus', b'macedon',\n",
      "Step 10000, Average Loss= 198.8738\n",
      "Step 20000, Average Loss= 93.6720\n",
      "Step 30000, Average Loss= 64.9207\n",
      "Step 40000, Average Loss= 50.8529\n",
      "Step 50000, Average Loss= 41.4173\n",
      "Step 60000, Average Loss= 36.3376\n",
      "Step 70000, Average Loss= 31.8888\n",
      "Step 80000, Average Loss= 29.5638\n",
      "Step 90000, Average Loss= 27.4067\n",
      "Step 100000, Average Loss= 24.7743\n",
      "Step 110000, Average Loss= 23.3918\n",
      "Step 120000, Average Loss= 21.1894\n",
      "Step 130000, Average Loss= 20.3342\n",
      "Step 140000, Average Loss= 19.3134\n",
      "Step 150000, Average Loss= 18.7551\n",
      "Step 160000, Average Loss= 17.7024\n",
      "Step 170000, Average Loss= 16.9829\n",
      "Step 180000, Average Loss= 16.3435\n",
      "Step 190000, Average Loss= 15.3900\n",
      "Step 200000, Average Loss= 14.5908\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'three', b'seven', b'six', b'four', b'eight', b'two', b'nine', b'zero',\n",
      "\"of\" nearest neighbors: b'the', b'a', b'with', b'and', b'by', b'in', b'is', b'to',\n",
      "\"going\" nearest neighbors: b'them', b'called', b'while', b'used', b'so', b'or', b'both', b'in',\n",
      "\"hardware\" nearest neighbors: b'apollo', b'campus', b'bc', b'examples', b'clinton', b'eight', b'six', b'oxford',\n",
      "\"american\" nearest neighbors: b'and', UNK, b'in', b'after', b's', b'about', b'from', b'may',\n",
      "\"britain\" nearest neighbors: b'with', b'years', UNK, b'but', b'also', b'used', b'see', b'over',\n",
      "Step 210000, Average Loss= 14.4425\n",
      "Step 220000, Average Loss= 14.9012\n",
      "Step 230000, Average Loss= 13.6235\n",
      "Step 240000, Average Loss= 14.0467\n",
      "Step 250000, Average Loss= 12.8043\n",
      "Step 260000, Average Loss= 12.9029\n",
      "Step 270000, Average Loss= 12.6223\n",
      "Step 280000, Average Loss= 11.7774\n",
      "Step 290000, Average Loss= 11.3169\n",
      "Step 300000, Average Loss= 11.4167\n",
      "Step 310000, Average Loss= 10.8247\n",
      "Step 320000, Average Loss= 10.8586\n",
      "Step 330000, Average Loss= 10.5550\n",
      "Step 340000, Average Loss= 10.4291\n",
      "Step 350000, Average Loss= 10.6934\n",
      "Step 360000, Average Loss= 10.3033\n",
      "Step 370000, Average Loss= 10.1580\n",
      "Step 380000, Average Loss= 10.0729\n",
      "Step 390000, Average Loss= 9.7426\n",
      "Step 400000, Average Loss= 9.6852\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'four', b'three', b'six', b'seven', b'two', b'eight', b'one', b'nine',\n",
      "\"of\" nearest neighbors: b'the', b'which', b'while', b'a', b'for', b'with', b'and', b'its',\n",
      "\"going\" nearest neighbors: b'called', b'now', b'known', b'up', b'see', b'list', b'were', b'by',\n",
      "\"hardware\" nearest neighbors: UNK, b'see', b's', b'while', b'one', b'such', b'known', b'was',\n",
      "\"american\" nearest neighbors: b'and', b'in', b'during', b'british', UNK, b's', b'including', b'from',\n",
      "\"britain\" nearest neighbors: b'first', b'since', b'like', b'who', b'him', b'example', b'on', b'british',\n",
      "Step 410000, Average Loss= 9.8043\n",
      "Step 420000, Average Loss= 9.2838\n",
      "Step 430000, Average Loss= 9.5335\n",
      "Step 440000, Average Loss= 9.2136\n",
      "Step 450000, Average Loss= 9.2466\n",
      "Step 460000, Average Loss= 8.9180\n",
      "Step 470000, Average Loss= 8.6436\n",
      "Step 480000, Average Loss= 8.7702\n",
      "Step 490000, Average Loss= 9.0352\n",
      "Step 500000, Average Loss= 8.8692\n",
      "Step 510000, Average Loss= 8.5773\n",
      "Step 520000, Average Loss= 8.6441\n",
      "Step 530000, Average Loss= 8.6241\n",
      "Step 540000, Average Loss= 8.3438\n",
      "Step 550000, Average Loss= 8.1874\n",
      "Step 560000, Average Loss= 8.1880\n",
      "Step 570000, Average Loss= 8.1331\n",
      "Step 580000, Average Loss= 7.9990\n",
      "Step 590000, Average Loss= 8.0771\n",
      "Step 600000, Average Loss= 7.8565\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'four', b'three', b'six', b'seven', b'eight', b'two', b'one', b'zero',\n",
      "\"of\" nearest neighbors: b'the', b'and', b'including', b'a', b'in', b'by', b'from', b'its',\n",
      "\"going\" nearest neighbors: b'is', b'military', b'the', b'list', b'and', b'their', b'called', b'example',\n",
      "\"hardware\" nearest neighbors: b'free', b'for', b'several', UNK, b'become', b'more', b'through', b'later',\n",
      "\"american\" nearest neighbors: b'english', b's', b'french', b'in', b'german', b'on', b'british', b'during',\n",
      "\"britain\" nearest neighbors: b'term', b'first', b'series', b'the', b'when', b'or', b'after', b'based',\n",
      "Step 610000, Average Loss= 7.9615\n",
      "Step 620000, Average Loss= 8.0701\n",
      "Step 630000, Average Loss= 7.8484\n",
      "Step 640000, Average Loss= 7.8957\n",
      "Step 650000, Average Loss= 7.6973\n",
      "Step 660000, Average Loss= 7.7324\n",
      "Step 670000, Average Loss= 7.6084\n",
      "Step 680000, Average Loss= 7.7710\n",
      "Step 690000, Average Loss= 7.5829\n",
      "Step 700000, Average Loss= 7.5655\n",
      "Step 710000, Average Loss= 7.4966\n",
      "Step 720000, Average Loss= 7.4452\n",
      "Step 730000, Average Loss= 7.3309\n",
      "Step 740000, Average Loss= 7.1600\n",
      "Step 750000, Average Loss= 7.4537\n",
      "Step 760000, Average Loss= 7.2907\n",
      "Step 770000, Average Loss= 7.4934\n",
      "Step 780000, Average Loss= 7.1819\n",
      "Step 790000, Average Loss= 7.2683\n",
      "Step 800000, Average Loss= 7.3011\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'four', b'six', b'three', b'seven', b'two', b'eight', b'one', b'nine',\n",
      "\"of\" nearest neighbors: b'and', b'the', b'a', b'in', b'for', b'while', b'including', b'by',\n",
      "\"going\" nearest neighbors: b'various', b'down', b'the', b'public', b'called', b'being', b'military', b'in',\n",
      "\"hardware\" nearest neighbors: b'for', b'become', b'led', b'set', b'including', b'of', b'based', b'high',\n",
      "\"american\" nearest neighbors: b'british', b'born', b'english', b'john', b'german', b'french', b'b', b'd',\n",
      "\"britain\" nearest neighbors: b'line', b'british', b'film', b'on', b'book', b'island', b'term', b'by',\n",
      "Step 810000, Average Loss= 7.1236\n",
      "Step 820000, Average Loss= 7.0255\n",
      "Step 830000, Average Loss= 7.1746\n",
      "Step 840000, Average Loss= 7.0142\n",
      "Step 850000, Average Loss= 7.0169\n",
      "Step 860000, Average Loss= 7.0360\n",
      "Step 870000, Average Loss= 6.9512\n",
      "Step 880000, Average Loss= 7.0817\n",
      "Step 890000, Average Loss= 7.0221\n",
      "Step 900000, Average Loss= 6.9787\n",
      "Step 910000, Average Loss= 7.0222\n",
      "Step 920000, Average Loss= 6.8184\n",
      "Step 930000, Average Loss= 6.8953\n",
      "Step 940000, Average Loss= 6.9394\n",
      "Step 950000, Average Loss= 6.7369\n",
      "Step 960000, Average Loss= 6.9170\n",
      "Step 970000, Average Loss= 6.7263\n",
      "Step 980000, Average Loss= 6.8638\n",
      "Step 990000, Average Loss= 6.6477\n",
      "Step 1000000, Average Loss= 6.5523\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'three', b'six', b'four', b'seven', b'eight', b'two', b'nine', b'zero',\n",
      "\"of\" nearest neighbors: b'the', b'first', b'from', b'including', b'and', b'became', b'with', b'second',\n",
      "\"going\" nearest neighbors: b'public', b'military', b'various', b'although', b'whose', b'each', b'down', b'main',\n",
      "\"hardware\" nearest neighbors: b'use', b'most', b'for', b'at', b'game', b'based', b'each', b'free',\n",
      "\"american\" nearest neighbors: b'b', b'born', b'd', UNK, b'john', b'nine', b'english', b'french',\n",
      "\"britain\" nearest neighbors: b'british', b'under', b'state', b'including', b'along', b'new', b'film', b'in',\n",
      "Step 1010000, Average Loss= 6.6093\n",
      "Step 1020000, Average Loss= 6.8293\n",
      "Step 1030000, Average Loss= 6.6697\n",
      "Step 1040000, Average Loss= 6.6896\n",
      "Step 1050000, Average Loss= 6.6377\n",
      "Step 1060000, Average Loss= 6.6722\n",
      "Step 1070000, Average Loss= 6.6675\n",
      "Step 1080000, Average Loss= 6.5329\n",
      "Step 1090000, Average Loss= 6.5603\n",
      "Step 1100000, Average Loss= 6.6314\n",
      "Step 1110000, Average Loss= 6.4858\n",
      "Step 1120000, Average Loss= 6.5700\n",
      "Step 1130000, Average Loss= 6.4548\n",
      "Step 1140000, Average Loss= 6.5512\n",
      "Step 1150000, Average Loss= 6.6403\n",
      "Step 1160000, Average Loss= 6.4996\n",
      "Step 1170000, Average Loss= 6.5341\n",
      "Step 1180000, Average Loss= 6.4448\n",
      "Step 1190000, Average Loss= 6.4779\n",
      "Step 1200000, Average Loss= 6.4184\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'four', b'six', b'three', b'seven', b'eight', b'two', b'nine', b'zero',\n",
      "\"of\" nearest neighbors: b'the', b'and', b'including', b'in', b'on', b'with', b'through', b'for',\n",
      "\"going\" nearest neighbors: b'whose', b'down', b'public', b'earth', b'by', b'here', b'each', b'women',\n",
      "\"hardware\" nearest neighbors: b'game', b'games', b'information', b'off', b'on', b'non', b'free', b'each',\n",
      "\"american\" nearest neighbors: b'born', b'john', b'german', b'd', b'b', b'english', b'british', UNK,\n",
      "\"britain\" nearest neighbors: b'under', b'british', b'first', b'the', b'including', b'following', b'second', b'former',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1210000, Average Loss= 6.5240\n",
      "Step 1220000, Average Loss= 6.3780\n",
      "Step 1230000, Average Loss= 6.4328\n",
      "Step 1240000, Average Loss= 6.4118\n",
      "Step 1250000, Average Loss= 6.3701\n",
      "Step 1260000, Average Loss= 6.3322\n",
      "Step 1270000, Average Loss= 6.0913\n",
      "Step 1280000, Average Loss= 6.3714\n",
      "Step 1290000, Average Loss= 6.3236\n",
      "Step 1300000, Average Loss= 6.4620\n",
      "Step 1310000, Average Loss= 6.2363\n",
      "Step 1320000, Average Loss= 6.3063\n",
      "Step 1330000, Average Loss= 6.3616\n",
      "Step 1340000, Average Loss= 6.2485\n",
      "Step 1350000, Average Loss= 6.2162\n",
      "Step 1360000, Average Loss= 6.2830\n",
      "Step 1370000, Average Loss= 6.2403\n",
      "Step 1380000, Average Loss= 6.2398\n",
      "Step 1390000, Average Loss= 6.2610\n",
      "Step 1400000, Average Loss= 6.2087\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'four', b'three', b'six', b'two', b'eight', b'seven', b'one', b'zero',\n",
      "\"of\" nearest neighbors: b'the', b'and', b'including', b'in', b'its', b'modern', b'for', b'by',\n",
      "\"going\" nearest neighbors: b'here', b'whose', b'women', b'down', b'public', b'earth', b'original', b'like',\n",
      "\"hardware\" nearest neighbors: b'games', b'game', b'for', b'code', b'become', b'free', b'information', b'computer',\n",
      "\"american\" nearest neighbors: b'french', b'film', b'english', b'german', b'born', b'british', b'john', b's',\n",
      "\"britain\" nearest neighbors: b'british', b'after', b'act', b'army', b'first', b'became', b'following', b'from',\n",
      "Step 1410000, Average Loss= 6.2768\n",
      "Step 1420000, Average Loss= 6.2976\n",
      "Step 1430000, Average Loss= 6.2578\n",
      "Step 1440000, Average Loss= 6.2669\n",
      "Step 1450000, Average Loss= 6.1529\n",
      "Step 1460000, Average Loss= 6.1830\n",
      "Step 1470000, Average Loss= 6.2177\n",
      "Step 1480000, Average Loss= 6.1345\n",
      "Step 1490000, Average Loss= 6.2430\n",
      "Step 1500000, Average Loss= 6.1370\n",
      "Step 1510000, Average Loss= 6.1889\n",
      "Step 1520000, Average Loss= 6.0638\n",
      "Step 1530000, Average Loss= 5.9603\n",
      "Step 1540000, Average Loss= 6.0173\n",
      "Step 1550000, Average Loss= 6.1980\n",
      "Step 1560000, Average Loss= 6.0851\n",
      "Step 1570000, Average Loss= 6.1569\n",
      "Step 1580000, Average Loss= 6.0004\n",
      "Step 1590000, Average Loss= 6.1034\n",
      "Step 1600000, Average Loss= 6.1328\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'four', b'three', b'six', b'two', b'seven', b'eight', b'zero', b'one',\n",
      "\"of\" nearest neighbors: b'and', b'the', b'including', b'through', b'from', b'in', b'its', b'with',\n",
      "\"going\" nearest neighbors: b'here', b'public', b'women', b'down', b'whose', b'once', b'out', b'various',\n",
      "\"hardware\" nearest neighbors: b'games', b'free', b'for', b'computer', b'set', b'game', b'code', b'information',\n",
      "\"american\" nearest neighbors: b'english', b'born', b'french', b'john', b'british', b'german', b'william', b'b',\n",
      "\"britain\" nearest neighbors: b'act', b'from', b'following', b'army', b'in', b'home', b'state', b'first',\n",
      "Step 1610000, Average Loss= 6.0449\n",
      "Step 1620000, Average Loss= 6.0100\n",
      "Step 1630000, Average Loss= 6.1096\n",
      "Step 1640000, Average Loss= 6.0138\n",
      "Step 1650000, Average Loss= 6.0786\n",
      "Step 1660000, Average Loss= 5.9837\n",
      "Step 1670000, Average Loss= 6.0677\n",
      "Step 1680000, Average Loss= 6.1323\n",
      "Step 1690000, Average Loss= 6.0746\n",
      "Step 1700000, Average Loss= 6.0507\n",
      "Step 1710000, Average Loss= 5.9976\n",
      "Step 1720000, Average Loss= 6.0230\n",
      "Step 1730000, Average Loss= 5.9798\n",
      "Step 1740000, Average Loss= 6.0813\n",
      "Step 1750000, Average Loss= 5.9398\n",
      "Step 1760000, Average Loss= 6.0325\n",
      "Step 1770000, Average Loss= 5.9829\n",
      "Step 1780000, Average Loss= 5.9807\n",
      "Step 1790000, Average Loss= 5.9567\n",
      "Step 1800000, Average Loss= 5.7153\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'four', b'three', b'six', b'seven', b'eight', b'two', b'nine', b'zero',\n",
      "\"of\" nearest neighbors: b'the', b'became', b'first', b'from', b'in', b'second', b'and', b'at',\n",
      "\"going\" nearest neighbors: b'here', b'once', b'earth', b'public', b'whose', b'various', b'either', b're',\n",
      "\"hardware\" nearest neighbors: b'games', b'use', b'code', b'information', b'for', b'computer', b'game', b'free',\n",
      "\"american\" nearest neighbors: b'b', b'born', b'd', b'actor', UNK, b'english', b'john', b'robert',\n",
      "\"britain\" nearest neighbors: b'british', b'first', b'act', b'along', b'led', b'forces', b'including', b'government',\n",
      "Step 1810000, Average Loss= 5.9450\n",
      "Step 1820000, Average Loss= 5.9493\n",
      "Step 1830000, Average Loss= 6.0274\n",
      "Step 1840000, Average Loss= 5.8667\n",
      "Step 1850000, Average Loss= 5.9414\n",
      "Step 1860000, Average Loss= 5.9654\n",
      "Step 1870000, Average Loss= 5.9084\n",
      "Step 1880000, Average Loss= 5.8849\n",
      "Step 1890000, Average Loss= 5.9128\n",
      "Step 1900000, Average Loss= 5.9095\n",
      "Step 1910000, Average Loss= 5.8902\n",
      "Step 1920000, Average Loss= 5.9418\n",
      "Step 1930000, Average Loss= 5.8740\n",
      "Step 1940000, Average Loss= 5.9328\n",
      "Step 1950000, Average Loss= 5.9868\n",
      "Step 1960000, Average Loss= 5.9168\n",
      "Step 1970000, Average Loss= 5.9143\n",
      "Step 1980000, Average Loss= 5.8547\n",
      "Step 1990000, Average Loss= 5.8927\n",
      "Step 2000000, Average Loss= 5.8516\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'four', b'six', b'three', b'seven', b'eight', b'two', b'nine', b'zero',\n",
      "\"of\" nearest neighbors: b'the', b'from', b'its', b'for', b'modern', b'and', b'in', b'first',\n",
      "\"going\" nearest neighbors: b'once', b'here', b'down', b'earth', b'still', b'separate', b'public', b'hand',\n",
      "\"hardware\" nearest neighbors: b'games', b'available', b'code', b'computer', b'game', b'information', b'free', b'software',\n",
      "\"american\" nearest neighbors: b'born', b'actor', b'john', b'd', b'robert', b'william', b'b', b'james',\n",
      "\"britain\" nearest neighbors: b'act', b'former', b'began', b'british', b'under', b'military', b'first', b'led',\n",
      "Step 2010000, Average Loss= 5.9069\n",
      "Step 2020000, Average Loss= 5.8844\n",
      "Step 2030000, Average Loss= 5.8849\n",
      "Step 2040000, Average Loss= 5.8665\n",
      "Step 2050000, Average Loss= 5.8305\n",
      "Step 2060000, Average Loss= 5.7111\n",
      "Step 2070000, Average Loss= 5.6847\n",
      "Step 2080000, Average Loss= 5.8994\n",
      "Step 2090000, Average Loss= 5.7937\n",
      "Step 2100000, Average Loss= 5.9161\n",
      "Step 2110000, Average Loss= 5.6997\n",
      "Step 2120000, Average Loss= 5.8066\n",
      "Step 2130000, Average Loss= 5.8745\n",
      "Step 2140000, Average Loss= 5.8033\n",
      "Step 2150000, Average Loss= 5.7407\n",
      "Step 2160000, Average Loss= 5.8378\n",
      "Step 2170000, Average Loss= 5.7725\n",
      "Step 2180000, Average Loss= 5.8193\n",
      "Step 2190000, Average Loss= 5.7805\n",
      "Step 2200000, Average Loss= 5.7953\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'four', b'three', b'six', b'seven', b'eight', b'two', b'zero', b'one',\n",
      "\"of\" nearest neighbors: b'the', b'and', b'in', b'its', b'from', b'form', b'through', b'a',\n",
      "\"going\" nearest neighbors: b'while', b'separate', b'once', b'by', b'earth', b'our', b'here', b'down',\n",
      "\"hardware\" nearest neighbors: b'games', b'computer', b'code', b'free', b'programs', b'available', b'software', b'game',\n",
      "\"american\" nearest neighbors: b'english', b'french', b'film', b'german', b'british', b'italian', b'author', b'born',\n",
      "\"britain\" nearest neighbors: b'act', b'british', b'established', b'former', b'first', b'state', b'began', b'government',\n",
      "Step 2210000, Average Loss= 5.8551\n",
      "Step 2220000, Average Loss= 5.8310\n",
      "Step 2230000, Average Loss= 5.7973\n",
      "Step 2240000, Average Loss= 5.8297\n",
      "Step 2250000, Average Loss= 5.7636\n",
      "Step 2260000, Average Loss= 5.7678\n",
      "Step 2270000, Average Loss= 5.8060\n",
      "Step 2280000, Average Loss= 5.7064\n",
      "Step 2290000, Average Loss= 5.8282\n",
      "Step 2300000, Average Loss= 5.7460\n",
      "Step 2310000, Average Loss= 5.8066\n",
      "Step 2320000, Average Loss= 5.6987\n",
      "Step 2330000, Average Loss= 5.5224\n",
      "Step 2340000, Average Loss= 5.6890\n",
      "Step 2350000, Average Loss= 5.7915\n",
      "Step 2360000, Average Loss= 5.7585\n",
      "Step 2370000, Average Loss= 5.6840\n",
      "Step 2380000, Average Loss= 5.7186\n",
      "Step 2390000, Average Loss= 5.7402\n",
      "Step 2400000, Average Loss= 5.7393\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'four', b'three', b'six', b'two', b'seven', b'eight', b'zero', b'nine',\n",
      "\"of\" nearest neighbors: b'the', b'and', b'in', b'including', b'from', b'within', b'through', b'group',\n",
      "\"going\" nearest neighbors: b'help', b'separate', b'down', b'were', b'once', b'hand', b'our', b'although',\n",
      "\"hardware\" nearest neighbors: b'games', b'programs', b'computer', b'run', b'available', b'free', b'game', b'code',\n",
      "\"american\" nearest neighbors: b'actor', b'english', b'author', b'writer', b'born', b'british', b'italian', b'german',\n",
      "\"britain\" nearest neighbors: b'government', b'act', b'independence', b'former', b'state', b'british', b'began', b'established',\n",
      "Step 2410000, Average Loss= 5.6631\n",
      "Step 2420000, Average Loss= 5.7166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2430000, Average Loss= 5.7137\n",
      "Step 2440000, Average Loss= 5.7181\n",
      "Step 2450000, Average Loss= 5.7517\n",
      "Step 2460000, Average Loss= 5.6651\n",
      "Step 2470000, Average Loss= 5.7205\n",
      "Step 2480000, Average Loss= 5.7987\n",
      "Step 2490000, Average Loss= 5.7186\n",
      "Step 2500000, Average Loss= 5.7330\n",
      "Step 2510000, Average Loss= 5.6686\n",
      "Step 2520000, Average Loss= 5.7100\n",
      "Step 2530000, Average Loss= 5.6541\n",
      "Step 2540000, Average Loss= 5.7321\n",
      "Step 2550000, Average Loss= 5.6767\n",
      "Step 2560000, Average Loss= 5.7031\n",
      "Step 2570000, Average Loss= 5.6856\n",
      "Step 2580000, Average Loss= 5.6663\n",
      "Step 2590000, Average Loss= 5.6261\n",
      "Step 2600000, Average Loss= 5.4499\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'four', b'three', b'six', b'seven', b'eight', b'nine', b'two', b'zero',\n",
      "\"of\" nearest neighbors: b'the', b'and', b'became', b'first', b'famous', b'part', b'for', b'including',\n",
      "\"going\" nearest neighbors: b'help', b'separate', b'down', b'your', b'their', b'instead', b'off', b'while',\n",
      "\"hardware\" nearest neighbors: b'programs', b'games', b'available', b'software', b'free', b'computer', b'run', b'code',\n",
      "\"american\" nearest neighbors: b'actor', b'born', b'writer', b'd', b'b', b'author', b'singer', b'john',\n",
      "\"britain\" nearest neighbors: b'the', b'established', b'began', b'government', b'act', b'independence', b'france', b'british',\n",
      "Step 2610000, Average Loss= 5.7058\n",
      "Step 2620000, Average Loss= 5.6276\n",
      "Step 2630000, Average Loss= 5.7332\n",
      "Step 2640000, Average Loss= 5.5601\n",
      "Step 2650000, Average Loss= 5.6394\n",
      "Step 2660000, Average Loss= 5.6995\n",
      "Step 2670000, Average Loss= 5.6348\n",
      "Step 2680000, Average Loss= 5.5835\n",
      "Step 2690000, Average Loss= 5.6816\n",
      "Step 2700000, Average Loss= 5.6217\n",
      "Step 2710000, Average Loss= 5.6476\n",
      "Step 2720000, Average Loss= 5.6631\n",
      "Step 2730000, Average Loss= 5.6089\n",
      "Step 2740000, Average Loss= 5.6841\n",
      "Step 2750000, Average Loss= 5.6821\n",
      "Step 2760000, Average Loss= 5.6450\n",
      "Step 2770000, Average Loss= 5.6810\n",
      "Step 2780000, Average Loss= 5.5968\n",
      "Step 2790000, Average Loss= 5.6277\n",
      "Step 2800000, Average Loss= 5.6452\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'four', b'six', b'three', b'eight', b'seven', b'nine', b'two', b'one',\n",
      "\"of\" nearest neighbors: b'the', b'in', b'and', b'from', b'includes', b'following', b'through', b'including',\n",
      "\"going\" nearest neighbors: b'help', b'separate', b'each', b'your', b'them', b'up', b'their', b'hand',\n",
      "\"hardware\" nearest neighbors: b'programs', b'available', b'computer', b'games', b'software', b'code', b'run', b'free',\n",
      "\"american\" nearest neighbors: b'writer', b'actor', b'singer', b'author', b'born', b'canadian', b'italian', b'john',\n",
      "\"britain\" nearest neighbors: b'independence', b'act', b'established', b'france', b'government', b'british', b'began', b'former',\n",
      "Step 2810000, Average Loss= 5.5764\n",
      "Step 2820000, Average Loss= 5.6816\n",
      "Step 2830000, Average Loss= 5.5840\n",
      "Step 2840000, Average Loss= 5.6712\n",
      "Step 2850000, Average Loss= 5.5461\n",
      "Step 2860000, Average Loss= 5.4382\n",
      "Step 2870000, Average Loss= 5.5209\n",
      "Step 2880000, Average Loss= 5.6604\n",
      "Step 2890000, Average Loss= 5.5792\n",
      "Step 2900000, Average Loss= 5.5747\n",
      "Step 2910000, Average Loss= 5.5508\n",
      "Step 2920000, Average Loss= 5.5772\n",
      "Step 2930000, Average Loss= 5.6308\n",
      "Step 2940000, Average Loss= 5.5423\n",
      "Step 2950000, Average Loss= 5.5643\n",
      "Step 2960000, Average Loss= 5.6324\n",
      "Step 2970000, Average Loss= 5.5486\n",
      "Step 2980000, Average Loss= 5.6117\n",
      "Step 2990000, Average Loss= 5.5333\n",
      "Step 3000000, Average Loss= 5.6046\n",
      "Evaluation...\n",
      "\"five\" nearest neighbors: b'four', b'three', b'six', b'seven', b'eight', b'two', b'zero', b'one',\n",
      "\"of\" nearest neighbors: b'the', b'and', b'in', b'includes', b'within', b'for', b'from', b'through',\n",
      "\"going\" nearest neighbors: b'help', b'your', b'separate', b'up', b'down', b'instead', b'behind', b'once',\n",
      "\"hardware\" nearest neighbors: b'programs', b'computer', b'software', b'games', b'available', b'code', b'free', b'basic',\n",
      "\"american\" nearest neighbors: b'canadian', b'english', b'british', b'author', b'irish', b'italian', b'french', b'film',\n",
      "\"britain\" nearest neighbors: b'established', b'british', b'independence', b'act', b'throughout', b'france', b'europe', b'began',\n"
     ]
    }
   ],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Testing data\n",
    "    x_test = np.array([word2id[bytes(w, 'gbk')] for w in eval_words])\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(1, num_steps + 1):\n",
    "        # Get a new batch of data\n",
    "        batch_x, batch_y = next_batch(batch_size, num_skips, skip_window)\n",
    "        # Run training op\n",
    "        _, loss = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})\n",
    "        average_loss += loss\n",
    "\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            if step > 1:\n",
    "                average_loss /= display_step\n",
    "            print(\"Step \" + str(step) + \", Average Loss= \" + \\\n",
    "                  \"{:.4f}\".format(average_loss))\n",
    "            average_loss = 0\n",
    "\n",
    "        # Evaluation\n",
    "        if step % eval_step == 0 or step == 1:\n",
    "            print(\"Evaluation...\")\n",
    "            sim = sess.run(cosine_sim_op, feed_dict={X: x_test})\n",
    "            for i in range(len(eval_words)):\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = '\"%s\" nearest neighbors:' % eval_words[i]\n",
    "                for k in range(top_k):\n",
    "                    log_str = '%s %s,' % (log_str, id2word[nearest[k]])\n",
    "                print(log_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-63-218a0468959e>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 各种Optimizerhttps://zhuanlan.zhihu.com/p/54420022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 9131.7734, Training Accuracy= 0.367\n",
      "Step 100, Minibatch Loss= 123.0282, Training Accuracy= 0.875\n",
      "Step 200, Minibatch Loss= 86.6065, Training Accuracy= 0.898\n",
      "Step 300, Minibatch Loss= 29.4975, Training Accuracy= 0.891\n",
      "Step 400, Minibatch Loss= 45.0123, Training Accuracy= 0.844\n",
      "Step 500, Minibatch Loss= 43.2104, Training Accuracy= 0.898\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8513\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "                                      Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Neural Network (tf.layers/estimator api) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 1000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.train.images}, y=mnist.train.labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "def neural_net(x_dict):\n",
    "    # TF Estimator input is a dict, in case of multiple inputs\n",
    "    x = x_dict['images']\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.layers.dense(x, n_hidden_1)\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.layers.dense(layer_1, n_hidden_2)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.layers.dense(layer_2, num_classes)\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    \"\"\"\n",
    "    必须这样定义，就算是不用某个参数，也要把它定义出来\n",
    "    :param features: 是estimator传过来的feature\n",
    "    :param labels: 数据标签\n",
    "    :param mode: tf.estimator.TRAIN/tf.estimator.EVAL/tf.estimator.PREDICTION\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the neural network\n",
    "    logits = neural_net(features)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0910 22:16:14.861448  7632 estimator.py:1790] Using default config.\n",
      "W0910 22:16:14.868429  7632 estimator.py:1811] Using temporary folder as model directory: C:\\Users\\MR5E8F~1.WU\\AppData\\Local\\Temp\\tmpg339901o\n",
      "I0910 22:16:14.869427  7632 estimator.py:209] Using config: {'_model_dir': 'C:\\\\Users\\\\MR5E8F~1.WU\\\\AppData\\\\Local\\\\Temp\\\\tmpg339901o', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001BC81596F28>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Build the Estimator\n",
    "model = tf.estimator.Estimator(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0910 22:16:24.981563  7632 estimator.py:1145] Calling model_fn.\n",
      "W0910 22:16:24.982561  7632 deprecation.py:323] From <ipython-input-84-9edf98100391>:6: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0910 22:16:25.187098  7632 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BCD06F24E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BCD06F24E0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W0910 22:16:25.295807  7632 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BCD06F24E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BCD06F24E0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BCD06F24E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BCD06F24E0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BCD06F24E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BCD06F24E0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0910 22:16:25.402522  7632 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BCD06F24E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BCD06F24E0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "I0910 22:16:25.478320  7632 estimator.py:1147] Done calling model_fn.\n",
      "I0910 22:16:25.481311  7632 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0910 22:16:25.606976  7632 monitored_session.py:240] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BCD06F24E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BCD06F24E0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0910 22:16:25.699726  7632 session_manager.py:500] Running local_init_op.\n",
      "I0910 22:16:25.709699  7632 session_manager.py:502] Done running local_init_op.\n",
      "I0910 22:16:25.851322  7632 basic_session_run_hooks.py:606] Saving checkpoints for 0 into C:\\Users\\MR5E8F~1.WU\\AppData\\Local\\Temp\\tmpg339901o\\model.ckpt.\n",
      "I0910 22:16:25.988959  7632 basic_session_run_hooks.py:262] loss = 2.566136, step = 1\n",
      "I0910 22:16:26.602311  7632 basic_session_run_hooks.py:692] global_step/sec: 162.773\n",
      "I0910 22:16:26.604309  7632 basic_session_run_hooks.py:260] loss = 0.4100687, step = 101 (0.616 sec)\n",
      "I0910 22:16:27.131283  7632 basic_session_run_hooks.py:692] global_step/sec: 188.69\n",
      "I0910 22:16:27.133278  7632 basic_session_run_hooks.py:260] loss = 0.30063283, step = 201 (0.529 sec)\n",
      "I0910 22:16:27.736095  7632 basic_session_run_hooks.py:692] global_step/sec: 165.341\n",
      "I0910 22:16:27.739087  7632 basic_session_run_hooks.py:260] loss = 0.3630145, step = 301 (0.606 sec)\n",
      "I0910 22:16:28.309006  7632 basic_session_run_hooks.py:692] global_step/sec: 174.547\n",
      "I0910 22:16:28.311001  7632 basic_session_run_hooks.py:260] loss = 0.21177647, step = 401 (0.572 sec)\n",
      "I0910 22:16:28.828058  7632 basic_session_run_hooks.py:692] global_step/sec: 192.659\n",
      "I0910 22:16:28.829055  7632 basic_session_run_hooks.py:260] loss = 0.23765714, step = 501 (0.518 sec)\n",
      "I0910 22:16:29.324123  7632 basic_session_run_hooks.py:692] global_step/sec: 201.587\n",
      "I0910 22:16:29.326117  7632 basic_session_run_hooks.py:260] loss = 0.28106537, step = 601 (0.497 sec)\n",
      "I0910 22:16:29.994671  7632 basic_session_run_hooks.py:692] global_step/sec: 149.132\n",
      "I0910 22:16:29.996665  7632 basic_session_run_hooks.py:260] loss = 0.35033828, step = 701 (0.671 sec)\n",
      "I0910 22:16:30.516414  7632 basic_session_run_hooks.py:692] global_step/sec: 191.665\n",
      "I0910 22:16:30.518409  7632 basic_session_run_hooks.py:260] loss = 0.3101122, step = 801 (0.522 sec)\n",
      "I0910 22:16:31.033507  7632 basic_session_run_hooks.py:692] global_step/sec: 193.389\n",
      "I0910 22:16:31.035501  7632 basic_session_run_hooks.py:260] loss = 0.33142394, step = 901 (0.517 sec)\n",
      "I0910 22:16:31.959705  7632 basic_session_run_hooks.py:606] Saving checkpoints for 1000 into C:\\Users\\MR5E8F~1.WU\\AppData\\Local\\Temp\\tmpg339901o\\model.ckpt.\n",
      "I0910 22:16:33.107712  7632 estimator.py:368] Loss for final step: 0.25316948.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x1bc81596e48>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0910 22:17:07.467416  7632 estimator.py:1145] Calling model_fn.\n",
      "W0910 22:17:07.579104  7632 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC8185C898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC8185C898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W0910 22:17:07.692837  7632 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC8185C898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC8185C898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC8185C898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC8185C898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC8185C898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC8185C898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0910 22:17:07.800511  7632 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC8185C898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC8185C898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "I0910 22:17:07.862347  7632 estimator.py:1147] Done calling model_fn.\n",
      "I0910 22:17:07.891268  7632 evaluation.py:255] Starting evaluation at 2019-09-10T22:17:07Z\n",
      "I0910 22:17:07.963077  7632 monitored_session.py:240] Graph was finalized.\n",
      "I0910 22:17:07.976057  7632 saver.py:1280] Restoring parameters from C:\\Users\\MR5E8F~1.WU\\AppData\\Local\\Temp\\tmpg339901o\\model.ckpt-1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC8185C898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC8185C898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0910 22:17:08.014955  7632 session_manager.py:500] Running local_init_op.\n",
      "I0910 22:17:08.022931  7632 session_manager.py:502] Done running local_init_op.\n",
      "I0910 22:17:08.414884  7632 evaluation.py:275] Finished evaluation at 2019-09-10-22:17:08\n",
      "I0910 22:17:08.415881  7632 estimator.py:2039] Saving dict for global step 1000: accuracy = 0.92, global_step = 1000, loss = 0.2829643\n",
      "I0910 22:17:08.458767  7632 estimator.py:2099] Saving 'checkpoint_path' summary for global step 1000: C:\\Users\\MR5E8F~1.WU\\AppData\\Local\\Temp\\tmpg339901o\\model.ckpt-1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.92, 'loss': 0.2829643, 'global_step': 1000}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0910 22:17:36.472138  7632 estimator.py:1145] Calling model_fn.\n",
      "W0910 22:17:36.582842  7632 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC81BF49E8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC81BF49E8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W0910 22:17:36.684570  7632 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC81BF49E8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC81BF49E8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC81BF49E8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC81BF49E8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC81BF49E8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC81BF49E8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0910 22:17:36.798265  7632 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC81BF2B70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC81BF2B70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "I0910 22:17:36.804251  7632 estimator.py:1147] Done calling model_fn.\n",
      "I0910 22:17:36.877573  7632 monitored_session.py:240] Graph was finalized.\n",
      "I0910 22:17:36.883536  7632 saver.py:1280] Restoring parameters from C:\\Users\\MR5E8F~1.WU\\AppData\\Local\\Temp\\tmpg339901o\\model.ckpt-1000\n",
      "I0910 22:17:36.913455  7632 session_manager.py:500] Running local_init_op.\n",
      "I0910 22:17:36.917444  7632 session_manager.py:502] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC81BF2B70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001BC81BF2B70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAMD0lEQVR4nO3dXagc5R3H8d+vabwwepFUE0OUxIqiRTEpQYSEavEFG4SYC4sRSqTC8cJAhF5U7IVCKUio9sIL4YjBVKwvRINR60sIkrQ3mqOmGo1GK6kec8hRFHxDrMm/F2dSjvHs7HFnZmc9/+8HDrs7z87OnyG/PM/szOzjiBCAme9HbRcAoD8IO5AEYQeSIOxAEoQdSOLH/dyYbb76BxoWEZ5qeaWe3fYVtt+y/Y7tm6t8FoBmudfz7LZnSdov6TJJo5J2S1obEW+UrEPPDjSsiZ79AknvRMS7EfG1pIckra7weQAaVCXsiyS9P+n1aLHsW2wP2R6xPVJhWwAqqvIF3VRDhe8M0yNiWNKwxDAeaFOVnn1U0mmTXp8q6WC1cgA0pUrYd0s60/bpto+TdI2kbfWUBaBuPQ/jI+Ib2+slPStplqRNEfF6bZUBqFXPp9562hjH7EDjGrmoBsAPB2EHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSfR1ymbkc9ZZZ3Vse/PNN0vX3bBhQ2n7XXfd1VNNWdGzA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASnGdHo5YtW9ax7ciRI6Xrjo6O1l1OapXCbvuApM8kHZb0TUQsr6MoAPWro2f/ZUR8VMPnAGgQx+xAElXDHpKes/2S7aGp3mB7yPaI7ZGK2wJQQdVh/IqIOGh7vqTttt+MiF2T3xARw5KGJcl2VNwegB5V6tkj4mDxOC5pq6QL6igKQP16DrvtObZPPPpc0uWS9tZVGIB6VRnGL5C01fbRz/lbRDxTS1WYMZYuXdqx7Ysvvihdd+vWrXWXk1rPYY+IdyWdX2MtABrEqTcgCcIOJEHYgSQIO5AEYQeS4BZXVHLuueeWtq9fv75j2/333193OShBzw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXCeHZWcffbZpe1z5szp2Pbwww/XXQ5K0LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKO6N8kLcwIM/O8+OKLpe0nn3xyx7Zu98J3+6lpTC0iPNVyenYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIL72VFqyZIlpe3Lly8vbd+/f3/HNs6j91fXnt32JtvjtvdOWjbP9nbbbxePc5stE0BV0xnG3yfpimOW3SxpR0ScKWlH8RrAAOsa9ojYJenjYxavlrS5eL5Z0lU11wWgZr0esy+IiDFJiogx2/M7vdH2kKShHrcDoCaNf0EXEcOShiVuhAHa1Oupt0O2F0pS8TheX0kAmtBr2LdJWlc8Xyfp8XrKAdCUrsN42w9KuljSSbZHJd0q6XZJj9i+XtJ7kq5uski056KLLqq0/ocfflhTJaiqa9gjYm2HpktqrgVAg7hcFkiCsANJEHYgCcIOJEHYgSS4xRWlzjvvvErrb9y4saZKUBU9O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwZTNyV144YWl7U899VRp+4EDB0rbV6xY0bHtq6++Kl0XvWHKZiA5wg4kQdiBJAg7kARhB5Ig7EAShB1IgvvZk7v00ktL2+fNm1fa/swzz5S2cy59cNCzA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASnGdP7vzzzy9t7/Z7B1u2bKmzHDSoa89ue5Ptcdt7Jy27zfYHtvcUf6uaLRNAVdMZxt8n6Yoplv8lIpYWf3+vtywAdesa9ojYJenjPtQCoEFVvqBbb/vVYpg/t9ObbA/ZHrE9UmFbACrqNex3SzpD0lJJY5Lu6PTGiBiOiOURsbzHbQGoQU9hj4hDEXE4Io5IukfSBfWWBaBuPYXd9sJJL9dI2tvpvQAGQ9ffjbf9oKSLJZ0k6ZCkW4vXSyWFpAOSboiIsa4b43fj++6UU04pbd+zZ09p+yeffFLafs4553zvmtCsTr8b3/WimohYO8XieytXBKCvuFwWSIKwA0kQdiAJwg4kQdiBJLjFdYa77rrrStvnz59f2v7000/XWA3aRM8OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0lwnn2GW7x4caX1u93iih8OenYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSILz7DPclVdeWWn9J554oqZK0DZ6diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgvPsM8DKlSs7tnWbshl5dO3ZbZ9m+3nb+2y/bntDsXye7e223y4e5zZfLoBeTWcY/42k30XEOZIulHSj7Z9JulnSjog4U9KO4jWAAdU17BExFhEvF88/k7RP0iJJqyVtLt62WdJVTRUJoLrvdcxue4mkZZJekLQgIsakif8QbE85aZjtIUlD1coEUNW0w277BEmPSropIj61Pa31ImJY0nDxGdFLkQCqm9apN9uzNRH0ByLisWLxIdsLi/aFksabKRFAHbr27J7owu+VtC8i7pzUtE3SOkm3F4+PN1IhulqzZk3HtlmzZpWu+8orr5S279q1q6eaMHimM4xfIek3kl6zvadYdosmQv6I7eslvSfp6mZKBFCHrmGPiH9K6nSAfkm95QBoCpfLAkkQdiAJwg4kQdiBJAg7kAS3uP4AHH/88aXtq1at6vmzt2zZUtp++PDhnj8bg4WeHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeScET/fjyGX6rpzezZs0vbd+7c2bFtfLz8N0Wuvfba0vYvv/yytB2DJyKmvEuVnh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuA8OzDDcJ4dSI6wA0kQdiAJwg4kQdiBJAg7kARhB5LoGnbbp9l+3vY+26/b3lAsv832B7b3FH+9/3g5gMZ1vajG9kJJCyPiZdsnSnpJ0lWSfi3p84j487Q3xkU1QOM6XVQznfnZxySNFc8/s71P0qJ6ywPQtO91zG57iaRlkl4oFq23/artTbbndlhnyPaI7ZFKlQKoZNrXxts+QdJOSX+KiMdsL5D0kaSQ9EdNDPV/2+UzGMYDDes0jJ9W2G3PlvSkpGcj4s4p2pdIejIizu3yOYQdaFjPN8LYtqR7Je2bHPTii7uj1kjaW7VIAM2ZzrfxKyX9Q9Jrko4Ui2+RtFbSUk0M4w9IuqH4Mq/ss+jZgYZVGsbXhbADzeN+diA5wg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJdf3CyZh9J+s+k1ycVywbRoNY2qHVJ1NarOmtb3Kmhr/ezf2fj9khELG+tgBKDWtug1iVRW6/6VRvDeCAJwg4k0XbYh1vefplBrW1Q65KorVd9qa3VY3YA/dN2zw6gTwg7kEQrYbd9he23bL9j++Y2aujE9gHbrxXTULc6P10xh9647b2Tls2zvd3228XjlHPstVTbQEzjXTLNeKv7ru3pz/t+zG57lqT9ki6TNCppt6S1EfFGXwvpwPYBScsjovULMGz/QtLnkv56dGot2xslfRwRtxf/Uc6NiN8PSG236XtO491QbZ2mGb9OLe67Oqc/70UbPfsFkt6JiHcj4mtJD0la3UIdAy8idkn6+JjFqyVtLp5v1sQ/lr7rUNtAiIixiHi5eP6ZpKPTjLe670rq6os2wr5I0vuTXo9qsOZ7D0nP2X7J9lDbxUxhwdFptorH+S3Xc6yu03j30zHTjA/Mvutl+vOq2gj7VFPTDNL5vxUR8XNJv5J0YzFcxfTcLekMTcwBOCbpjjaLKaYZf1TSTRHxaZu1TDZFXX3Zb22EfVTSaZNenyrpYAt1TCkiDhaP45K2auKwY5AcOjqDbvE43nI9/xcRhyLicEQckXSPWtx3xTTjj0p6ICIeKxa3vu+mqqtf+62NsO+WdKbt020fJ+kaSdtaqOM7bM8pvjiR7TmSLtfgTUW9TdK64vk6SY+3WMu3DMo03p2mGVfL+6716c8jou9/klZp4hv5f0v6Qxs1dKjrp5L+Vfy93nZtkh7UxLDuv5oYEV0v6SeSdkh6u3icN0C13a+Jqb1f1USwFrZU20pNHBq+KmlP8beq7X1XUldf9huXywJJcAUdkARhB5Ig7EAShB1IgrADSRB2IAnCDiTxP1f9vw27cFAZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 0\n"
     ]
    }
   ],
   "source": [
    "# Predict single images\n",
    "n_images = 4\n",
    "# Get images from test set\n",
    "test_images = mnist.test.images[:n_images]\n",
    "# Prepare the input data\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': test_images}, shuffle=False)\n",
    "# Use the model to predict the images class\n",
    "preds = list(model.predict(input_fn))\n",
    "\n",
    "# Display\n",
    "for i in range(n_images):\n",
    "    plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n",
    "    plt.show()\n",
    "    print(\"Model prediction:\", preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
