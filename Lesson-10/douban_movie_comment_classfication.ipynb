{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "content = pd.read_csv('D:\\Github_project\\datasource\\movie_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 261497 entries, 0 to 261496\n",
      "Data columns (total 5 columns):\n",
      "id         261497 non-null object\n",
      "link       261497 non-null object\n",
      "name       261497 non-null object\n",
      "comment    261495 non-null object\n",
      "star       261497 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 10.0+ MB\n"
     ]
    }
   ],
   "source": [
    "content.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.dropna(subset=['comment'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 261495 entries, 0 to 261496\n",
      "Data columns (total 5 columns):\n",
      "id         261495 non-null object\n",
      "link       261495 non-null object\n",
      "name       261495 non-null object\n",
      "comment    261495 non-null object\n",
      "star       261495 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 12.0+ MB\n"
     ]
    }
   ],
   "source": [
    "content.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.drop_duplicates(subset=['comment'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 213970 entries, 0 to 261496\n",
      "Data columns (total 5 columns):\n",
      "id         213970 non-null object\n",
      "link       213970 non-null object\n",
      "name       213970 non-null object\n",
      "comment    213970 non-null object\n",
      "star       213970 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 9.8+ MB\n"
     ]
    }
   ],
   "source": [
    "content.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, '1', 2, '2', '3', 3, 4, '4', 5, '5', 'star'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(content.star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.drop(content[content.star=='star'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 213969 entries, 0 to 261496\n",
      "Data columns (total 5 columns):\n",
      "id         213969 non-null object\n",
      "link       213969 non-null object\n",
      "name       213969 non-null object\n",
      "comment    213969 non-null object\n",
      "star       213969 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 9.8+ MB\n"
     ]
    }
   ],
   "source": [
    "content.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, '1', 2, '2', '3', 3, 4, '4', 5, '5'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(content.star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [int(star) for star in content.star]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = content.comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213969, 213969)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y), len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量相加构建句向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def cut(string): return ' '.join(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def token(string):\n",
    "    return re.findall(r'[\\d|\\w]+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\MR5E8F~1.WU\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.627 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "comments_cut = [cut(''.join(token(comment))) for comment in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吴京', '意淫', '到', '了', '脑残', '的', '地步', '看', '了', '恶心', '想', '吐']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_cut[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采用之前维基百科+新闻语料训练的词向量\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "word2vec = Word2Vec.load(r'D:\\data\\词向量\\news_word2vec.w2v').wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616395"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan Wu\\Anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv['中二'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan Wu\\Anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.34566391e-01,  2.79686805e-02,  4.15266939e-02, -1.78346932e-01,\n",
       "        1.02283709e-01,  9.09975618e-02, -8.90697166e-02, -9.23506841e-02,\n",
       "       -1.94960028e-01,  2.81524897e-01, -2.67716378e-01,  3.56442302e-01,\n",
       "       -1.56728461e-01,  1.41186714e-01, -4.05162051e-02, -3.29683810e-01,\n",
       "        1.43675506e-01,  2.80216038e-02,  8.96399096e-02, -1.10607661e-01,\n",
       "       -5.41270664e-03, -2.17700794e-01, -1.87995419e-01, -6.25246391e-02,\n",
       "        3.53753656e-01,  1.66690182e-02,  1.82034761e-01,  1.33014679e-01,\n",
       "       -1.18101671e-01, -1.40211239e-01,  2.02820256e-01, -3.42190489e-02,\n",
       "       -1.96076140e-01, -2.19466701e-01,  9.41848755e-02, -7.00309873e-02,\n",
       "       -1.09779820e-01, -1.05037972e-01, -2.15431079e-01,  2.40143076e-01,\n",
       "       -2.14520514e-01, -3.69605832e-02,  9.37635377e-02,  1.88759536e-01,\n",
       "        3.43713313e-01, -1.39666110e-01,  6.32417575e-02, -3.31544608e-01,\n",
       "       -3.61580737e-02, -1.49203002e-01,  1.72923356e-02, -4.85782363e-02,\n",
       "        2.10080639e-01,  1.73718810e-01, -7.65178874e-02, -1.23988111e-02,\n",
       "       -2.97433645e-01,  9.15666148e-02,  6.93079233e-02, -2.09979131e-03,\n",
       "        1.42797038e-01,  6.16450561e-03, -3.05126339e-01,  2.17682853e-01,\n",
       "       -1.94364280e-01, -2.28253491e-02,  1.93165451e-01,  3.53029251e-01,\n",
       "       -5.04827917e-01,  1.20705878e-02,  2.13978440e-02, -1.98508054e-01,\n",
       "       -9.18700621e-02, -4.56245840e-01, -7.42848366e-02,  8.09857324e-02,\n",
       "        2.04450041e-01, -1.95136085e-01, -2.16274336e-01, -7.65029714e-02,\n",
       "        6.76734895e-02,  4.44254190e-01,  1.32556453e-01, -4.95452434e-02,\n",
       "       -4.46643978e-02,  1.25138596e-01,  9.81590971e-02,  7.51057416e-02,\n",
       "        2.14367062e-02,  2.02820763e-01,  1.67439565e-01, -3.48275527e-02,\n",
       "       -4.65181582e-02, -3.83334726e-01,  3.07861328e-01,  1.41356664e-04,\n",
       "       -1.05877612e-02, -2.93581575e-01,  7.40599632e-02, -8.93863812e-02,\n",
       "        1.10821448e-01,  2.99251944e-01, -8.17696229e-02,  1.07653514e-01,\n",
       "       -4.36448961e-01,  4.33180958e-01,  9.31080282e-02,  1.00722881e-02,\n",
       "       -2.44505584e-01,  1.26053020e-01,  3.08698136e-02, -4.66575958e-02,\n",
       "       -1.59951434e-01, -3.50746885e-02, -2.67618418e-01,  5.27938129e-03,\n",
       "        4.59665954e-01, -1.76894362e-03,  2.59472579e-01, -4.07533273e-02,\n",
       "       -3.89112771e-01,  2.45527282e-01, -6.12333752e-02,  4.53594625e-01,\n",
       "        3.49852324e-01,  1.92871064e-01,  1.17920034e-01, -2.16657162e-01,\n",
       "       -3.11634749e-01,  9.40337181e-02, -1.65413365e-01,  4.87064332e-01,\n",
       "       -1.89034268e-01, -1.42325193e-01,  4.98131253e-02,  5.40502071e-02,\n",
       "       -1.70407370e-01, -8.63367096e-02, -7.73279220e-02, -2.19365042e-02,\n",
       "        9.15738568e-02,  1.53212368e-01, -1.35377631e-01, -3.13775122e-01,\n",
       "       -3.75796147e-02,  2.17792451e-01, -1.52451783e-01, -5.07033605e-04,\n",
       "        1.30235806e-01,  2.98271388e-01,  3.24870497e-01,  1.25308543e-01,\n",
       "       -2.14170981e-02,  1.00151878e-02, -5.21343462e-02,  3.84329483e-02,\n",
       "        8.82761851e-02,  5.99926174e-01,  1.58856675e-01,  1.42602727e-01,\n",
       "        3.92323695e-02, -5.19848876e-02,  1.83013752e-01,  1.17212474e-01,\n",
       "       -3.69636007e-02, -1.85882583e-01, -2.63155694e-03,  2.16480479e-01,\n",
       "        5.59776388e-02, -1.95418317e-02,  1.80433959e-01,  1.99153274e-01,\n",
       "        1.56415015e-01,  5.61213680e-02, -6.35124277e-04, -1.05985515e-01,\n",
       "       -1.67094376e-02, -4.96828631e-02,  8.62722173e-02,  1.80972293e-01,\n",
       "       -3.03196013e-01, -6.52916059e-02,  7.17900917e-02,  2.59445995e-01,\n",
       "        1.61953092e-01, -1.96382269e-01, -3.25061768e-01, -1.57336995e-01,\n",
       "       -1.23521775e-01, -2.56855488e-02,  8.96114949e-03,  2.82175988e-01,\n",
       "       -1.12291433e-01, -2.22335249e-01, -5.67854717e-02, -7.92326900e-05,\n",
       "       -5.57894036e-02, -3.18986416e-01, -6.84710667e-02, -1.11153990e-01,\n",
       "        1.38132527e-01, -7.98783824e-02,  1.31589547e-02, -5.28586954e-02,\n",
       "       -1.12797543e-01, -3.07160057e-02, -1.87839251e-02, -1.83694556e-01,\n",
       "       -7.86933452e-02, -8.50439351e-03, -4.21789326e-02,  1.87831640e-01,\n",
       "        5.29417098e-02,  4.25850712e-02, -4.44578677e-01,  1.86674640e-01,\n",
       "        2.72846758e-01,  2.90683907e-04, -1.98305354e-01, -2.66341358e-01,\n",
       "        2.17759367e-02,  2.95321792e-02,  1.73599869e-01,  2.65104584e-02,\n",
       "       -1.43438019e-02, -1.26518995e-01,  5.02150692e-02,  7.46248066e-02,\n",
       "        4.84299734e-02,  3.80760401e-01, -5.29301912e-02,  1.02955803e-01,\n",
       "       -1.74151614e-01, -1.22358799e-01,  2.12279007e-01, -1.56327054e-01,\n",
       "       -1.44786090e-01, -4.95186448e-02, -1.49656102e-01,  5.68802133e-02,\n",
       "        5.98645099e-02, -2.73510575e-01,  2.18421295e-01, -1.05988838e-01,\n",
       "       -2.43553713e-01,  1.98243499e-01,  3.82736683e-01,  2.29329690e-01,\n",
       "       -3.31028908e-01,  1.46890551e-01, -2.76678920e-01, -2.38871679e-01,\n",
       "       -4.38713767e-02, -1.36018008e-01, -3.24888229e-01,  1.40075371e-01,\n",
       "        1.45046636e-01, -1.98807389e-01,  1.00694941e-02, -2.69313872e-01,\n",
       "        1.45222113e-01,  4.17365134e-01,  2.27261689e-02, -2.56968420e-02,\n",
       "        4.48097855e-01,  2.71129996e-01, -3.42412233e-01,  5.73128834e-03,\n",
       "        6.58615679e-02,  3.69892665e-03,  8.50820094e-02,  2.06268519e-01,\n",
       "        1.45645514e-01,  1.20162237e-02, -4.85895239e-02, -4.15096544e-02,\n",
       "        4.84896936e-02, -2.81329334e-01,  3.92261922e-01,  2.02456072e-01,\n",
       "        3.80444884e-01,  6.17957652e-01,  2.18323648e-01, -4.67418671e-01,\n",
       "       -1.88159451e-01,  2.18083449e-02, -1.05423413e-01,  2.00973660e-01,\n",
       "        1.75373152e-01,  2.56007880e-01,  1.45521596e-01, -4.79993690e-03,\n",
       "       -2.73901701e-01,  1.84036553e-01, -1.43269971e-01, -7.88653046e-02,\n",
       "       -5.03338575e-02,  1.06457351e-02,  1.60871074e-01,  3.57132368e-02,\n",
       "       -3.64945322e-01,  1.84012681e-01, -3.76631200e-01,  9.78727825e-03,\n",
       "        3.60415578e-01, -1.58641562e-01,  2.05005065e-01,  1.21341050e-02,\n",
       "        8.56080372e-03,  5.87021947e-01, -3.08706820e-01, -1.04545839e-01,\n",
       "       -4.10544127e-03, -7.12169632e-02,  2.19285905e-01,  1.35718852e-01,\n",
       "       -4.11899477e-01,  3.01067531e-01,  6.17518499e-02,  2.61253148e-01,\n",
       "        8.31748918e-02, -1.23054631e-01, -1.22791091e-02,  3.51856425e-02,\n",
       "       -1.02803402e-01, -2.13258445e-01, -2.14233696e-01, -1.01239914e-02,\n",
       "        5.98601475e-02, -5.85025251e-02,  1.62625268e-01, -2.00495556e-01,\n",
       "        2.46733308e-01,  3.25762667e-02,  2.18874142e-01,  1.96246639e-01,\n",
       "        3.46627757e-02, -2.31570564e-02,  2.39248425e-01, -1.62265986e-01,\n",
       "       -3.34232301e-02, -5.25414646e-01,  3.20202224e-02,  1.96464032e-01,\n",
       "        7.21818767e-03, -7.85344616e-02, -7.53967836e-02, -1.55477613e-01,\n",
       "        3.77031751e-02, -1.78947330e-01,  1.18282974e-01, -5.95749021e-02,\n",
       "       -1.56447008e-01, -1.65458888e-01,  7.74892420e-02, -3.29069644e-01,\n",
       "        2.83632725e-01, -1.43402815e-02,  2.49999255e-01, -7.58519694e-02,\n",
       "       -3.53124142e-02, -1.89507797e-01, -1.58308759e-01, -1.14292040e-01,\n",
       "       -1.57656357e-01, -3.28772426e-01,  2.75952727e-01,  1.13101177e-01,\n",
       "        1.96804285e-01, -4.47839111e-01, -9.02542546e-02,  5.68613708e-01,\n",
       "       -1.01325192e-01,  3.27892117e-02, -1.23264998e-01, -2.43478149e-01,\n",
       "        1.90026909e-01,  1.67184770e-01,  1.93548664e-01,  1.19479217e-01,\n",
       "       -1.21081054e-01,  2.53245860e-01,  2.14421395e-02,  7.92756081e-02,\n",
       "        1.47873729e-01,  9.12430696e-03, -1.93013430e-01, -7.09521621e-02,\n",
       "       -1.53427050e-02,  3.84192556e-01, -2.56408840e-01,  7.78420866e-02,\n",
       "        2.51858160e-02, -6.14042394e-03, -2.25675151e-01, -4.13776860e-02,\n",
       "        4.06853706e-02, -3.96425039e-01, -2.74151772e-01, -5.55338599e-02,\n",
       "        5.15250601e-02,  1.15460806e-01, -1.74045935e-02, -1.90620810e-01,\n",
       "        2.40352899e-01,  1.12855127e-02, -1.52222633e-01,  8.54814053e-02,\n",
       "        1.59815431e-01,  3.14875722e-01, -4.11992669e-02, -2.73757517e-01,\n",
       "       -3.50146949e-01, -3.55051190e-01,  1.46807969e-01, -2.42565408e-01,\n",
       "       -2.68763036e-01,  5.95240667e-02, -8.93791616e-02, -1.43548936e-01,\n",
       "        1.17382035e-01, -2.39742175e-01, -7.11434260e-02,  2.85921365e-01,\n",
       "        2.70584255e-01, -1.45273909e-01, -1.50072753e-01,  2.38502361e-02,\n",
       "        3.26315373e-01,  1.62424147e-01, -2.24869788e-01, -1.49908483e-01,\n",
       "        1.08727157e-01, -3.07196230e-01,  1.57661721e-01, -3.12079117e-02,\n",
       "       -3.20898920e-01,  1.08063318e-01, -1.14470154e-01, -3.99739034e-02,\n",
       "        5.06878756e-02,  1.46329701e-02,  1.94373161e-01, -1.50958717e-01,\n",
       "       -2.72972226e-01,  1.19210042e-01, -2.14425832e-01, -2.33135559e-02,\n",
       "       -2.55770758e-02, -1.24003720e-02, -1.16939712e-02, -1.91093296e-01,\n",
       "       -2.21589640e-01,  2.65432715e-01, -1.06739275e-01,  2.73199767e-01,\n",
       "        1.37009695e-01,  1.85736790e-01,  1.45821944e-01, -3.59686208e-03,\n",
       "        1.77894384e-01,  1.31982088e-01, -1.94530919e-01, -1.48488775e-01,\n",
       "       -2.20973611e-01, -2.11316228e-01,  2.11024322e-02,  4.37641181e-02,\n",
       "        3.19353819e-01,  1.89225003e-01, -1.04042903e-01, -1.20829701e-01,\n",
       "        1.94638699e-01,  1.40928878e-02, -9.27638113e-02, -3.38804238e-02,\n",
       "       -3.39843869e-01,  2.55207978e-02, -6.01928905e-02,  2.20932782e-01,\n",
       "       -7.95788318e-02, -3.39089960e-01,  8.60174447e-02, -9.49221775e-02,\n",
       "       -3.16800117e-01,  1.94177963e-02,  2.53316432e-01,  1.03434548e-01,\n",
       "        1.05144240e-01,  3.59781921e-01, -8.04896206e-02, -4.68558073e-01,\n",
       "       -1.72131151e-01,  1.61785632e-01,  3.18480223e-01,  3.27687971e-02,\n",
       "       -1.69227093e-01,  3.42539668e-01, -1.75582930e-01,  4.67025816e-01])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(500, None) + word2vec.wv['中二']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(sentences):\n",
    "    sentence_vecs = []\n",
    "    for sentence in sentences:\n",
    "        s = 0\n",
    "        sentence_vec = np.zeros(500, None)\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            if word in word2vec.vocab:\n",
    "                sentence_vec += word2vec.wv[word]\n",
    "                s += 1\n",
    "            else:\n",
    "                continue\n",
    "        if s != 0:\n",
    "            sentence_vec = sentence_vec / s\n",
    "        else:\n",
    "            sentence_vec = np.zeros(500, None)\n",
    "        sentence_vecs.append(sentence_vec)\n",
    "    return sentence_vecs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vecs = sentence2vec(comments_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vecs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213969, 213969)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_vecs), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentence_vecs, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171175, 42794, 171175, 42794)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_tr_arr = np.array(X_train, dtype='float32')\n",
    "X_ts_arr = np.array(X_test, dtype='float32')\n",
    "y_tr_arr = np.array(y_train)\n",
    "y_ts_arr = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan Wu\\Anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Ryan Wu\\Anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression().fit(X_tr_arr, y_tr_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.401992113334307"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_tr_arr, y_tr_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39881759125111"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_ts_arr, y_ts_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171175, 500)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171175,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 6\n",
    "\n",
    "def reformat(labels):\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (171175, 500) (171175, 6)\n",
      "Test set (42794, 500) (42794, 6)\n"
     ]
    }
   ],
   "source": [
    "y_tr_arr = reformat(y_tr_arr)\n",
    "y_ts_arr = reformat(y_ts_arr)\n",
    "print('Training set', X_tr_arr.shape, y_tr_arr.shape)\n",
    "print('Test set', X_ts_arr.shape, y_ts_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "hiden_layer_node_num = 1024\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, 500))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_test_dataset = tf.constant(X_ts_arr)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([500, hiden_layer_node_num], stddev=0.04))\n",
    "  biases1 = tf.Variable(tf.zeros([hiden_layer_node_num]))\n",
    "    \n",
    "  # input layer output (batch_size, hiden_layer_node_num)\n",
    "  weights2 = tf.Variable(tf.truncated_normal([hiden_layer_node_num, num_labels], stddev=0.04))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1), weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  regularizers = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)\n",
    "  loss = tf.reduce_mean(loss + beta * regularizers)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 5.017567\n",
      "Minibatch accuracy: 12.0%\n",
      "Minibatch loss at step 500: 1.567699\n",
      "Minibatch accuracy: 30.0%\n",
      "Minibatch loss at step 1000: 1.472009\n",
      "Minibatch accuracy: 36.0%\n",
      "Minibatch loss at step 1500: 1.574567\n",
      "Minibatch accuracy: 31.0%\n",
      "Minibatch loss at step 2000: 1.499942\n",
      "Minibatch accuracy: 35.0%\n",
      "Minibatch loss at step 2500: 1.455968\n",
      "Minibatch accuracy: 33.0%\n",
      "Minibatch loss at step 3000: 1.469914\n",
      "Minibatch accuracy: 41.0%\n",
      "Test accuracy: 34.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (y_tr_arr.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = X_tr_arr[offset:(offset + batch_size), :]\n",
    "    batch_labels = y_tr_arr[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), y_ts_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以上效果不佳，试试情感分类工具snownlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SnowNLP(u'这个东西真赞。但是我不喜欢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9269068736444782"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SnowNLP(u'恶心')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12413793103448256"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'吴京意淫到了脑残的地步，看了恶心想吐'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0323066138237218"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowNLP(comments[0]).sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = [SnowNLP(comment).sentiments for comment in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0323066138237218"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentiments, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_tr_arr = np.array(X_train, dtype='float32')\n",
    "X_ts_arr = np.array(X_test, dtype='float32')\n",
    "y_tr_arr = np.array(y_train)\n",
    "y_ts_arr = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((171175,), (171175,), (42794,), (42794,))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_arr.shape, y_tr_arr.shape, X_ts_arr.shape, y_ts_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99956065"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mr. Wu\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression().fit(X_tr_arr.reshape(-1, 1), y_tr_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33087191470717103"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_tr_arr.reshape(-1, 1), y_tr_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32983595831191287"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_ts_arr.reshape(-1, 1), y_ts_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(comments, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comments_with_label.txt', 'w', encoding='utf-8') as f:\n",
    "    for comment, label in zip(X_train, y_train):\n",
    "        label = str('__label__' + str(label))\n",
    "        comment = cut(comment)\n",
    "        f.write(label + ' , ' + comment)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=\"comments_with_label.txt\", lr=0.1, epoch=50, wordNgrams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comments_test.txt', 'w', encoding='utf-8') as f:\n",
    "     for comment, label in zip(X_test, y_test):\n",
    "        label = str('__label__' + str(label))\n",
    "        comment = cut(comment)\n",
    "        f.write(label + ' , ' + comment)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42794, 0.4331448333878581, 0.4331448333878581)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test('comments_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量+GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'吴京 意淫 到 了 脑残 的 地步 看 了 恶心 想 吐'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_cut[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(comments_cut, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171175, 42794, 171175, 42794)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 5\n",
    "\n",
    "def reformat(labels):\n",
    "  # Map 1 to [1.0, 0.0, 0.0 ...], 2 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == (labels[:,None]-1)).astype(np.float32)\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = reformat(y_train)\n",
    "y_test = reformat(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_features = 50000\n",
    "maxlen = 100\n",
    "embed_size = 500\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171175, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'的': 1,\n",
       " '了': 2,\n",
       " '是': 3,\n",
       " '我': 4,\n",
       " '都': 5,\n",
       " '很': 6,\n",
       " '看': 7,\n",
       " '电影': 8,\n",
       " '也': 9,\n",
       " '和': 10,\n",
       " '在': 11,\n",
       " '不': 12,\n",
       " '有': 13,\n",
       " '就': 14,\n",
       " '人': 15,\n",
       " '好': 16,\n",
       " '啊': 17,\n",
       " '这': 18,\n",
       " '一个': 19,\n",
       " '你': 20,\n",
       " '还': 21,\n",
       " '还是': 22,\n",
       " '但': 23,\n",
       " '故事': 24,\n",
       " '没有': 25,\n",
       " '就是': 26,\n",
       " '让': 27,\n",
       " '喜欢': 28,\n",
       " '太': 29,\n",
       " '又': 30,\n",
       " '剧情': 31,\n",
       " '说': 32,\n",
       " '他': 33,\n",
       " '吧': 34,\n",
       " '没': 35,\n",
       " '得': 36,\n",
       " '到': 37,\n",
       " '给': 38,\n",
       " '上': 39,\n",
       " '被': 40,\n",
       " '这个': 41,\n",
       " '对': 42,\n",
       " '最后': 43,\n",
       " '一部': 44,\n",
       " '不错': 45,\n",
       " '能': 46,\n",
       " '片子': 47,\n",
       " '什么': 48,\n",
       " '与': 49,\n",
       " '多': 50,\n",
       " '可以': 51,\n",
       " '不是': 52,\n",
       " '最': 53,\n",
       " '觉得': 54,\n",
       " '中': 55,\n",
       " '导演': 56,\n",
       " '自己': 57,\n",
       " '拍': 58,\n",
       " '要': 59,\n",
       " '真的': 60,\n",
       " '感觉': 61,\n",
       " '里': 62,\n",
       " '但是': 63,\n",
       " '那': 64,\n",
       " '想': 65,\n",
       " '好看': 66,\n",
       " '会': 67,\n",
       " '有点': 68,\n",
       " '这部': 69,\n",
       " '去': 70,\n",
       " '把': 71,\n",
       " '着': 72,\n",
       " '这么': 73,\n",
       " '而': 74,\n",
       " '小': 75,\n",
       " '个': 76,\n",
       " '这样': 77,\n",
       " '真是': 78,\n",
       " '这种': 79,\n",
       " '那么': 80,\n",
       " '片': 81,\n",
       " '不过': 82,\n",
       " '更': 83,\n",
       " '时候': 84,\n",
       " '比': 85,\n",
       " '却': 86,\n",
       " '挺': 87,\n",
       " '爱': 88,\n",
       " '我们': 89,\n",
       " '大': 90,\n",
       " '像': 91,\n",
       " '虽然': 92,\n",
       " '演技': 93,\n",
       " '其实': 94,\n",
       " '看到': 95,\n",
       " '再': 96,\n",
       " '知道': 97,\n",
       " '演员': 98,\n",
       " '那个': 99,\n",
       " '来': 100,\n",
       " '才': 101,\n",
       " '真': 102,\n",
       " '生活': 103,\n",
       " '完': 104,\n",
       " '影片': 105,\n",
       " '吗': 106,\n",
       " '非常': 107,\n",
       " '怎么': 108,\n",
       " '为': 109,\n",
       " '她': 110,\n",
       " '一样': 111,\n",
       " '还有': 112,\n",
       " '呢': 113,\n",
       " '做': 114,\n",
       " '结尾': 115,\n",
       " '从': 116,\n",
       " '完全': 117,\n",
       " '很多': 118,\n",
       " '镜头': 119,\n",
       " '跟': 120,\n",
       " '只是': 121,\n",
       " '人物': 122,\n",
       " '因为': 123,\n",
       " '有些': 124,\n",
       " '打': 125,\n",
       " '用': 126,\n",
       " '情节': 127,\n",
       " '死': 128,\n",
       " '角色': 129,\n",
       " '实在': 130,\n",
       " '笑': 131,\n",
       " '经典': 132,\n",
       " '看过': 133,\n",
       " '一': 134,\n",
       " '点': 135,\n",
       " '不能': 136,\n",
       " '后': 137,\n",
       " '爱情': 138,\n",
       " '如果': 139,\n",
       " '现在': 140,\n",
       " '逼': 141,\n",
       " '表演': 142,\n",
       " '中国': 143,\n",
       " '已经': 144,\n",
       " '下': 145,\n",
       " '么': 146,\n",
       " '戏': 147,\n",
       " '世界': 148,\n",
       " '过': 149,\n",
       " '结局': 150,\n",
       " '只': 151,\n",
       " '为了': 152,\n",
       " '出来': 153,\n",
       " '节奏': 154,\n",
       " '它': 155,\n",
       " '一直': 156,\n",
       " '动作': 157,\n",
       " '演': 158,\n",
       " '青春': 159,\n",
       " '开始': 160,\n",
       " '一般': 161,\n",
       " '比较': 162,\n",
       " '为什么': 163,\n",
       " '应该': 164,\n",
       " '地': 165,\n",
       " '烂片': 166,\n",
       " '各种': 167,\n",
       " '年': 168,\n",
       " '他们': 169,\n",
       " '画面': 170,\n",
       " '老': 171,\n",
       " '2': 172,\n",
       " '可爱': 173,\n",
       " '只有': 174,\n",
       " '一点': 175,\n",
       " '并': 176,\n",
       " '两个': 177,\n",
       " '谁': 178,\n",
       " '剧本': 179,\n",
       " '所以': 180,\n",
       " '一起': 181,\n",
       " '作品': 182,\n",
       " '精彩': 183,\n",
       " '可能': 184,\n",
       " '人生': 185,\n",
       " '当': 186,\n",
       " '时': 187,\n",
       " '如此': 188,\n",
       " '居然': 189,\n",
       " '风格': 190,\n",
       " '配乐': 191,\n",
       " '题材': 192,\n",
       " '除了': 193,\n",
       " '音乐': 194,\n",
       " '喜剧': 195,\n",
       " '里面': 196,\n",
       " '特别': 197,\n",
       " '观众': 198,\n",
       " '哭': 199,\n",
       " '不会': 200,\n",
       " '前': 201,\n",
       " '感动': 202,\n",
       " '烂': 203,\n",
       " '真实': 204,\n",
       " '讲': 205,\n",
       " '星': 206,\n",
       " '1': 207,\n",
       " '分': 208,\n",
       " '不要': 209,\n",
       " '之': 210,\n",
       " '一种': 211,\n",
       " '那些': 212,\n",
       " '美国': 213,\n",
       " '简直': 214,\n",
       " '以为': 215,\n",
       " '可惜': 216,\n",
       " '女人': 217,\n",
       " '出': 218,\n",
       " '们': 219,\n",
       " '男人': 220,\n",
       " '问题': 221,\n",
       " '作为': 222,\n",
       " '美': 223,\n",
       " '搞笑': 224,\n",
       " '时间': 225,\n",
       " '而且': 226,\n",
       " '无聊': 227,\n",
       " '台词': 228,\n",
       " '日本': 229,\n",
       " '3': 230,\n",
       " '时代': 231,\n",
       " '那段': 232,\n",
       " '孩子': 233,\n",
       " '之后': 234,\n",
       " '一星': 235,\n",
       " '简单': 236,\n",
       " '场面': 237,\n",
       " '细节': 238,\n",
       " '第一部': 239,\n",
       " '东西': 240,\n",
       " '地方': 241,\n",
       " '编剧': 242,\n",
       " '主角': 243,\n",
       " '这是': 244,\n",
       " '现实': 245,\n",
       " '一次': 246,\n",
       " '确实': 247,\n",
       " '每个': 248,\n",
       " '一些': 249,\n",
       " '只能': 250,\n",
       " '可': 251,\n",
       " '版': 252,\n",
       " '所有': 253,\n",
       " '叙事': 254,\n",
       " '叫': 255,\n",
       " '特效': 256,\n",
       " '永远': 257,\n",
       " '无法': 258,\n",
       " '起来': 259,\n",
       " '以及': 260,\n",
       " '部分': 261,\n",
       " '一切': 262,\n",
       " '完美': 263,\n",
       " '本片': 264,\n",
       " '三星': 265,\n",
       " '希望': 266,\n",
       " '以': 267,\n",
       " '最好': 268,\n",
       " '一定': 269,\n",
       " '竟然': 270,\n",
       " '原来': 271,\n",
       " '几个': 272,\n",
       " '男': 273,\n",
       " '总是': 274,\n",
       " '女主': 275,\n",
       " '看着': 276,\n",
       " '啥': 277,\n",
       " '无': 278,\n",
       " '成': 279,\n",
       " '表现': 280,\n",
       " '需要': 281,\n",
       " '一下': 282,\n",
       " '将': 283,\n",
       " '值得': 284,\n",
       " '走': 285,\n",
       " '适合': 286,\n",
       " '情感': 287,\n",
       " '分钟': 288,\n",
       " '你们': 289,\n",
       " '不够': 290,\n",
       " '美好': 291,\n",
       " '香港': 292,\n",
       " '剪辑': 293,\n",
       " '于': 294,\n",
       " '不如': 295,\n",
       " '结果': 296,\n",
       " '小时': 297,\n",
       " '系列': 298,\n",
       " '等': 299,\n",
       " '其他': 300,\n",
       " '啦': 301,\n",
       " '年代': 302,\n",
       " '算': 303,\n",
       " '感情': 304,\n",
       " '整个': 305,\n",
       " '懂': 306,\n",
       " '社会': 307,\n",
       " '半': 308,\n",
       " '那种': 309,\n",
       " '尤其': 310,\n",
       " '英雄': 311,\n",
       " '尴尬': 312,\n",
       " '女': 313,\n",
       " '算是': 314,\n",
       " '果然': 315,\n",
       " '看看': 316,\n",
       " '依然': 317,\n",
       " '终于': 318,\n",
       " '然后': 319,\n",
       " '发现': 320,\n",
       " '个人': 321,\n",
       " '好像': 322,\n",
       " '男主': 323,\n",
       " '家庭': 324,\n",
       " '话': 325,\n",
       " '唯一': 326,\n",
       " '感': 327,\n",
       " '倒': 328,\n",
       " '场景': 329,\n",
       " '差': 330,\n",
       " '对于': 331,\n",
       " '这片': 332,\n",
       " '出现': 333,\n",
       " '连': 334,\n",
       " '动画': 335,\n",
       " '亮点': 336,\n",
       " '傻': 337,\n",
       " '蛮': 338,\n",
       " '哈哈哈': 339,\n",
       " '牛': 340,\n",
       " '战争': 341,\n",
       " '没什么': 342,\n",
       " '嘛': 343,\n",
       " '人性': 344,\n",
       " '一段': 345,\n",
       " '呀': 346,\n",
       " '之前': 347,\n",
       " '电影院': 348,\n",
       " '五星': 349,\n",
       " 'ps': 350,\n",
       " '摄影': 351,\n",
       " '真正': 352,\n",
       " '本身': 353,\n",
       " '来说': 354,\n",
       " '后面': 355,\n",
       " '可是': 356,\n",
       " '主题': 357,\n",
       " '印象': 358,\n",
       " '类型': 359,\n",
       " '关于': 360,\n",
       " '绝对': 361,\n",
       " '这些': 362,\n",
       " '期待': 363,\n",
       " '5': 364,\n",
       " '找': 365,\n",
       " '4': 366,\n",
       " '一场': 367,\n",
       " '高': 368,\n",
       " '年轻': 369,\n",
       " '逻辑': 370,\n",
       " '令人': 371,\n",
       " '新': 372,\n",
       " '深刻': 373,\n",
       " '女主角': 374,\n",
       " '当年': 375,\n",
       " '每': 376,\n",
       " '记得': 377,\n",
       " '事': 378,\n",
       " '生命': 379,\n",
       " '成为': 380,\n",
       " '到底': 381,\n",
       " '有趣': 382,\n",
       " '整体': 383,\n",
       " '搞': 384,\n",
       " '的话': 385,\n",
       " '想起': 386,\n",
       " '大家': 387,\n",
       " '理解': 388,\n",
       " '失望': 389,\n",
       " '本来': 390,\n",
       " '狗血': 391,\n",
       " '式': 392,\n",
       " '不同': 393,\n",
       " '效果': 394,\n",
       " '充满': 395,\n",
       " '之间': 396,\n",
       " '成功': 397,\n",
       " '幽默': 398,\n",
       " '毫无': 399,\n",
       " '存在': 400,\n",
       " '不好': 401,\n",
       " '开头': 402,\n",
       " '感人': 403,\n",
       " '关系': 404,\n",
       " '励志': 405,\n",
       " '第一次': 406,\n",
       " '明白': 407,\n",
       " '温情': 408,\n",
       " '总': 409,\n",
       " '兄弟': 410,\n",
       " '吃': 411,\n",
       " '甚至': 412,\n",
       " '惊喜': 413,\n",
       " '相当': 414,\n",
       " '多少': 415,\n",
       " '重要': 416,\n",
       " '拿': 417,\n",
       " '意义': 418,\n",
       " '带': 419,\n",
       " '方式': 420,\n",
       " '文艺': 421,\n",
       " '回忆': 422,\n",
       " '所': 423,\n",
       " '全片': 424,\n",
       " '也许': 425,\n",
       " '变成': 426,\n",
       " '真心': 427,\n",
       " '好莱坞': 428,\n",
       " '很棒': 429,\n",
       " '哈哈': 430,\n",
       " '超级': 431,\n",
       " '漂亮': 432,\n",
       " '容易': 433,\n",
       " '另外': 434,\n",
       " '快': 435,\n",
       " '味道': 436,\n",
       " '韩国': 437,\n",
       " '设定': 438,\n",
       " '依旧': 439,\n",
       " '玩': 440,\n",
       " '自然': 441,\n",
       " '大概': 442,\n",
       " '够': 443,\n",
       " '历史': 444,\n",
       " '制作': 445,\n",
       " '三个': 446,\n",
       " '当然': 447,\n",
       " '表达': 448,\n",
       " '煽情': 449,\n",
       " '任何': 450,\n",
       " '以后': 451,\n",
       " '脸': 452,\n",
       " '靠': 453,\n",
       " '有意思': 454,\n",
       " '明显': 455,\n",
       " '该': 456,\n",
       " '名字': 457,\n",
       " '朋友': 458,\n",
       " '想象': 459,\n",
       " '越': 460,\n",
       " '不了': 461,\n",
       " '基本': 462,\n",
       " '清新': 463,\n",
       " '是不是': 464,\n",
       " '而是': 465,\n",
       " '选择': 466,\n",
       " '梦想': 467,\n",
       " '设计': 468,\n",
       " '打斗': 469,\n",
       " '直接': 470,\n",
       " '突然': 471,\n",
       " '小时候': 472,\n",
       " '哦': 473,\n",
       " '不少': 474,\n",
       " '主演': 475,\n",
       " '国产': 476,\n",
       " '不到': 477,\n",
       " '最佳': 478,\n",
       " '几乎': 479,\n",
       " '不行': 480,\n",
       " '整部': 481,\n",
       " '如何': 482,\n",
       " '片中': 483,\n",
       " '显得': 484,\n",
       " '手法': 485,\n",
       " '四星': 486,\n",
       " '好好': 487,\n",
       " '改编': 488,\n",
       " '一颗': 489,\n",
       " '政治': 490,\n",
       " '最大': 491,\n",
       " '加': 492,\n",
       " '之一': 493,\n",
       " '垃圾': 494,\n",
       " '台湾': 495,\n",
       " '豆瓣': 496,\n",
       " '意思': 497,\n",
       " '听': 498,\n",
       " '情绪': 499,\n",
       " '人类': 500,\n",
       " '套路': 501,\n",
       " '男主角': 502,\n",
       " '刻意': 503,\n",
       " '向': 504,\n",
       " '写': 505,\n",
       " '事情': 506,\n",
       " '元素': 507,\n",
       " '平淡': 508,\n",
       " '动作片': 509,\n",
       " '以前': 510,\n",
       " '后来': 511,\n",
       " '越来越': 512,\n",
       " '大片': 513,\n",
       " '成长': 514,\n",
       " '如': 515,\n",
       " '高潮': 516,\n",
       " '暴力': 517,\n",
       " '老套': 518,\n",
       " '那样': 519,\n",
       " '好多': 520,\n",
       " '老师': 521,\n",
       " '当时': 522,\n",
       " '别人': 523,\n",
       " '温暖': 524,\n",
       " '震撼': 525,\n",
       " '难看': 526,\n",
       " '黑': 527,\n",
       " '相信': 528,\n",
       " '另': 529,\n",
       " '呵呵': 530,\n",
       " '处理': 531,\n",
       " '最终': 532,\n",
       " '不想': 533,\n",
       " '出彩': 534,\n",
       " '过于': 535,\n",
       " '残酷': 536,\n",
       " '心': 537,\n",
       " '来看': 538,\n",
       " '反派': 539,\n",
       " '过去': 540,\n",
       " '3d': 541,\n",
       " '一句': 542,\n",
       " '精神': 543,\n",
       " '背景': 544,\n",
       " '内心': 545,\n",
       " '所谓': 546,\n",
       " '内容': 547,\n",
       " '少年': 548,\n",
       " '棒': 549,\n",
       " '些': 550,\n",
       " '样子': 551,\n",
       " '前面': 552,\n",
       " '更好': 553,\n",
       " '似乎': 554,\n",
       " '之作': 555,\n",
       " '笑点': 556,\n",
       " '情怀': 557,\n",
       " '过程': 558,\n",
       " '少': 559,\n",
       " '长': 560,\n",
       " '帅': 561,\n",
       " '有人': 562,\n",
       " '美丽': 563,\n",
       " '一群': 564,\n",
       " '拍摄': 565,\n",
       " '女性': 566,\n",
       " '没想到': 567,\n",
       " '想要': 568,\n",
       " '大叔': 569,\n",
       " '告诉': 570,\n",
       " '致敬': 571,\n",
       " '根本': 572,\n",
       " '此片': 573,\n",
       " '接受': 574,\n",
       " '桥段': 575,\n",
       " '还好': 576,\n",
       " '至少': 577,\n",
       " '已': 578,\n",
       " '像是': 579,\n",
       " '水准': 580,\n",
       " '努力': 581,\n",
       " '恶心': 582,\n",
       " '就算': 583,\n",
       " '推荐': 584,\n",
       " '跑': 585,\n",
       " '请': 586,\n",
       " '原著': 587,\n",
       " '莫名其妙': 588,\n",
       " '细腻': 589,\n",
       " '父亲': 590,\n",
       " '加上': 591,\n",
       " '幸福': 592,\n",
       " '记忆': 593,\n",
       " '这次': 594,\n",
       " '结构': 595,\n",
       " '哪里': 596,\n",
       " '是因为': 597,\n",
       " '感受': 598,\n",
       " '赞': 599,\n",
       " '钱': 600,\n",
       " 'b': 601,\n",
       " '下去': 602,\n",
       " '游戏': 603,\n",
       " '儿子': 604,\n",
       " '童年': 605,\n",
       " '形象': 606,\n",
       " '结束': 607,\n",
       " '即使': 608,\n",
       " '孤独': 609,\n",
       " '主旋律': 610,\n",
       " '继续': 611,\n",
       " '伟大': 612,\n",
       " '好听': 613,\n",
       " '拖沓': 614,\n",
       " '难得': 615,\n",
       " '配音': 616,\n",
       " '欢乐': 617,\n",
       " '今天': 618,\n",
       " '典型': 619,\n",
       " '曾经': 620,\n",
       " '俗套': 621,\n",
       " '同样': 622,\n",
       " '少女': 623,\n",
       " '十分': 624,\n",
       " '才能': 625,\n",
       " '警察': 626,\n",
       " '港片': 627,\n",
       " '做作': 628,\n",
       " '人们': 629,\n",
       " '反而': 630,\n",
       " '到位': 631,\n",
       " '气质': 632,\n",
       " '梦': 633,\n",
       " '水平': 634,\n",
       " '妈': 635,\n",
       " '成龙': 636,\n",
       " '然而': 637,\n",
       " '一生': 638,\n",
       " '部': 639,\n",
       " '想到': 640,\n",
       " '妈妈': 641,\n",
       " '自由': 642,\n",
       " '可怕': 643,\n",
       " '童话': 644,\n",
       " '找到': 645,\n",
       " '这里': 646,\n",
       " '囧': 647,\n",
       " '影院': 648,\n",
       " '方面': 649,\n",
       " '意外': 650,\n",
       " '看来': 651,\n",
       " '生硬': 652,\n",
       " '厉害': 653,\n",
       " '观影': 654,\n",
       " '发生': 655,\n",
       " '女儿': 656,\n",
       " '母亲': 657,\n",
       " '吸引': 658,\n",
       " '讨厌': 659,\n",
       " '字幕': 660,\n",
       " '或许': 661,\n",
       " '或': 662,\n",
       " '悲剧': 663,\n",
       " '国家': 664,\n",
       " '浪费': 665,\n",
       " '小说': 666,\n",
       " '剧': 667,\n",
       " '今年': 668,\n",
       " '塑造': 669,\n",
       " '色彩': 670,\n",
       " '浪漫': 671,\n",
       " '自我': 672,\n",
       " '奥斯卡': 673,\n",
       " '看得': 674,\n",
       " '全程': 675,\n",
       " '其': 676,\n",
       " '还要': 677,\n",
       " '紧凑': 678,\n",
       " '影响': 679,\n",
       " '惊艳': 680,\n",
       " '片尾': 681,\n",
       " '爆': 682,\n",
       " '视角': 683,\n",
       " '开心': 684,\n",
       " '出色': 685,\n",
       " '男女': 686,\n",
       " '更是': 687,\n",
       " '过瘾': 688,\n",
       " '改变': 689,\n",
       " '亲情': 690,\n",
       " '爸爸': 691,\n",
       " '神': 692,\n",
       " '其中': 693,\n",
       " '或者': 694,\n",
       " '低': 695,\n",
       " '印度': 696,\n",
       " '狗': 697,\n",
       " '讲述': 698,\n",
       " '黑暗': 699,\n",
       " '刻画': 700,\n",
       " '讽刺': 701,\n",
       " '坚持': 702,\n",
       " '压抑': 703,\n",
       " '语言': 704,\n",
       " '未来': 705,\n",
       " '杀': 706,\n",
       " '电视剧': 707,\n",
       " '最近': 708,\n",
       " '肯定': 709,\n",
       " '的确': 710,\n",
       " '不用': 711,\n",
       " '中间': 712,\n",
       " '热血': 713,\n",
       " '温馨': 714,\n",
       " '动人': 715,\n",
       " '经历': 716,\n",
       " '新意': 717,\n",
       " '声音': 718,\n",
       " '同时': 719,\n",
       " '小孩': 720,\n",
       " '多么': 721,\n",
       " '文化': 722,\n",
       " '冲突': 723,\n",
       " '两星': 724,\n",
       " '难': 725,\n",
       " '青春片': 726,\n",
       " '上映': 727,\n",
       " '哪': 728,\n",
       " '更加': 729,\n",
       " '而已': 730,\n",
       " '不管': 731,\n",
       " '空间': 732,\n",
       " '混乱': 733,\n",
       " '岁': 734,\n",
       " '宗教': 735,\n",
       " '科幻': 736,\n",
       " '鬼': 737,\n",
       " '大师': 738,\n",
       " '属于': 739,\n",
       " '心里': 740,\n",
       " '心理': 741,\n",
       " '智商': 742,\n",
       " '轻松': 743,\n",
       " '眼睛': 744,\n",
       " '美女': 745,\n",
       " '命运': 746,\n",
       " '失去': 747,\n",
       " '技术': 748,\n",
       " '好笑': 749,\n",
       " '力量': 750,\n",
       " '死亡': 751,\n",
       " '无论': 752,\n",
       " '只要': 753,\n",
       " '電影': 754,\n",
       " '见': 755,\n",
       " '平庸': 756,\n",
       " '主要': 757,\n",
       " '起': 758,\n",
       " '纯粹': 759,\n",
       " '相比': 760,\n",
       " '造型': 761,\n",
       " '悬疑': 762,\n",
       " '哎': 763,\n",
       " '本': 764,\n",
       " '十足': 765,\n",
       " '看起来': 766,\n",
       " '完整': 767,\n",
       " '拯救': 768,\n",
       " '大陆': 769,\n",
       " '掉': 770,\n",
       " '评分': 771,\n",
       " '黑色幽默': 772,\n",
       " '配角': 773,\n",
       " '流畅': 774,\n",
       " '明星': 775,\n",
       " '线': 776,\n",
       " '面对': 777,\n",
       " '明明': 778,\n",
       " '必须': 779,\n",
       " '演得': 780,\n",
       " '冲着': 781,\n",
       " '别': 782,\n",
       " '忘': 783,\n",
       " '行': 784,\n",
       " '疯狂': 785,\n",
       " '且': 786,\n",
       " '事件': 787,\n",
       " '能够': 788,\n",
       " '城市': 789,\n",
       " '不知': 790,\n",
       " '梗': 791,\n",
       " '估计': 792,\n",
       " '艺术': 793,\n",
       " '后半段': 794,\n",
       " '魅力': 795,\n",
       " '还行': 796,\n",
       " '不得不': 797,\n",
       " '超': 798,\n",
       " '在于': 799,\n",
       " '强大': 800,\n",
       " '续集': 801,\n",
       " '一遍': 802,\n",
       " '要是': 803,\n",
       " '毕竟': 804,\n",
       " '奇怪': 805,\n",
       " '第二部': 806,\n",
       " '长镜头': 807,\n",
       " '枪战': 808,\n",
       " '不断': 809,\n",
       " '略': 810,\n",
       " '人家': 811,\n",
       " '对白': 812,\n",
       " '太多': 813,\n",
       " '渣': 814,\n",
       " '失败': 815,\n",
       " '展现': 816,\n",
       " '女孩': 817,\n",
       " '多年': 818,\n",
       " '强': 819,\n",
       " '西部片': 820,\n",
       " '得到': 821,\n",
       " '每次': 822,\n",
       " '女神': 823,\n",
       " '广告': 824,\n",
       " '下来': 825,\n",
       " '二': 826,\n",
       " '功夫': 827,\n",
       " '表情': 828,\n",
       " '海报': 829,\n",
       " '思考': 830,\n",
       " '创意': 831,\n",
       " '华丽': 832,\n",
       " '夸张': 833,\n",
       " '发展': 834,\n",
       " '月': 835,\n",
       " '无奈': 836,\n",
       " '复杂': 837,\n",
       " '哥哥': 838,\n",
       " '用心': 839,\n",
       " '传统': 840,\n",
       " '信仰': 841,\n",
       " '正': 842,\n",
       " '普通': 843,\n",
       " '痛苦': 844,\n",
       " '形式': 845,\n",
       " '看不下去': 846,\n",
       " '能力': 847,\n",
       " '舒服': 848,\n",
       " '片名': 849,\n",
       " '始终': 850,\n",
       " '比如': 851,\n",
       " '睡着': 852,\n",
       " '认为': 853,\n",
       " '角度': 854,\n",
       " '内': 855,\n",
       " '不算': 856,\n",
       " '槽': 857,\n",
       " '假': 858,\n",
       " '反正': 859,\n",
       " '吐': 860,\n",
       " '弱': 861,\n",
       " '乱': 862,\n",
       " '气氛': 863,\n",
       " '瞎': 864,\n",
       " '一半': 865,\n",
       " '放': 866,\n",
       " '通过': 867,\n",
       " '刺激': 868,\n",
       " '仍然': 869,\n",
       " '戏份': 870,\n",
       " '怎样': 871,\n",
       " '武士': 872,\n",
       " '变': 873,\n",
       " '一天': 874,\n",
       " '身上': 875,\n",
       " '恐怖': 876,\n",
       " '拍出': 877,\n",
       " '反转': 878,\n",
       " '杀手': 879,\n",
       " '狼': 880,\n",
       " '法国': 881,\n",
       " '10': 882,\n",
       " '有种': 883,\n",
       " '想法': 884,\n",
       " '想象力': 885,\n",
       " '20': 886,\n",
       " '性格': 887,\n",
       " '程度': 888,\n",
       " '年度': 889,\n",
       " '类似': 890,\n",
       " '动画片': 891,\n",
       " '遗憾': 892,\n",
       " '间': 893,\n",
       " '长大': 894,\n",
       " '唉': 895,\n",
       " '不再': 896,\n",
       " '三': 897,\n",
       " '化': 898,\n",
       " '风景': 899,\n",
       " '绝望': 900,\n",
       " '足够': 901,\n",
       " '太过': 902,\n",
       " '铺垫': 903,\n",
       " '尽管': 904,\n",
       " '犯罪': 905,\n",
       " '原因': 906,\n",
       " '小女孩': 907,\n",
       " '眼泪': 908,\n",
       " '无比': 909,\n",
       " '矫情': 910,\n",
       " '打动': 911,\n",
       " '却是': 912,\n",
       " '天才': 913,\n",
       " '天': 914,\n",
       " '总体': 915,\n",
       " '超越': 916,\n",
       " '治愈': 917,\n",
       " '前半段': 918,\n",
       " '视觉': 919,\n",
       " '则': 920,\n",
       " '变得': 921,\n",
       " '美的': 922,\n",
       " '很大': 923,\n",
       " '黑色': 924,\n",
       " '比起': 925,\n",
       " '尼玛': 926,\n",
       " '带来': 927,\n",
       " '评价': 928,\n",
       " '血腥': 929,\n",
       " '由': 930,\n",
       " '颜值': 931,\n",
       " '仍': 932,\n",
       " '票房': 933,\n",
       " '片段': 934,\n",
       " '影像': 935,\n",
       " '演出': 936,\n",
       " '模仿': 937,\n",
       " '评论': 938,\n",
       " '人心': 939,\n",
       " '影子': 940,\n",
       " '紧张': 941,\n",
       " '商业': 942,\n",
       " '悲伤': 943,\n",
       " '矛盾': 944,\n",
       " '反': 945,\n",
       " '爆米花': 946,\n",
       " '彩蛋': 947,\n",
       " '全部': 948,\n",
       " '氛围': 949,\n",
       " '字': 950,\n",
       " '版本': 951,\n",
       " '心中': 952,\n",
       " '嗯': 953,\n",
       " '啊啊啊': 954,\n",
       " '既': 955,\n",
       " '第一': 956,\n",
       " '路': 957,\n",
       " '快乐': 958,\n",
       " '酱油': 959,\n",
       " '家': 960,\n",
       " '三部曲': 961,\n",
       " '两部': 962,\n",
       " '亦': 963,\n",
       " '认真': 964,\n",
       " '眼神': 965,\n",
       " '飞机': 966,\n",
       " '荒诞': 967,\n",
       " '差点': 968,\n",
       " '电视': 969,\n",
       " '熟悉': 970,\n",
       " '猜': 971,\n",
       " '非': 972,\n",
       " '說': 973,\n",
       " '精致': 974,\n",
       " '穿越': 975,\n",
       " '穿': 976,\n",
       " '灵魂': 977,\n",
       " '演绎': 978,\n",
       " '者': 979,\n",
       " '缺乏': 980,\n",
       " '了解': 981,\n",
       " '难道': 982,\n",
       " '强烈': 983,\n",
       " '之外': 984,\n",
       " '追求': 985,\n",
       " '做到': 986,\n",
       " '对话': 987,\n",
       " '欣赏': 988,\n",
       " '商业片': 989,\n",
       " '回来': 990,\n",
       " '看似': 991,\n",
       " '设置': 992,\n",
       " '科恩': 993,\n",
       " '站': 994,\n",
       " '上帝': 995,\n",
       " '许多': 996,\n",
       " '演戏': 997,\n",
       " '同': 998,\n",
       " '般的': 999,\n",
       " '功力': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_vector = word2vec.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.99132760e-02, -7.90739432e-02,  1.13177627e-01, -1.02263480e-01,\n",
       "       -3.94922882e-01,  2.21264675e-01,  2.71036059e-01,  1.06118187e-01,\n",
       "        2.08997309e-01,  1.92035481e-01,  9.52772275e-02,  4.98892032e-02,\n",
       "        1.67051002e-01, -6.31192252e-02, -2.05391739e-03,  2.50397563e-01,\n",
       "        1.44434823e-02,  3.01672798e-02,  3.25987071e-01, -2.19077691e-01,\n",
       "        2.93559253e-01, -2.34922245e-01, -1.79424360e-01,  1.64352819e-01,\n",
       "        1.51898980e-01,  2.98251808e-02, -2.57482771e-02, -2.04278648e-01,\n",
       "       -8.56961161e-02, -1.02568209e-01,  1.34455618e-02,  1.51726797e-01,\n",
       "        1.96797885e-02, -4.69660051e-02,  2.20610946e-01, -1.26982492e-03,\n",
       "       -1.34490758e-01,  1.36391083e-02, -7.30082616e-02,  1.82307929e-01,\n",
       "        3.42141949e-02, -5.34046143e-02,  2.22519226e-02, -2.02291816e-01,\n",
       "       -2.73989201e-01,  7.76515231e-02, -4.33010422e-02,  3.51540074e-02,\n",
       "        2.10840598e-01,  3.01478028e-01,  9.44831669e-02,  4.01540101e-02,\n",
       "       -4.17172164e-01, -2.70549562e-02, -3.47403735e-02, -4.23471481e-02,\n",
       "        6.08665764e-01,  4.15374756e-01, -2.25687236e-01,  1.62677020e-01,\n",
       "       -2.09592313e-01,  1.21177323e-01,  1.57458887e-01, -2.82887280e-01,\n",
       "       -5.59653640e-02,  1.28729388e-01, -1.43838868e-01, -4.23701823e-01,\n",
       "        1.30024508e-01,  4.93946746e-02,  8.62544104e-02, -1.27994567e-02,\n",
       "        1.47197664e-01,  4.64918435e-01, -3.77034172e-02,  1.75588913e-02,\n",
       "        2.52685934e-01, -2.17303857e-01, -5.71172386e-05,  5.55024222e-02,\n",
       "        1.34064302e-01,  2.58622202e-03, -9.69883278e-02, -1.90378711e-01,\n",
       "       -4.39035855e-02, -9.66649503e-02, -6.23374581e-02, -1.81251809e-01,\n",
       "        2.23780349e-01, -2.94559021e-02, -1.64394826e-01,  6.85690865e-02,\n",
       "       -3.20584744e-01, -2.59402514e-01,  5.31704575e-02, -5.43848686e-02,\n",
       "        4.62754011e-01,  1.22401766e-01,  7.94423223e-02, -2.81430542e-01,\n",
       "       -1.84564628e-02, -2.90043533e-01,  1.73245192e-01,  2.08372265e-01,\n",
       "       -1.04899511e-01, -3.31053227e-01, -4.77016643e-02, -1.49242893e-01,\n",
       "        2.04734564e-01, -6.00487180e-02,  1.37521908e-01,  3.84803209e-03,\n",
       "       -6.62231212e-03, -2.11595014e-01, -1.81718692e-02,  3.22145931e-02,\n",
       "       -7.53551275e-02,  1.09019823e-01,  1.64338678e-01, -2.99719386e-02,\n",
       "        3.47276628e-01, -1.45182654e-01,  1.86992601e-01, -2.36188155e-02,\n",
       "        2.22027630e-01, -5.16194940e-01, -4.88908291e-01, -3.91844027e-02,\n",
       "       -9.84919071e-03,  2.56016888e-02, -1.69673681e-01, -7.04775676e-02,\n",
       "       -6.69516297e-03, -7.23478049e-02, -1.08673848e-01,  2.23646179e-01,\n",
       "        8.53678659e-02, -5.87283447e-03,  1.14194788e-01, -7.96703324e-02,\n",
       "        1.68017805e-01,  7.99409449e-02, -4.66409139e-02,  5.30595332e-02,\n",
       "       -1.32575393e-01, -1.05436996e-01, -3.28834444e-01,  1.42692635e-02,\n",
       "        8.16609412e-02, -2.47100726e-01, -6.92975074e-02, -1.92240104e-01,\n",
       "        8.70183483e-02,  2.00549979e-02, -1.35690674e-01,  1.24433031e-02,\n",
       "        1.31326571e-01,  1.41676128e-01,  3.23049992e-01, -1.61330923e-01,\n",
       "        4.07349691e-02, -5.00191264e-02, -2.10985318e-01, -6.50112480e-02,\n",
       "       -9.20290649e-02, -3.11037868e-01,  2.74616815e-02, -8.15889761e-02,\n",
       "       -1.75646678e-01,  1.91146508e-01, -5.69860525e-02,  2.03338429e-01,\n",
       "        1.32943073e-03,  2.90341228e-01, -1.92829683e-01,  4.60949801e-02,\n",
       "       -7.80315250e-02, -1.61241487e-01,  9.38915648e-03,  1.60939425e-01,\n",
       "        1.85390547e-01,  1.14358164e-01, -1.66309819e-01,  8.93667191e-02,\n",
       "        7.82728419e-02, -1.67757154e-01,  5.26069030e-02,  3.86021882e-01,\n",
       "       -7.14097694e-02, -5.20179197e-02,  1.67845219e-01,  6.52791932e-02,\n",
       "        3.52061689e-02,  5.45030355e-01,  7.95668960e-02,  1.27477229e-01,\n",
       "        1.64106175e-01,  2.63101280e-01,  2.87090298e-02, -1.53726831e-01,\n",
       "       -1.21973783e-01, -3.24348956e-02,  1.45173430e-01, -3.22030969e-02,\n",
       "       -1.96549334e-02, -9.45345461e-02,  8.35775137e-02,  1.78912193e-01,\n",
       "       -3.54545325e-01, -2.46907249e-01,  2.11842194e-01, -2.50247449e-01,\n",
       "       -1.40224636e-01, -3.66151989e-01,  2.15101331e-01,  1.05836943e-01,\n",
       "       -1.22857966e-01, -5.23130670e-02,  1.08491875e-01, -9.65045691e-02,\n",
       "       -4.94949445e-02, -6.47566989e-02, -1.82808042e-01,  1.99099332e-01,\n",
       "       -2.72649556e-01,  6.01799712e-02,  4.57828166e-03, -4.71366644e-02,\n",
       "       -4.18569982e-01, -1.42311215e-01,  1.76651821e-01, -8.67891386e-02,\n",
       "        4.63915616e-02,  7.41402060e-02,  8.27540457e-02, -1.73974574e-01,\n",
       "       -2.95579713e-02, -4.27781232e-02,  1.61777139e-01, -1.09456517e-01,\n",
       "        2.84799904e-01, -1.99391931e-01,  1.12014703e-01, -7.57752955e-02,\n",
       "        3.08220714e-01,  3.47125018e-03,  4.13859114e-02,  1.81046613e-02,\n",
       "        2.82183200e-01, -3.21797282e-02,  7.76868984e-02,  1.13193735e-01,\n",
       "       -2.41300687e-01, -6.35438561e-02,  1.03865683e-01,  1.38367876e-01,\n",
       "        1.46296605e-01, -6.19513122e-03, -1.18903399e-01,  2.14462262e-03,\n",
       "       -4.56755698e-01,  1.52382717e-01, -6.55708537e-02, -1.32690100e-02,\n",
       "       -2.66459823e-01, -9.97923166e-02,  1.50235534e-01,  1.85124144e-01,\n",
       "       -2.24164665e-01, -4.04942930e-01,  1.58094957e-01,  1.98956728e-01,\n",
       "        1.26629010e-01,  8.54004174e-02, -1.04934275e-01,  9.28411111e-02,\n",
       "        1.02396391e-01,  2.88748443e-01, -1.10252298e-01,  3.77716906e-02,\n",
       "       -2.55087763e-01, -2.41092622e-01, -1.33778691e-01,  5.07185683e-02,\n",
       "        1.49742723e-01, -1.07364140e-01,  7.89088309e-02, -1.98224738e-01,\n",
       "       -1.73818544e-01, -1.98864177e-01,  1.99036926e-01, -5.46124503e-02,\n",
       "       -2.58410536e-02,  2.13147432e-01, -7.88981002e-03,  2.01673299e-01,\n",
       "       -1.24189280e-01, -2.34827369e-01,  1.34991091e-02, -8.05512592e-02,\n",
       "        3.07462305e-01, -1.19663082e-01,  1.65290460e-01, -3.90913039e-02,\n",
       "        6.58328505e-03,  7.95055553e-02, -3.17060538e-02,  1.59565702e-01,\n",
       "       -2.72466630e-01, -1.66858673e-01,  3.28013718e-01,  9.38003138e-02,\n",
       "       -5.60287498e-02,  1.52690131e-02, -1.06525041e-01,  3.71952504e-02,\n",
       "        1.05972454e-01,  1.02866694e-01,  2.23674878e-01, -2.93509483e-01,\n",
       "       -1.86218724e-01,  3.68828833e-01,  3.70422332e-03,  1.02638654e-01,\n",
       "        4.80861105e-02,  6.70745373e-02,  2.02360690e-01, -1.02683768e-01,\n",
       "       -1.12381484e-03,  2.96716869e-01,  2.06179708e-01,  1.25432014e-01,\n",
       "       -1.33612365e-01, -1.01354301e-01,  4.08335805e-01, -1.44734651e-01,\n",
       "        9.42511782e-02, -2.12261043e-02, -7.61568220e-03, -5.32431342e-02,\n",
       "        1.16835013e-01,  3.21169645e-01,  1.73180759e-01,  1.22949146e-01,\n",
       "       -1.40668461e-02,  1.98743805e-01,  6.77471235e-02, -9.28719789e-02,\n",
       "        2.04948381e-01, -1.84977725e-02, -3.18887178e-03,  5.03000095e-02,\n",
       "        2.07975164e-01,  1.36092544e-01,  1.18567102e-01,  1.26267001e-01,\n",
       "        1.14043271e-02, -4.36713137e-02, -4.04645316e-02, -3.02992705e-02,\n",
       "        2.54104257e-01,  4.17903997e-02, -1.36289507e-01, -1.02575630e-01,\n",
       "        3.17175798e-02, -3.26482989e-02,  9.86449718e-02, -2.19922438e-01,\n",
       "       -4.94770944e-01, -1.34746775e-01,  1.95571147e-02, -3.17022353e-02,\n",
       "       -8.28376934e-02, -2.98039228e-01,  2.64445782e-01,  6.39448762e-02,\n",
       "       -1.68403983e-01,  5.62184975e-02,  1.68841466e-01, -5.36470227e-02,\n",
       "        1.30023137e-01, -2.36347746e-02,  1.61236510e-01,  1.44462362e-01,\n",
       "        9.71134380e-02,  4.81924385e-01,  1.49764448e-01,  2.03358009e-01,\n",
       "       -5.99394478e-02,  6.79216012e-02,  2.00615063e-01, -3.90149057e-02,\n",
       "       -1.24569237e-02,  5.43212593e-02,  3.61541435e-02,  1.83923934e-02,\n",
       "       -2.71804873e-02,  1.76343888e-01, -3.08107644e-01, -1.50618389e-01,\n",
       "       -1.48357853e-01, -1.84345841e-01,  1.05861738e-01,  3.63017097e-02,\n",
       "       -3.69201861e-02, -2.53241450e-01, -4.98845018e-02,  6.93432288e-03,\n",
       "        3.88137013e-01, -1.09337330e-01,  1.44855812e-01, -7.04814196e-02,\n",
       "        1.01148514e-02,  7.64561892e-02, -6.43902719e-02, -1.46899046e-02,\n",
       "        1.78685591e-01, -1.54432043e-01, -1.38810977e-01, -1.06156655e-01,\n",
       "        5.04090674e-02,  1.00502089e-01,  7.61534926e-03, -2.78505571e-02,\n",
       "       -8.73658508e-02, -1.91425875e-01,  1.06062330e-01,  8.64792615e-02,\n",
       "       -2.65985936e-01,  1.87201872e-01, -7.40243271e-02,  1.52853087e-01,\n",
       "        1.05717734e-01,  1.52022704e-01, -3.80768180e-01,  3.93545255e-02,\n",
       "       -5.70523031e-02, -2.55627692e-01,  1.59787640e-01,  2.14431420e-01,\n",
       "       -8.76766816e-02, -2.60978471e-02, -1.52852297e-01,  1.57054085e-02,\n",
       "        8.39821771e-02,  8.82306546e-02,  3.53709375e-03, -2.48712555e-01,\n",
       "       -1.06800392e-01, -3.27854138e-03, -1.64931208e-01, -3.05236995e-01,\n",
       "        8.36815983e-02, -2.81623900e-02, -2.40631863e-01,  1.97017323e-02,\n",
       "        2.23959804e-01,  2.26214856e-01, -5.23918979e-02, -6.75672442e-02,\n",
       "        5.42503782e-02,  3.08020618e-02,  1.92231417e-01,  2.16290161e-01,\n",
       "        1.50752708e-01,  2.15135485e-01, -1.63666710e-01,  3.12661082e-01,\n",
       "        8.43932554e-02,  7.90946484e-02, -2.38119155e-01,  5.60978830e-01,\n",
       "       -1.56321466e-01, -1.98892623e-01,  1.07967257e-01, -1.07303791e-01,\n",
       "       -4.78412956e-02,  2.99339741e-02,  1.63374573e-01, -8.16383138e-02,\n",
       "       -1.13129802e-03,  1.93494856e-01, -1.01005241e-01,  1.83932781e-01,\n",
       "        1.63622908e-02,  2.96653770e-02,  9.99718457e-02, -3.56399477e-01,\n",
       "       -4.09209579e-01,  5.17754182e-02,  1.32153183e-01, -1.78057969e-01,\n",
       "        2.02744961e-01,  1.17329046e-01, -4.71788784e-03, -6.12913407e-02,\n",
       "       -5.70695847e-03, -5.20367660e-02,  1.81540236e-01,  1.11439608e-01])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            # verbose = 0 为不在标准输出流输出日志信息\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "#     x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(5, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171175, 100)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 171175 samples, validate on 42794 samples\n",
      "Epoch 1/2\n",
      " - 2820s - loss: 0.4287 - accuracy: 0.8035 - val_loss: 0.4079 - val_accuracy: 0.8077\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.768180 \n",
      "\n",
      "Epoch 2/2\n",
      " - 2702s - loss: 0.3840 - accuracy: 0.8188 - val_loss: 0.4055 - val_accuracy: 0.8077\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.771471 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(x_test, y_test), interval=1)\n",
    "\n",
    "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test),\n",
    "                 callbacks=[RocAuc], verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 171175 samples, validate on 42794 samples\n",
      "Epoch 1/10\n",
      " - 2826s - loss: 0.4290 - accuracy: 0.8033 - val_loss: 0.4077 - val_accuracy: 0.8063\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.768108 \n",
      "\n",
      "Epoch 2/10\n",
      " - 2645s - loss: 0.3839 - accuracy: 0.8188 - val_loss: 0.4093 - val_accuracy: 0.8074\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.770490 \n",
      "\n",
      "Epoch 3/10\n",
      " - 3168s - loss: 0.3495 - accuracy: 0.8389 - val_loss: 0.4183 - val_accuracy: 0.8038\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.764857 \n",
      "\n",
      "Epoch 4/10\n",
      " - 2881s - loss: 0.3161 - accuracy: 0.8582 - val_loss: 0.4395 - val_accuracy: 0.7996\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.755710 \n",
      "\n",
      "Epoch 5/10\n",
      " - 2722s - loss: 0.2861 - accuracy: 0.8745 - val_loss: 0.4638 - val_accuracy: 0.7953\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.747633 \n",
      "\n",
      "Epoch 6/10\n",
      " - 2808s - loss: 0.2586 - accuracy: 0.8891 - val_loss: 0.4939 - val_accuracy: 0.7910\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.740757 \n",
      "\n",
      "Epoch 7/10\n",
      " - 2683s - loss: 0.2348 - accuracy: 0.9010 - val_loss: 0.5284 - val_accuracy: 0.7874\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.736117 \n",
      "\n",
      "Epoch 8/10\n",
      " - 2623s - loss: 0.2133 - accuracy: 0.9115 - val_loss: 0.5586 - val_accuracy: 0.7882\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.730340 \n",
      "\n",
      "Epoch 9/10\n",
      " - 2668s - loss: 0.1947 - accuracy: 0.9206 - val_loss: 0.5821 - val_accuracy: 0.7846\n",
      "\n",
      " ROC-AUC - epoch: 9 - score: 0.726182 \n",
      "\n",
      "Epoch 10/10\n",
      " - 2620s - loss: 0.1786 - accuracy: 0.9281 - val_loss: 0.6171 - val_accuracy: 0.7822\n",
      "\n",
      " ROC-AUC - epoch: 10 - score: 0.723332 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(x_test, y_test), interval=1)\n",
    "\n",
    "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test),\n",
    "                 callbacks=[RocAuc], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "## 为什么加上这句会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(5, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 171175 samples, validate on 42794 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "AlreadyExistsError",
     "evalue": "Resource __per_step_30411/training_5/Adam/gradients/bidirectional_3/while_1/ReadVariableOp/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\n\t [[{{node training_5/Adam/gradients/bidirectional_3/while_1/ReadVariableOp/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-052b37e28386>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test),\n\u001b[1;32m---> 11\u001b[1;33m                  callbacks=[RocAuc], verbose=2)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\AI\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\AI\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAlreadyExistsError\u001b[0m: Resource __per_step_30411/training_5/Adam/gradients/bidirectional_3/while_1/ReadVariableOp/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\n\t [[{{node training_5/Adam/gradients/bidirectional_3/while_1/ReadVariableOp/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var}}]]"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(x_test, y_test), interval=1)\n",
    "\n",
    "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test),\n",
    "                 callbacks=[RocAuc], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100\n",
    "vocabulary_size = 50000\n",
    "embedding_dim = 500\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model...\n",
      "Traning Model...\n",
      "Train on 171175 samples, validate on 42794 samples\n",
      "Epoch 1/5\n",
      "171175/171175 [==============================] - 8830s 52ms/step - loss: 0.4345 - accuracy: 0.8038 - val_loss: 0.4057 - val_accuracy: 0.8089\n",
      "Epoch 2/5\n",
      "171175/171175 [==============================] - 8321s 49ms/step - loss: 0.3797 - accuracy: 0.8226 - val_loss: 0.4010 - val_accuracy: 0.8096\n",
      "Epoch 3/5\n",
      "171175/171175 [==============================] - 9216s 54ms/step - loss: 0.3422 - accuracy: 0.8457 - val_loss: 0.4078 - val_accuracy: 0.8067\n",
      "Epoch 4/5\n",
      "171175/171175 [==============================] - 8727s 51ms/step - loss: 0.2998 - accuracy: 0.8713 - val_loss: 0.4223 - val_accuracy: 0.8020\n",
      "Epoch 5/5\n",
      "171175/171175 [==============================] - 8200s 48ms/step - loss: 0.2550 - accuracy: 0.8956 - val_loss: 0.4445 - val_accuracy: 0.7972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1e744db6c50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this returns a tensor\n",
    "print(\"Creating Model...\")\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=5, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"Traning Model...\")\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))  # starts training callbacks=[checkpoint],\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
