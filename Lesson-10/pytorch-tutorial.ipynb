{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "#                         Table of Contents                          #\n",
    "# ================================================================== #\n",
    "\n",
    "# 1. Basic autograd example 1               (Line 25 to 39)\n",
    "# 2. Basic autograd example 2               (Line 46 to 83)\n",
    "# 3. Loading data from numpy                (Line 90 to 97)\n",
    "# 4. Input pipline                          (Line 104 to 129)\n",
    "# 5. Input pipline for custom dataset       (Line 136 to 156)\n",
    "# 6. Pretrained model                       (Line 163 to 176)\n",
    "# 7. Save and load model                    (Line 183 to 189) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "#                     1. Basic autograd example 1                    #\n",
    "# ================================================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors.\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients.\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "#                    2. Basic autograd example 2                     #\n",
    "# ================================================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[ 0.4992,  0.2770,  0.1992],\n",
      "        [ 0.1647, -0.1774, -0.5663]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([ 0.3061, -0.1481], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build loss function and optimizer.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass.\n",
    "pred = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.0415191650390625\n"
     ]
    }
   ],
   "source": [
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw:  tensor([[ 0.3719, -0.1274,  0.2646],\n",
      "        [-0.1529,  0.1879, -0.3598]])\n",
      "dL/db:  tensor([-0.3903, -0.0911])\n"
     ]
    }
   ],
   "source": [
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-step gradient descent.\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 step optimization:  1.035805344581604\n"
     ]
    }
   ],
   "source": [
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "#                     3. Loading data from numpy                     #\n",
    "# ================================================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array.\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Convert the numpy array to a torch tensor.\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "# Convert the torch tensor to a numpy array.\n",
    "z = y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                         4. Input pipline                           #\n",
    "# ================================================================== #\n",
    "\n",
    "# Download and construct CIFAR-10 dataset.\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)\n",
    "\n",
    "# Fetch one data pair (read data from disk).\n",
    "image, label = train_dataset[0]\n",
    "print (image.size())\n",
    "print (label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader (this provides queues and threads in a very simple way).\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "# Actual usage of the data loader is as below.\n",
    "for images, labels in train_loader:\n",
    "    # Training code should be written here.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-28982eb6adcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n\u001b[0;32m     24\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                                            shuffle=True)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context)\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# map-style\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_source, replacement, num_samples)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[1;32m---> 94\u001b[1;33m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                5. Input pipline for custom dataset                 #\n",
    "# ================================================================== #\n",
    "\n",
    "# You should build your custom dataset as below.\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "        # 1. Initialize file paths or a list of file names. \n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return 0 \n",
    "\n",
    "# You can then use the prebuilt data loader. \n",
    "custom_dataset = CustomDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里报错是因为没写对应函数内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to C:\\Users\\Mr. Wu/.cache\\torch\\checkpoints\\resnet18-5c106cde.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                        6. Pretrained model                         #\n",
    "# ================================================================== #\n",
    "\n",
    "# Download and load the pretrained ResNet-18.\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# If you want to finetune only the top layer of the model, set as below.\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the top layer for finetuning.\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example.\n",
    "\n",
    "# Forward pass.\n",
    "images = torch.randn(64, 3, 224, 224)\n",
    "outputs = resnet(images)\n",
    "print (outputs.size())     # (64, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "#                      7. Save and load the model                    #\n",
    "# ================================================================== #\n",
    "\n",
    "# Save and load the entire model.\n",
    "torch.save(resnet, 'model.ckpt')\n",
    "model = torch.load('model.ckpt')\n",
    "\n",
    "# Save and load only the model parameters (recommended).\n",
    "torch.save(resnet.state_dict(), 'params.ckpt')\n",
    "resnet.load_state_dict(torch.load('params.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Toy dataset\n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 4.8994\n",
      "Epoch [10/60], Loss: 2.1245\n",
      "Epoch [15/60], Loss: 1.0003\n",
      "Epoch [20/60], Loss: 0.5447\n",
      "Epoch [25/60], Loss: 0.3601\n",
      "Epoch [30/60], Loss: 0.2852\n",
      "Epoch [35/60], Loss: 0.2547\n",
      "Epoch [40/60], Loss: 0.2423\n",
      "Epoch [45/60], Loss: 0.2372\n",
      "Epoch [50/60], Loss: 0.2350\n",
      "Epoch [55/60], Loss: 0.2340\n",
      "Epoch [60/60], Loss: 0.2335\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linear regression model\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    inputs = torch.from_numpy(x_train)\n",
    "    targets = torch.from_numpy(y_train)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Plot the graph\n",
    "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset (images and labels)\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 2.2534\n",
      "Epoch [1/5], Step [200/600], Loss: 2.1395\n",
      "Epoch [1/5], Step [300/600], Loss: 2.0485\n",
      "Epoch [1/5], Step [400/600], Loss: 1.9770\n",
      "Epoch [1/5], Step [500/600], Loss: 1.8643\n",
      "Epoch [1/5], Step [600/600], Loss: 1.8293\n",
      "Epoch [2/5], Step [100/600], Loss: 1.7757\n",
      "Epoch [2/5], Step [200/600], Loss: 1.6627\n",
      "Epoch [2/5], Step [300/600], Loss: 1.5991\n",
      "Epoch [2/5], Step [400/600], Loss: 1.6044\n",
      "Epoch [2/5], Step [500/600], Loss: 1.4955\n",
      "Epoch [2/5], Step [600/600], Loss: 1.4027\n",
      "Epoch [3/5], Step [100/600], Loss: 1.4214\n",
      "Epoch [3/5], Step [200/600], Loss: 1.4436\n",
      "Epoch [3/5], Step [300/600], Loss: 1.3576\n",
      "Epoch [3/5], Step [400/600], Loss: 1.2695\n",
      "Epoch [3/5], Step [500/600], Loss: 1.2638\n",
      "Epoch [3/5], Step [600/600], Loss: 1.3191\n",
      "Epoch [4/5], Step [100/600], Loss: 1.2104\n",
      "Epoch [4/5], Step [200/600], Loss: 1.1853\n",
      "Epoch [4/5], Step [300/600], Loss: 1.1690\n",
      "Epoch [4/5], Step [400/600], Loss: 1.2242\n",
      "Epoch [4/5], Step [500/600], Loss: 1.2125\n",
      "Epoch [4/5], Step [600/600], Loss: 1.0205\n",
      "Epoch [5/5], Step [100/600], Loss: 1.0085\n",
      "Epoch [5/5], Step [200/600], Loss: 1.0658\n",
      "Epoch [5/5], Step [300/600], Loss: 1.0117\n",
      "Epoch [5/5], Step [400/600], Loss: 1.0562\n",
      "Epoch [5/5], Step [500/600], Loss: 1.0769\n",
      "Epoch [5/5], Step [600/600], Loss: 1.0231\n",
      "Accuracy of the model on the 10000 test images: 83 %\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression model\n",
    "model = nn.Linear(input_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "# nn.CrossEntropyLoss() computes softmax internally\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape images to (batch_size, input_size)\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.3070\n",
      "Epoch [1/5], Step [200/600], Loss: 0.2062\n",
      "Epoch [1/5], Step [300/600], Loss: 0.2144\n",
      "Epoch [1/5], Step [400/600], Loss: 0.2050\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1291\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0825\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0569\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1050\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0758\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0716\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0689\n",
      "Epoch [2/5], Step [600/600], Loss: 0.1437\n",
      "Epoch [3/5], Step [100/600], Loss: 0.1478\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0412\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0744\n",
      "Epoch [3/5], Step [400/600], Loss: 0.2049\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0392\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0641\n",
      "Epoch [4/5], Step [100/600], Loss: 0.1016\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0419\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0731\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0556\n",
      "Epoch [4/5], Step [500/600], Loss: 0.1559\n",
      "Epoch [4/5], Step [600/600], Loss: 0.1873\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0863\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0298\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0233\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0716\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0813\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0320\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.02 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "?torch.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4084, -0.4905, -1.0438]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.4084]),\n",
       "indices=tensor([0]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST\\raw\\train-images-idx3-ubyte.gz to ../../data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ../../data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../../data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../../data/MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1571\n",
      "Epoch [1/5], Step [200/600], Loss: 0.1239\n",
      "Epoch [1/5], Step [300/600], Loss: 0.0374\n",
      "Epoch [1/5], Step [400/600], Loss: 0.1678\n",
      "Epoch [1/5], Step [500/600], Loss: 0.0613\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0263\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0544\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0051\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0401\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0199\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0122\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0912\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0356\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0495\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0372\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0289\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0090\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0443\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0242\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0831\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0054\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0458\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0062\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0330\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0187\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0176\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0076\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0143\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0071\n",
      "Epoch [5/5], Step [600/600], Loss: 0.1144\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98.52 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 80\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/cifar-10-python.tar.gz to ../../data/\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://img-blog.csdn.net/20180114184946861?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGFucmFuMg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [100/500] Loss: 1.6555\n",
      "Epoch [1/80], Step [200/500] Loss: 1.4872\n",
      "Epoch [1/80], Step [300/500] Loss: 1.1150\n",
      "Epoch [1/80], Step [400/500] Loss: 1.1670\n",
      "Epoch [1/80], Step [500/500] Loss: 1.1839\n",
      "Epoch [2/80], Step [100/500] Loss: 1.2111\n",
      "Epoch [2/80], Step [200/500] Loss: 1.0577\n",
      "Epoch [2/80], Step [300/500] Loss: 0.7485\n",
      "Epoch [2/80], Step [400/500] Loss: 0.8553\n",
      "Epoch [2/80], Step [500/500] Loss: 0.9321\n",
      "Epoch [3/80], Step [100/500] Loss: 0.7481\n",
      "Epoch [3/80], Step [200/500] Loss: 0.8588\n",
      "Epoch [3/80], Step [300/500] Loss: 0.8780\n",
      "Epoch [3/80], Step [400/500] Loss: 0.8394\n",
      "Epoch [3/80], Step [500/500] Loss: 0.7793\n",
      "Epoch [4/80], Step [100/500] Loss: 0.7854\n",
      "Epoch [4/80], Step [200/500] Loss: 0.5770\n",
      "Epoch [4/80], Step [300/500] Loss: 0.6597\n",
      "Epoch [4/80], Step [400/500] Loss: 0.7649\n",
      "Epoch [4/80], Step [500/500] Loss: 0.5849\n",
      "Epoch [5/80], Step [100/500] Loss: 0.6561\n",
      "Epoch [5/80], Step [200/500] Loss: 0.6855\n",
      "Epoch [5/80], Step [300/500] Loss: 0.7685\n",
      "Epoch [5/80], Step [400/500] Loss: 0.5601\n",
      "Epoch [5/80], Step [500/500] Loss: 0.8043\n",
      "Epoch [6/80], Step [100/500] Loss: 0.6636\n",
      "Epoch [6/80], Step [200/500] Loss: 0.5386\n",
      "Epoch [6/80], Step [300/500] Loss: 0.6125\n",
      "Epoch [6/80], Step [400/500] Loss: 0.6515\n",
      "Epoch [6/80], Step [500/500] Loss: 0.6145\n",
      "Epoch [7/80], Step [100/500] Loss: 0.5372\n",
      "Epoch [7/80], Step [200/500] Loss: 0.5541\n",
      "Epoch [7/80], Step [300/500] Loss: 0.5742\n",
      "Epoch [7/80], Step [400/500] Loss: 0.7545\n",
      "Epoch [7/80], Step [500/500] Loss: 0.5382\n",
      "Epoch [8/80], Step [100/500] Loss: 0.6953\n",
      "Epoch [8/80], Step [200/500] Loss: 0.4790\n",
      "Epoch [8/80], Step [300/500] Loss: 0.4592\n",
      "Epoch [8/80], Step [400/500] Loss: 0.6179\n",
      "Epoch [8/80], Step [500/500] Loss: 0.6616\n",
      "Epoch [9/80], Step [100/500] Loss: 0.5539\n",
      "Epoch [9/80], Step [200/500] Loss: 0.6372\n",
      "Epoch [9/80], Step [300/500] Loss: 0.4699\n",
      "Epoch [9/80], Step [400/500] Loss: 0.7486\n",
      "Epoch [9/80], Step [500/500] Loss: 0.4645\n",
      "Epoch [10/80], Step [100/500] Loss: 0.6393\n",
      "Epoch [10/80], Step [200/500] Loss: 0.4903\n",
      "Epoch [10/80], Step [300/500] Loss: 0.5655\n",
      "Epoch [10/80], Step [400/500] Loss: 0.3712\n",
      "Epoch [10/80], Step [500/500] Loss: 0.4726\n",
      "Epoch [11/80], Step [100/500] Loss: 0.7134\n",
      "Epoch [11/80], Step [200/500] Loss: 0.6078\n",
      "Epoch [11/80], Step [300/500] Loss: 0.3892\n",
      "Epoch [11/80], Step [400/500] Loss: 0.5967\n",
      "Epoch [11/80], Step [500/500] Loss: 0.3896\n",
      "Epoch [12/80], Step [100/500] Loss: 0.3542\n",
      "Epoch [12/80], Step [200/500] Loss: 0.4436\n",
      "Epoch [12/80], Step [300/500] Loss: 0.6003\n",
      "Epoch [12/80], Step [400/500] Loss: 0.5146\n",
      "Epoch [12/80], Step [500/500] Loss: 0.5265\n",
      "Epoch [13/80], Step [100/500] Loss: 0.4879\n",
      "Epoch [13/80], Step [200/500] Loss: 0.3152\n",
      "Epoch [13/80], Step [300/500] Loss: 0.3384\n",
      "Epoch [13/80], Step [400/500] Loss: 0.3198\n",
      "Epoch [13/80], Step [500/500] Loss: 0.4890\n",
      "Epoch [14/80], Step [100/500] Loss: 0.4421\n",
      "Epoch [14/80], Step [200/500] Loss: 0.4782\n",
      "Epoch [14/80], Step [300/500] Loss: 0.3973\n",
      "Epoch [14/80], Step [400/500] Loss: 0.4880\n",
      "Epoch [14/80], Step [500/500] Loss: 0.4366\n",
      "Epoch [15/80], Step [100/500] Loss: 0.5023\n",
      "Epoch [15/80], Step [200/500] Loss: 0.3697\n",
      "Epoch [15/80], Step [300/500] Loss: 0.3117\n",
      "Epoch [15/80], Step [400/500] Loss: 0.2983\n",
      "Epoch [15/80], Step [500/500] Loss: 0.2963\n",
      "Epoch [16/80], Step [100/500] Loss: 0.5203\n",
      "Epoch [16/80], Step [200/500] Loss: 0.3412\n",
      "Epoch [16/80], Step [300/500] Loss: 0.4296\n",
      "Epoch [16/80], Step [400/500] Loss: 0.3529\n",
      "Epoch [16/80], Step [500/500] Loss: 0.3491\n",
      "Epoch [17/80], Step [100/500] Loss: 0.3033\n",
      "Epoch [17/80], Step [200/500] Loss: 0.2920\n",
      "Epoch [17/80], Step [300/500] Loss: 0.4429\n",
      "Epoch [17/80], Step [400/500] Loss: 0.3412\n",
      "Epoch [17/80], Step [500/500] Loss: 0.3751\n",
      "Epoch [18/80], Step [100/500] Loss: 0.3541\n",
      "Epoch [18/80], Step [200/500] Loss: 0.3387\n",
      "Epoch [18/80], Step [300/500] Loss: 0.3386\n",
      "Epoch [18/80], Step [400/500] Loss: 0.3890\n",
      "Epoch [18/80], Step [500/500] Loss: 0.4026\n",
      "Epoch [19/80], Step [100/500] Loss: 0.2416\n",
      "Epoch [19/80], Step [200/500] Loss: 0.3125\n",
      "Epoch [19/80], Step [300/500] Loss: 0.2644\n",
      "Epoch [19/80], Step [400/500] Loss: 0.3242\n",
      "Epoch [19/80], Step [500/500] Loss: 0.3966\n",
      "Epoch [20/80], Step [100/500] Loss: 0.4252\n",
      "Epoch [20/80], Step [200/500] Loss: 0.3369\n",
      "Epoch [20/80], Step [300/500] Loss: 0.5346\n",
      "Epoch [20/80], Step [400/500] Loss: 0.3602\n",
      "Epoch [20/80], Step [500/500] Loss: 0.3741\n",
      "Epoch [21/80], Step [100/500] Loss: 0.3032\n",
      "Epoch [21/80], Step [200/500] Loss: 0.3778\n",
      "Epoch [21/80], Step [300/500] Loss: 0.4005\n",
      "Epoch [21/80], Step [400/500] Loss: 0.3589\n",
      "Epoch [21/80], Step [500/500] Loss: 0.3206\n",
      "Epoch [22/80], Step [100/500] Loss: 0.2377\n",
      "Epoch [22/80], Step [200/500] Loss: 0.2890\n",
      "Epoch [22/80], Step [300/500] Loss: 0.2498\n",
      "Epoch [22/80], Step [400/500] Loss: 0.1912\n",
      "Epoch [22/80], Step [500/500] Loss: 0.3209\n",
      "Epoch [23/80], Step [100/500] Loss: 0.2696\n",
      "Epoch [23/80], Step [200/500] Loss: 0.2508\n",
      "Epoch [23/80], Step [300/500] Loss: 0.2650\n",
      "Epoch [23/80], Step [400/500] Loss: 0.2392\n",
      "Epoch [23/80], Step [500/500] Loss: 0.1611\n",
      "Epoch [24/80], Step [100/500] Loss: 0.2673\n",
      "Epoch [24/80], Step [200/500] Loss: 0.2790\n",
      "Epoch [24/80], Step [300/500] Loss: 0.2891\n",
      "Epoch [24/80], Step [400/500] Loss: 0.2161\n",
      "Epoch [24/80], Step [500/500] Loss: 0.2862\n",
      "Epoch [25/80], Step [100/500] Loss: 0.2551\n",
      "Epoch [25/80], Step [200/500] Loss: 0.1894\n",
      "Epoch [25/80], Step [300/500] Loss: 0.2792\n",
      "Epoch [25/80], Step [400/500] Loss: 0.2483\n",
      "Epoch [25/80], Step [500/500] Loss: 0.2521\n",
      "Epoch [26/80], Step [100/500] Loss: 0.2350\n",
      "Epoch [26/80], Step [200/500] Loss: 0.2785\n",
      "Epoch [26/80], Step [300/500] Loss: 0.2649\n",
      "Epoch [26/80], Step [400/500] Loss: 0.3124\n",
      "Epoch [26/80], Step [500/500] Loss: 0.3340\n",
      "Epoch [27/80], Step [100/500] Loss: 0.2802\n",
      "Epoch [27/80], Step [200/500] Loss: 0.2424\n",
      "Epoch [27/80], Step [300/500] Loss: 0.2445\n",
      "Epoch [27/80], Step [400/500] Loss: 0.2036\n",
      "Epoch [27/80], Step [500/500] Loss: 0.3555\n",
      "Epoch [28/80], Step [100/500] Loss: 0.1556\n",
      "Epoch [28/80], Step [200/500] Loss: 0.2030\n",
      "Epoch [28/80], Step [300/500] Loss: 0.2430\n",
      "Epoch [28/80], Step [400/500] Loss: 0.3365\n",
      "Epoch [28/80], Step [500/500] Loss: 0.1855\n",
      "Epoch [29/80], Step [100/500] Loss: 0.2267\n",
      "Epoch [29/80], Step [200/500] Loss: 0.1972\n",
      "Epoch [29/80], Step [300/500] Loss: 0.4148\n",
      "Epoch [29/80], Step [400/500] Loss: 0.1362\n",
      "Epoch [29/80], Step [500/500] Loss: 0.2131\n",
      "Epoch [30/80], Step [100/500] Loss: 0.3960\n",
      "Epoch [30/80], Step [200/500] Loss: 0.3002\n",
      "Epoch [30/80], Step [300/500] Loss: 0.2049\n",
      "Epoch [30/80], Step [400/500] Loss: 0.1781\n",
      "Epoch [30/80], Step [500/500] Loss: 0.2802\n",
      "Epoch [31/80], Step [100/500] Loss: 0.2984\n",
      "Epoch [31/80], Step [200/500] Loss: 0.2182\n",
      "Epoch [31/80], Step [300/500] Loss: 0.3140\n",
      "Epoch [31/80], Step [400/500] Loss: 0.1546\n",
      "Epoch [31/80], Step [500/500] Loss: 0.1695\n",
      "Epoch [32/80], Step [100/500] Loss: 0.1852\n",
      "Epoch [32/80], Step [200/500] Loss: 0.2992\n",
      "Epoch [32/80], Step [300/500] Loss: 0.1934\n",
      "Epoch [32/80], Step [400/500] Loss: 0.2746\n",
      "Epoch [32/80], Step [500/500] Loss: 0.2238\n",
      "Epoch [33/80], Step [100/500] Loss: 0.2555\n",
      "Epoch [33/80], Step [200/500] Loss: 0.2978\n",
      "Epoch [33/80], Step [300/500] Loss: 0.1708\n",
      "Epoch [33/80], Step [400/500] Loss: 0.1024\n",
      "Epoch [33/80], Step [500/500] Loss: 0.2755\n",
      "Epoch [34/80], Step [100/500] Loss: 0.2389\n",
      "Epoch [34/80], Step [200/500] Loss: 0.1981\n",
      "Epoch [34/80], Step [300/500] Loss: 0.2708\n",
      "Epoch [34/80], Step [400/500] Loss: 0.1336\n",
      "Epoch [34/80], Step [500/500] Loss: 0.2094\n",
      "Epoch [35/80], Step [100/500] Loss: 0.1813\n",
      "Epoch [35/80], Step [200/500] Loss: 0.1926\n",
      "Epoch [35/80], Step [300/500] Loss: 0.2531\n",
      "Epoch [35/80], Step [400/500] Loss: 0.3217\n",
      "Epoch [35/80], Step [500/500] Loss: 0.3804\n",
      "Epoch [36/80], Step [100/500] Loss: 0.1608\n",
      "Epoch [36/80], Step [200/500] Loss: 0.1722\n",
      "Epoch [36/80], Step [300/500] Loss: 0.1212\n",
      "Epoch [36/80], Step [400/500] Loss: 0.1846\n",
      "Epoch [36/80], Step [500/500] Loss: 0.2284\n",
      "Epoch [37/80], Step [100/500] Loss: 0.2175\n",
      "Epoch [37/80], Step [200/500] Loss: 0.2474\n",
      "Epoch [37/80], Step [300/500] Loss: 0.2388\n",
      "Epoch [37/80], Step [400/500] Loss: 0.3082\n",
      "Epoch [37/80], Step [500/500] Loss: 0.0886\n",
      "Epoch [38/80], Step [100/500] Loss: 0.1357\n",
      "Epoch [38/80], Step [200/500] Loss: 0.2474\n",
      "Epoch [38/80], Step [300/500] Loss: 0.1504\n",
      "Epoch [38/80], Step [400/500] Loss: 0.2282\n",
      "Epoch [38/80], Step [500/500] Loss: 0.2032\n",
      "Epoch [39/80], Step [100/500] Loss: 0.2078\n",
      "Epoch [39/80], Step [200/500] Loss: 0.2952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/80], Step [300/500] Loss: 0.1628\n",
      "Epoch [39/80], Step [400/500] Loss: 0.1706\n",
      "Epoch [39/80], Step [500/500] Loss: 0.3111\n",
      "Epoch [40/80], Step [100/500] Loss: 0.1035\n",
      "Epoch [40/80], Step [200/500] Loss: 0.1804\n",
      "Epoch [40/80], Step [300/500] Loss: 0.1544\n",
      "Epoch [40/80], Step [400/500] Loss: 0.3522\n",
      "Epoch [40/80], Step [500/500] Loss: 0.2950\n",
      "Epoch [41/80], Step [100/500] Loss: 0.0638\n",
      "Epoch [41/80], Step [200/500] Loss: 0.1534\n",
      "Epoch [41/80], Step [300/500] Loss: 0.1772\n",
      "Epoch [41/80], Step [400/500] Loss: 0.1736\n",
      "Epoch [41/80], Step [500/500] Loss: 0.1437\n",
      "Epoch [42/80], Step [100/500] Loss: 0.1880\n",
      "Epoch [42/80], Step [200/500] Loss: 0.1524\n",
      "Epoch [42/80], Step [300/500] Loss: 0.2974\n",
      "Epoch [42/80], Step [400/500] Loss: 0.1315\n",
      "Epoch [42/80], Step [500/500] Loss: 0.0559\n",
      "Epoch [43/80], Step [100/500] Loss: 0.2105\n",
      "Epoch [43/80], Step [200/500] Loss: 0.2430\n",
      "Epoch [43/80], Step [300/500] Loss: 0.1791\n",
      "Epoch [43/80], Step [400/500] Loss: 0.2350\n",
      "Epoch [43/80], Step [500/500] Loss: 0.0928\n",
      "Epoch [44/80], Step [100/500] Loss: 0.2157\n",
      "Epoch [44/80], Step [200/500] Loss: 0.1474\n",
      "Epoch [44/80], Step [300/500] Loss: 0.2623\n",
      "Epoch [44/80], Step [400/500] Loss: 0.0942\n",
      "Epoch [44/80], Step [500/500] Loss: 0.1579\n",
      "Epoch [45/80], Step [100/500] Loss: 0.2591\n",
      "Epoch [45/80], Step [200/500] Loss: 0.1813\n",
      "Epoch [45/80], Step [300/500] Loss: 0.1613\n",
      "Epoch [45/80], Step [400/500] Loss: 0.0915\n",
      "Epoch [45/80], Step [500/500] Loss: 0.1101\n",
      "Epoch [46/80], Step [100/500] Loss: 0.1473\n",
      "Epoch [46/80], Step [200/500] Loss: 0.1691\n",
      "Epoch [46/80], Step [300/500] Loss: 0.2358\n",
      "Epoch [46/80], Step [400/500] Loss: 0.2228\n",
      "Epoch [46/80], Step [500/500] Loss: 0.2217\n",
      "Epoch [47/80], Step [100/500] Loss: 0.2003\n",
      "Epoch [47/80], Step [200/500] Loss: 0.1914\n",
      "Epoch [47/80], Step [300/500] Loss: 0.1745\n",
      "Epoch [47/80], Step [400/500] Loss: 0.1757\n",
      "Epoch [47/80], Step [500/500] Loss: 0.2592\n",
      "Epoch [48/80], Step [100/500] Loss: 0.1945\n",
      "Epoch [48/80], Step [200/500] Loss: 0.3157\n",
      "Epoch [48/80], Step [300/500] Loss: 0.1665\n",
      "Epoch [48/80], Step [400/500] Loss: 0.2051\n",
      "Epoch [48/80], Step [500/500] Loss: 0.1915\n",
      "Epoch [49/80], Step [100/500] Loss: 0.1990\n",
      "Epoch [49/80], Step [200/500] Loss: 0.1136\n",
      "Epoch [49/80], Step [300/500] Loss: 0.1282\n",
      "Epoch [49/80], Step [400/500] Loss: 0.2031\n",
      "Epoch [49/80], Step [500/500] Loss: 0.2739\n",
      "Epoch [50/80], Step [100/500] Loss: 0.1488\n",
      "Epoch [50/80], Step [200/500] Loss: 0.1219\n",
      "Epoch [50/80], Step [300/500] Loss: 0.0890\n",
      "Epoch [50/80], Step [400/500] Loss: 0.1356\n",
      "Epoch [50/80], Step [500/500] Loss: 0.1308\n",
      "Epoch [51/80], Step [100/500] Loss: 0.2534\n",
      "Epoch [51/80], Step [200/500] Loss: 0.1973\n",
      "Epoch [51/80], Step [300/500] Loss: 0.1767\n",
      "Epoch [51/80], Step [400/500] Loss: 0.1564\n",
      "Epoch [51/80], Step [500/500] Loss: 0.1391\n",
      "Epoch [52/80], Step [100/500] Loss: 0.1847\n",
      "Epoch [52/80], Step [200/500] Loss: 0.1879\n",
      "Epoch [52/80], Step [300/500] Loss: 0.2345\n",
      "Epoch [52/80], Step [400/500] Loss: 0.2070\n",
      "Epoch [52/80], Step [500/500] Loss: 0.1059\n",
      "Epoch [53/80], Step [100/500] Loss: 0.1003\n",
      "Epoch [53/80], Step [200/500] Loss: 0.1925\n",
      "Epoch [53/80], Step [300/500] Loss: 0.1890\n",
      "Epoch [53/80], Step [400/500] Loss: 0.1861\n",
      "Epoch [53/80], Step [500/500] Loss: 0.1295\n",
      "Epoch [54/80], Step [100/500] Loss: 0.0899\n",
      "Epoch [54/80], Step [200/500] Loss: 0.2145\n",
      "Epoch [54/80], Step [300/500] Loss: 0.1629\n",
      "Epoch [54/80], Step [400/500] Loss: 0.2391\n",
      "Epoch [54/80], Step [500/500] Loss: 0.2391\n",
      "Epoch [55/80], Step [100/500] Loss: 0.1496\n",
      "Epoch [55/80], Step [200/500] Loss: 0.1948\n",
      "Epoch [55/80], Step [300/500] Loss: 0.1899\n",
      "Epoch [55/80], Step [400/500] Loss: 0.1173\n",
      "Epoch [55/80], Step [500/500] Loss: 0.1963\n",
      "Epoch [56/80], Step [100/500] Loss: 0.1295\n",
      "Epoch [56/80], Step [200/500] Loss: 0.1654\n",
      "Epoch [56/80], Step [300/500] Loss: 0.1221\n",
      "Epoch [56/80], Step [400/500] Loss: 0.2445\n",
      "Epoch [56/80], Step [500/500] Loss: 0.1474\n",
      "Epoch [57/80], Step [100/500] Loss: 0.2304\n",
      "Epoch [57/80], Step [200/500] Loss: 0.1028\n",
      "Epoch [57/80], Step [300/500] Loss: 0.1756\n",
      "Epoch [57/80], Step [400/500] Loss: 0.1820\n",
      "Epoch [57/80], Step [500/500] Loss: 0.0926\n",
      "Epoch [58/80], Step [100/500] Loss: 0.1850\n",
      "Epoch [58/80], Step [200/500] Loss: 0.2124\n",
      "Epoch [58/80], Step [300/500] Loss: 0.1128\n",
      "Epoch [58/80], Step [400/500] Loss: 0.0438\n",
      "Epoch [58/80], Step [500/500] Loss: 0.1188\n",
      "Epoch [59/80], Step [100/500] Loss: 0.1303\n",
      "Epoch [59/80], Step [200/500] Loss: 0.0986\n",
      "Epoch [59/80], Step [300/500] Loss: 0.1204\n",
      "Epoch [59/80], Step [400/500] Loss: 0.1032\n",
      "Epoch [59/80], Step [500/500] Loss: 0.1223\n",
      "Epoch [60/80], Step [100/500] Loss: 0.1244\n",
      "Epoch [60/80], Step [200/500] Loss: 0.1437\n",
      "Epoch [60/80], Step [300/500] Loss: 0.1493\n",
      "Epoch [60/80], Step [400/500] Loss: 0.1517\n",
      "Epoch [60/80], Step [500/500] Loss: 0.0822\n",
      "Epoch [61/80], Step [100/500] Loss: 0.1248\n",
      "Epoch [61/80], Step [200/500] Loss: 0.1638\n",
      "Epoch [61/80], Step [300/500] Loss: 0.1690\n",
      "Epoch [61/80], Step [400/500] Loss: 0.0933\n",
      "Epoch [61/80], Step [500/500] Loss: 0.2524\n",
      "Epoch [62/80], Step [100/500] Loss: 0.1329\n",
      "Epoch [62/80], Step [200/500] Loss: 0.2041\n",
      "Epoch [62/80], Step [300/500] Loss: 0.3302\n",
      "Epoch [62/80], Step [400/500] Loss: 0.1008\n",
      "Epoch [62/80], Step [500/500] Loss: 0.0716\n",
      "Epoch [63/80], Step [100/500] Loss: 0.1863\n",
      "Epoch [63/80], Step [200/500] Loss: 0.1763\n",
      "Epoch [63/80], Step [300/500] Loss: 0.0898\n",
      "Epoch [63/80], Step [400/500] Loss: 0.1894\n",
      "Epoch [63/80], Step [500/500] Loss: 0.1846\n",
      "Epoch [64/80], Step [100/500] Loss: 0.1086\n",
      "Epoch [64/80], Step [200/500] Loss: 0.0710\n",
      "Epoch [64/80], Step [300/500] Loss: 0.2185\n",
      "Epoch [64/80], Step [400/500] Loss: 0.1076\n",
      "Epoch [64/80], Step [500/500] Loss: 0.1549\n",
      "Epoch [65/80], Step [100/500] Loss: 0.1086\n",
      "Epoch [65/80], Step [200/500] Loss: 0.0610\n",
      "Epoch [65/80], Step [300/500] Loss: 0.1474\n",
      "Epoch [65/80], Step [400/500] Loss: 0.0909\n",
      "Epoch [65/80], Step [500/500] Loss: 0.1350\n",
      "Epoch [66/80], Step [100/500] Loss: 0.0955\n",
      "Epoch [66/80], Step [200/500] Loss: 0.1551\n",
      "Epoch [66/80], Step [300/500] Loss: 0.1574\n",
      "Epoch [66/80], Step [400/500] Loss: 0.0796\n",
      "Epoch [66/80], Step [500/500] Loss: 0.2221\n",
      "Epoch [67/80], Step [100/500] Loss: 0.1892\n",
      "Epoch [67/80], Step [200/500] Loss: 0.1329\n",
      "Epoch [67/80], Step [300/500] Loss: 0.1196\n",
      "Epoch [67/80], Step [400/500] Loss: 0.1298\n",
      "Epoch [67/80], Step [500/500] Loss: 0.1753\n",
      "Epoch [68/80], Step [100/500] Loss: 0.0877\n",
      "Epoch [68/80], Step [200/500] Loss: 0.1377\n",
      "Epoch [68/80], Step [300/500] Loss: 0.2448\n",
      "Epoch [68/80], Step [400/500] Loss: 0.1532\n",
      "Epoch [68/80], Step [500/500] Loss: 0.0694\n",
      "Epoch [69/80], Step [100/500] Loss: 0.1400\n",
      "Epoch [69/80], Step [200/500] Loss: 0.1300\n",
      "Epoch [69/80], Step [300/500] Loss: 0.1390\n",
      "Epoch [69/80], Step [400/500] Loss: 0.2133\n",
      "Epoch [69/80], Step [500/500] Loss: 0.1194\n",
      "Epoch [70/80], Step [100/500] Loss: 0.1904\n",
      "Epoch [70/80], Step [200/500] Loss: 0.1423\n",
      "Epoch [70/80], Step [300/500] Loss: 0.0662\n",
      "Epoch [70/80], Step [400/500] Loss: 0.3136\n",
      "Epoch [70/80], Step [500/500] Loss: 0.2180\n",
      "Epoch [71/80], Step [100/500] Loss: 0.1132\n",
      "Epoch [71/80], Step [200/500] Loss: 0.1280\n",
      "Epoch [71/80], Step [300/500] Loss: 0.1800\n",
      "Epoch [71/80], Step [400/500] Loss: 0.1747\n",
      "Epoch [71/80], Step [500/500] Loss: 0.1311\n",
      "Epoch [72/80], Step [100/500] Loss: 0.2215\n",
      "Epoch [72/80], Step [200/500] Loss: 0.0660\n",
      "Epoch [72/80], Step [300/500] Loss: 0.0672\n",
      "Epoch [72/80], Step [400/500] Loss: 0.1927\n",
      "Epoch [72/80], Step [500/500] Loss: 0.1136\n",
      "Epoch [73/80], Step [100/500] Loss: 0.0925\n",
      "Epoch [73/80], Step [200/500] Loss: 0.0926\n",
      "Epoch [73/80], Step [300/500] Loss: 0.1398\n",
      "Epoch [73/80], Step [400/500] Loss: 0.0752\n",
      "Epoch [73/80], Step [500/500] Loss: 0.0733\n",
      "Epoch [74/80], Step [100/500] Loss: 0.1606\n",
      "Epoch [74/80], Step [200/500] Loss: 0.1350\n",
      "Epoch [74/80], Step [300/500] Loss: 0.0939\n",
      "Epoch [74/80], Step [400/500] Loss: 0.1152\n",
      "Epoch [74/80], Step [500/500] Loss: 0.0843\n",
      "Epoch [75/80], Step [100/500] Loss: 0.1089\n",
      "Epoch [75/80], Step [200/500] Loss: 0.1669\n",
      "Epoch [75/80], Step [300/500] Loss: 0.0865\n",
      "Epoch [75/80], Step [400/500] Loss: 0.1279\n",
      "Epoch [75/80], Step [500/500] Loss: 0.0928\n",
      "Epoch [76/80], Step [100/500] Loss: 0.0931\n",
      "Epoch [76/80], Step [200/500] Loss: 0.1408\n",
      "Epoch [76/80], Step [300/500] Loss: 0.1634\n",
      "Epoch [76/80], Step [400/500] Loss: 0.0598\n",
      "Epoch [76/80], Step [500/500] Loss: 0.1328\n",
      "Epoch [77/80], Step [100/500] Loss: 0.1586\n",
      "Epoch [77/80], Step [200/500] Loss: 0.0914\n",
      "Epoch [77/80], Step [300/500] Loss: 0.0611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/80], Step [400/500] Loss: 0.0951\n",
      "Epoch [77/80], Step [500/500] Loss: 0.1643\n",
      "Epoch [78/80], Step [100/500] Loss: 0.0649\n",
      "Epoch [78/80], Step [200/500] Loss: 0.1923\n",
      "Epoch [78/80], Step [300/500] Loss: 0.1686\n",
      "Epoch [78/80], Step [400/500] Loss: 0.1224\n",
      "Epoch [78/80], Step [500/500] Loss: 0.1005\n",
      "Epoch [79/80], Step [100/500] Loss: 0.0854\n",
      "Epoch [79/80], Step [200/500] Loss: 0.1197\n",
      "Epoch [79/80], Step [300/500] Loss: 0.0792\n",
      "Epoch [79/80], Step [400/500] Loss: 0.1397\n",
      "Epoch [79/80], Step [500/500] Loss: 0.2007\n",
      "Epoch [80/80], Step [100/500] Loss: 0.1588\n",
      "Epoch [80/80], Step [200/500] Loss: 0.1662\n",
      "Epoch [80/80], Step [300/500] Loss: 0.0812\n",
      "Epoch [80/80], Step [400/500] Loss: 0.1035\n",
      "Epoch [80/80], Step [500/500] Loss: 0.0897\n",
      "Accuracy of the model on the test images: 88.26 %\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'resnet.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/600], Loss: 0.4873\n",
      "Epoch [1/2], Step [200/600], Loss: 0.3824\n",
      "Epoch [1/2], Step [300/600], Loss: 0.1849\n",
      "Epoch [1/2], Step [400/600], Loss: 0.1911\n",
      "Epoch [1/2], Step [500/600], Loss: 0.1688\n",
      "Epoch [1/2], Step [600/600], Loss: 0.0288\n",
      "Epoch [2/2], Step [100/600], Loss: 0.1916\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0843\n",
      "Epoch [2/2], Step [300/600], Loss: 0.0589\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0433\n",
      "Epoch [2/2], Step [500/600], Loss: 0.0688\n",
      "Epoch [2/2], Step [600/600], Loss: 0.0979\n",
      "Test Accuracy of the model on the 10000 test images: 97.89 %\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Recurrent Neural Network## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.003\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional recurrent neural network (many-to-one)\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiRNN(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/600], Loss: 0.6560\n",
      "Epoch [1/2], Step [200/600], Loss: 0.1316\n",
      "Epoch [1/2], Step [300/600], Loss: 0.2301\n",
      "Epoch [1/2], Step [400/600], Loss: 0.1411\n",
      "Epoch [1/2], Step [500/600], Loss: 0.0342\n",
      "Epoch [1/2], Step [600/600], Loss: 0.1780\n",
      "Epoch [2/2], Step [100/600], Loss: 0.0519\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0886\n",
      "Epoch [2/2], Step [300/600], Loss: 0.1098\n",
      "Epoch [2/2], Step [400/600], Loss: 0.1470\n",
      "Epoch [2/2], Step [500/600], Loss: 0.0553\n",
      "Epoch [2/2], Step [600/600], Loss: 0.0819\n",
      "Test Accuracy of the model on the 10000 test images: 97.81 %\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model (RNN-LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "embed_size = 128\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "num_epochs = 5\n",
    "num_samples = 1000     # number of words to be sampled\n",
    "batch_size = 20\n",
    "seq_length = 30\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"Penn Treebank\" dataset\n",
    "corpus = Corpus()\n",
    "ids = corpus.get_data('train.txt', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Truncated backpropagation\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step[0/1549], Loss: 9.2128, Perplexity: 10025.02\n",
      "Epoch [1/5], Step[100/1549], Loss: 5.9897, Perplexity: 399.29\n",
      "Epoch [1/5], Step[200/1549], Loss: 5.9422, Perplexity: 380.78\n",
      "Epoch [1/5], Step[300/1549], Loss: 5.7781, Perplexity: 323.16\n",
      "Epoch [1/5], Step[400/1549], Loss: 5.7080, Perplexity: 301.27\n",
      "Epoch [1/5], Step[500/1549], Loss: 5.1303, Perplexity: 169.07\n",
      "Epoch [1/5], Step[600/1549], Loss: 5.1780, Perplexity: 177.34\n",
      "Epoch [1/5], Step[700/1549], Loss: 5.3648, Perplexity: 213.74\n",
      "Epoch [1/5], Step[800/1549], Loss: 5.1656, Perplexity: 175.15\n",
      "Epoch [1/5], Step[900/1549], Loss: 5.0731, Perplexity: 159.67\n",
      "Epoch [1/5], Step[1000/1549], Loss: 5.1105, Perplexity: 165.75\n",
      "Epoch [1/5], Step[1100/1549], Loss: 5.3461, Perplexity: 209.78\n",
      "Epoch [1/5], Step[1200/1549], Loss: 5.1456, Perplexity: 171.67\n",
      "Epoch [1/5], Step[1300/1549], Loss: 5.1091, Perplexity: 165.53\n",
      "Epoch [1/5], Step[1400/1549], Loss: 4.8377, Perplexity: 126.18\n",
      "Epoch [1/5], Step[1500/1549], Loss: 5.1470, Perplexity: 171.92\n",
      "Epoch [2/5], Step[0/1549], Loss: 5.4139, Perplexity: 224.51\n",
      "Epoch [2/5], Step[100/1549], Loss: 4.5447, Perplexity: 94.14\n",
      "Epoch [2/5], Step[200/1549], Loss: 4.7344, Perplexity: 113.80\n",
      "Epoch [2/5], Step[300/1549], Loss: 4.6915, Perplexity: 109.01\n",
      "Epoch [2/5], Step[400/1549], Loss: 4.5402, Perplexity: 93.71\n",
      "Epoch [2/5], Step[500/1549], Loss: 4.1610, Perplexity: 64.14\n",
      "Epoch [2/5], Step[600/1549], Loss: 4.4616, Perplexity: 86.62\n",
      "Epoch [2/5], Step[700/1549], Loss: 4.4108, Perplexity: 82.34\n",
      "Epoch [2/5], Step[800/1549], Loss: 4.4279, Perplexity: 83.76\n",
      "Epoch [2/5], Step[900/1549], Loss: 4.2532, Perplexity: 70.33\n",
      "Epoch [2/5], Step[1000/1549], Loss: 4.2971, Perplexity: 73.48\n",
      "Epoch [2/5], Step[1100/1549], Loss: 4.4995, Perplexity: 89.97\n",
      "Epoch [2/5], Step[1200/1549], Loss: 4.3980, Perplexity: 81.29\n",
      "Epoch [2/5], Step[1300/1549], Loss: 4.1997, Perplexity: 66.67\n",
      "Epoch [2/5], Step[1400/1549], Loss: 3.9741, Perplexity: 53.20\n",
      "Epoch [2/5], Step[1500/1549], Loss: 4.3104, Perplexity: 74.47\n",
      "Epoch [3/5], Step[0/1549], Loss: 4.4734, Perplexity: 87.65\n",
      "Epoch [3/5], Step[100/1549], Loss: 3.8591, Perplexity: 47.42\n",
      "Epoch [3/5], Step[200/1549], Loss: 4.0600, Perplexity: 57.97\n",
      "Epoch [3/5], Step[300/1549], Loss: 3.9796, Perplexity: 53.50\n",
      "Epoch [3/5], Step[400/1549], Loss: 3.8801, Perplexity: 48.43\n",
      "Epoch [3/5], Step[500/1549], Loss: 3.4444, Perplexity: 31.32\n",
      "Epoch [3/5], Step[600/1549], Loss: 3.8750, Perplexity: 48.18\n",
      "Epoch [3/5], Step[700/1549], Loss: 3.7648, Perplexity: 43.16\n",
      "Epoch [3/5], Step[800/1549], Loss: 3.8387, Perplexity: 46.46\n",
      "Epoch [3/5], Step[900/1549], Loss: 3.5238, Perplexity: 33.91\n",
      "Epoch [3/5], Step[1000/1549], Loss: 3.6190, Perplexity: 37.30\n",
      "Epoch [3/5], Step[1100/1549], Loss: 3.7297, Perplexity: 41.67\n",
      "Epoch [3/5], Step[1200/1549], Loss: 3.7093, Perplexity: 40.83\n",
      "Epoch [3/5], Step[1300/1549], Loss: 3.5066, Perplexity: 33.33\n",
      "Epoch [3/5], Step[1400/1549], Loss: 3.2356, Perplexity: 25.42\n",
      "Epoch [3/5], Step[1500/1549], Loss: 3.5701, Perplexity: 35.52\n",
      "Epoch [4/5], Step[0/1549], Loss: 3.5749, Perplexity: 35.69\n",
      "Epoch [4/5], Step[100/1549], Loss: 3.2262, Perplexity: 25.18\n",
      "Epoch [4/5], Step[200/1549], Loss: 3.5120, Perplexity: 33.52\n",
      "Epoch [4/5], Step[300/1549], Loss: 3.4473, Perplexity: 31.42\n",
      "Epoch [4/5], Step[400/1549], Loss: 3.2975, Perplexity: 27.04\n",
      "Epoch [4/5], Step[500/1549], Loss: 2.9264, Perplexity: 18.66\n",
      "Epoch [4/5], Step[600/1549], Loss: 3.4130, Perplexity: 30.36\n",
      "Epoch [4/5], Step[700/1549], Loss: 3.2584, Perplexity: 26.01\n",
      "Epoch [4/5], Step[800/1549], Loss: 3.4428, Perplexity: 31.27\n",
      "Epoch [4/5], Step[900/1549], Loss: 3.0108, Perplexity: 20.30\n",
      "Epoch [4/5], Step[1000/1549], Loss: 3.1340, Perplexity: 22.97\n",
      "Epoch [4/5], Step[1100/1549], Loss: 3.2302, Perplexity: 25.28\n",
      "Epoch [4/5], Step[1200/1549], Loss: 3.2122, Perplexity: 24.83\n",
      "Epoch [4/5], Step[1300/1549], Loss: 2.9735, Perplexity: 19.56\n",
      "Epoch [4/5], Step[1400/1549], Loss: 2.6920, Perplexity: 14.76\n",
      "Epoch [4/5], Step[1500/1549], Loss: 3.0751, Perplexity: 21.65\n",
      "Epoch [5/5], Step[0/1549], Loss: 3.1486, Perplexity: 23.30\n",
      "Epoch [5/5], Step[100/1549], Loss: 2.8771, Perplexity: 17.76\n",
      "Epoch [5/5], Step[200/1549], Loss: 3.1889, Perplexity: 24.26\n",
      "Epoch [5/5], Step[300/1549], Loss: 3.0704, Perplexity: 21.55\n",
      "Epoch [5/5], Step[400/1549], Loss: 2.9777, Perplexity: 19.64\n",
      "Epoch [5/5], Step[500/1549], Loss: 2.5356, Perplexity: 12.62\n",
      "Epoch [5/5], Step[600/1549], Loss: 3.1232, Perplexity: 22.72\n",
      "Epoch [5/5], Step[700/1549], Loss: 2.9647, Perplexity: 19.39\n",
      "Epoch [5/5], Step[800/1549], Loss: 3.0739, Perplexity: 21.63\n",
      "Epoch [5/5], Step[900/1549], Loss: 2.6835, Perplexity: 14.64\n",
      "Epoch [5/5], Step[1000/1549], Loss: 2.8272, Perplexity: 16.90\n",
      "Epoch [5/5], Step[1100/1549], Loss: 2.9415, Perplexity: 18.94\n",
      "Epoch [5/5], Step[1200/1549], Loss: 2.9465, Perplexity: 19.04\n",
      "Epoch [5/5], Step[1300/1549], Loss: 2.6266, Perplexity: 13.83\n",
      "Epoch [5/5], Step[1400/1549], Loss: 2.3566, Perplexity: 10.56\n",
      "Epoch [5/5], Step[1500/1549], Loss: 2.7740, Perplexity: 16.02\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        states = detach(states)\n",
    "        outputs, states = model(inputs, states)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled [100/1000] words and save to sample.txt\n",
      "Sampled [200/1000] words and save to sample.txt\n",
      "Sampled [300/1000] words and save to sample.txt\n",
      "Sampled [400/1000] words and save to sample.txt\n",
      "Sampled [500/1000] words and save to sample.txt\n",
      "Sampled [600/1000] words and save to sample.txt\n",
      "Sampled [700/1000] words and save to sample.txt\n",
      "Sampled [800/1000] words and save to sample.txt\n",
      "Sampled [900/1000] words and save to sample.txt\n",
      "Sampled [1000/1000] words and save to sample.txt\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    with open('sample.txt', 'w') as f:\n",
    "        # Set intial hidden ane cell states\n",
    "        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
    "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
    "\n",
    "        # Select one word id randomly\n",
    "        prob = torch.ones(vocab_size)\n",
    "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # Forward propagate RNN \n",
    "            output, state = model(input, state)\n",
    "\n",
    "            # Sample a word id\n",
    "            prob = output.exp()\n",
    "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "\n",
    "            # Fill input with sampled word id for the next time step\n",
    "            input.fill_(word_id)\n",
    "\n",
    "            # File write\n",
    "            word = corpus.dictionary.idx2word[word_id]\n",
    "            word = '\\n' if word == '<eos>' else word + ' '\n",
    "            f.write(word)\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))\n",
    "\n",
    "# Save the model checkpoints\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
