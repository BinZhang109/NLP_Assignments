{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "#                         Table of Contents                          #\n",
    "# ================================================================== #\n",
    "\n",
    "# 1. Basic autograd example 1               (Line 25 to 39)\n",
    "# 2. Basic autograd example 2               (Line 46 to 83)\n",
    "# 3. Loading data from numpy                (Line 90 to 97)\n",
    "# 4. Input pipline                          (Line 104 to 129)\n",
    "# 5. Input pipline for custom dataset       (Line 136 to 156)\n",
    "# 6. Pretrained model                       (Line 163 to 176)\n",
    "# 7. Save and load model                    (Line 183 to 189) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "#                     1. Basic autograd example 1                    #\n",
    "# ================================================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors.\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients.\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "#                    2. Basic autograd example 2                     #\n",
    "# ================================================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[ 0.4992,  0.2770,  0.1992],\n",
      "        [ 0.1647, -0.1774, -0.5663]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([ 0.3061, -0.1481], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build loss function and optimizer.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass.\n",
    "pred = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.0415191650390625\n"
     ]
    }
   ],
   "source": [
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw:  tensor([[ 0.3719, -0.1274,  0.2646],\n",
      "        [-0.1529,  0.1879, -0.3598]])\n",
      "dL/db:  tensor([-0.3903, -0.0911])\n"
     ]
    }
   ],
   "source": [
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-step gradient descent.\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 step optimization:  1.035805344581604\n"
     ]
    }
   ],
   "source": [
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "#                     3. Loading data from numpy                     #\n",
    "# ================================================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array.\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Convert the numpy array to a torch tensor.\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "# Convert the torch tensor to a numpy array.\n",
    "z = y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                         4. Input pipline                           #\n",
    "# ================================================================== #\n",
    "\n",
    "# Download and construct CIFAR-10 dataset.\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)\n",
    "\n",
    "# Fetch one data pair (read data from disk).\n",
    "image, label = train_dataset[0]\n",
    "print (image.size())\n",
    "print (label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader (this provides queues and threads in a very simple way).\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "# Actual usage of the data loader is as below.\n",
    "for images, labels in train_loader:\n",
    "    # Training code should be written here.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-28982eb6adcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n\u001b[0;32m     24\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                                            shuffle=True)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context)\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# map-style\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_source, replacement, num_samples)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[1;32m---> 94\u001b[1;33m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                5. Input pipline for custom dataset                 #\n",
    "# ================================================================== #\n",
    "\n",
    "# You should build your custom dataset as below.\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "        # 1. Initialize file paths or a list of file names. \n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return 0 \n",
    "\n",
    "# You can then use the prebuilt data loader. \n",
    "custom_dataset = CustomDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里报错是因为没写对应函数内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to C:\\Users\\Mr. Wu/.cache\\torch\\checkpoints\\resnet18-5c106cde.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                        6. Pretrained model                         #\n",
    "# ================================================================== #\n",
    "\n",
    "# Download and load the pretrained ResNet-18.\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# If you want to finetune only the top layer of the model, set as below.\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the top layer for finetuning.\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example.\n",
    "\n",
    "# Forward pass.\n",
    "images = torch.randn(64, 3, 224, 224)\n",
    "outputs = resnet(images)\n",
    "print (outputs.size())     # (64, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "#                      7. Save and load the model                    #\n",
    "# ================================================================== #\n",
    "\n",
    "# Save and load the entire model.\n",
    "torch.save(resnet, 'model.ckpt')\n",
    "model = torch.load('model.ckpt')\n",
    "\n",
    "# Save and load only the model parameters (recommended).\n",
    "torch.save(resnet.state_dict(), 'params.ckpt')\n",
    "resnet.load_state_dict(torch.load('params.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Toy dataset\n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 4.8994\n",
      "Epoch [10/60], Loss: 2.1245\n",
      "Epoch [15/60], Loss: 1.0003\n",
      "Epoch [20/60], Loss: 0.5447\n",
      "Epoch [25/60], Loss: 0.3601\n",
      "Epoch [30/60], Loss: 0.2852\n",
      "Epoch [35/60], Loss: 0.2547\n",
      "Epoch [40/60], Loss: 0.2423\n",
      "Epoch [45/60], Loss: 0.2372\n",
      "Epoch [50/60], Loss: 0.2350\n",
      "Epoch [55/60], Loss: 0.2340\n",
      "Epoch [60/60], Loss: 0.2335\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linear regression model\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    inputs = torch.from_numpy(x_train)\n",
    "    targets = torch.from_numpy(y_train)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Plot the graph\n",
    "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset (images and labels)\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 2.2534\n",
      "Epoch [1/5], Step [200/600], Loss: 2.1395\n",
      "Epoch [1/5], Step [300/600], Loss: 2.0485\n",
      "Epoch [1/5], Step [400/600], Loss: 1.9770\n",
      "Epoch [1/5], Step [500/600], Loss: 1.8643\n",
      "Epoch [1/5], Step [600/600], Loss: 1.8293\n",
      "Epoch [2/5], Step [100/600], Loss: 1.7757\n",
      "Epoch [2/5], Step [200/600], Loss: 1.6627\n",
      "Epoch [2/5], Step [300/600], Loss: 1.5991\n",
      "Epoch [2/5], Step [400/600], Loss: 1.6044\n",
      "Epoch [2/5], Step [500/600], Loss: 1.4955\n",
      "Epoch [2/5], Step [600/600], Loss: 1.4027\n",
      "Epoch [3/5], Step [100/600], Loss: 1.4214\n",
      "Epoch [3/5], Step [200/600], Loss: 1.4436\n",
      "Epoch [3/5], Step [300/600], Loss: 1.3576\n",
      "Epoch [3/5], Step [400/600], Loss: 1.2695\n",
      "Epoch [3/5], Step [500/600], Loss: 1.2638\n",
      "Epoch [3/5], Step [600/600], Loss: 1.3191\n",
      "Epoch [4/5], Step [100/600], Loss: 1.2104\n",
      "Epoch [4/5], Step [200/600], Loss: 1.1853\n",
      "Epoch [4/5], Step [300/600], Loss: 1.1690\n",
      "Epoch [4/5], Step [400/600], Loss: 1.2242\n",
      "Epoch [4/5], Step [500/600], Loss: 1.2125\n",
      "Epoch [4/5], Step [600/600], Loss: 1.0205\n",
      "Epoch [5/5], Step [100/600], Loss: 1.0085\n",
      "Epoch [5/5], Step [200/600], Loss: 1.0658\n",
      "Epoch [5/5], Step [300/600], Loss: 1.0117\n",
      "Epoch [5/5], Step [400/600], Loss: 1.0562\n",
      "Epoch [5/5], Step [500/600], Loss: 1.0769\n",
      "Epoch [5/5], Step [600/600], Loss: 1.0231\n",
      "Accuracy of the model on the 10000 test images: 83 %\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression model\n",
    "model = nn.Linear(input_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "# nn.CrossEntropyLoss() computes softmax internally\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape images to (batch_size, input_size)\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feedforward_neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.3070\n",
      "Epoch [1/5], Step [200/600], Loss: 0.2062\n",
      "Epoch [1/5], Step [300/600], Loss: 0.2144\n",
      "Epoch [1/5], Step [400/600], Loss: 0.2050\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1291\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0825\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0569\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1050\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0758\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0716\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0689\n",
      "Epoch [2/5], Step [600/600], Loss: 0.1437\n",
      "Epoch [3/5], Step [100/600], Loss: 0.1478\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0412\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0744\n",
      "Epoch [3/5], Step [400/600], Loss: 0.2049\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0392\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0641\n",
      "Epoch [4/5], Step [100/600], Loss: 0.1016\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0419\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0731\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0556\n",
      "Epoch [4/5], Step [500/600], Loss: 0.1559\n",
      "Epoch [4/5], Step [600/600], Loss: 0.1873\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0863\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0298\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0233\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0716\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0813\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0320\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.02 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "?torch.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4084, -0.4905, -1.0438]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.4084]),\n",
       "indices=tensor([0]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
