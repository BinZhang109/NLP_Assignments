{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22590299,  0.30924726, -0.08256867, ..., -0.14456667,\n",
       "        -0.47119054, -0.07071219],\n",
       "       [ 0.37245134,  0.35599762,  0.09506325, ...,  0.09425778,\n",
       "        -0.4482554 , -0.17623922],\n",
       "       [ 0.26777843,  0.22399886, -0.0217796 , ...,  0.02863549,\n",
       "        -0.37525114,  0.02919415]], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "bc.encode(['First do it', 'then do it right', 'then do it better'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/hanxiao/bert-as-service#building-a-qa-semantic-search-engine-in-3-minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a QA semantic search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 questions loaded, avg. len of 9\n"
     ]
    }
   ],
   "source": [
    "prefix_q = '##### **Q:** '\n",
    "with open('README.md', encoding='utf-8') as fp:\n",
    "    questions = [v.replace(prefix_q, '').strip() for v in fp if v.strip() and v.startswith(prefix_q)]\n",
    "    print('%d questions loaded, avg. len of %d' % (len(questions), np.mean([len(d.split()) for d in questions])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vecs = bc.encode(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your question: 你会说汉语吗\n",
      "> 14.697959\tCan I use my own tokenizer?\n",
      "> 14.687681\tDo I need to do segmentation for Chinese?\n",
      "> 14.655649\tHow can I choose `num_worker`?\n",
      "> 14.632836\tWhy my (English) word is tokenized to `##something`?\n",
      "> 14.62494\tWhat is backend based on?\n"
     ]
    }
   ],
   "source": [
    "topk=5\n",
    "\n",
    "query = input('your question: ')\n",
    "query_vec = bc.encode([query])[0]\n",
    "# compute normalized dot product as score\n",
    "score = np.sum(query_vec * doc_vecs, axis=1) / np.linalg.norm(doc_vecs, axis=1)\n",
    "topk_idx = np.argsort(score)[::-1][:topk]\n",
    "for idx in topk_idx:\n",
    "    print('> %s\\t%s' % (score[idx], questions[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving a fine-tuned BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting ELMo-like contextual word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_len = 25\n",
    "# pooling_strategy = NONE\n",
    "\n",
    "bc = BertClient()\n",
    "vec = bc.encode(['hey you', 'whats up?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 25, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec[0].shape\n",
    "# sentence embeddings for `hey you`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec[0][0].shape\n",
    "# word embedding for `[CLS]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec[0][1].shape\n",
    "# word embedding for `hey`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec[0][2].shape\n",
    "# word embedding for `you`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec[0][3].shape\n",
    "# word embedding for `[SEP]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec[0][4].shape\n",
    "# word embedding for padding symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 25 is out of bounds for axis 0 with size 25",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-459c1ea55640>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 25 is out of bounds for axis 0 with size 25"
     ]
    }
   ],
   "source": [
    "vec[0][25].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using your own tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['你好 世界!', '美好 一天']\n",
    "\n",
    "# a naive whitespace tokenizer\n",
    "texts2 = [s.split() for s in texts]\n",
    "\n",
    "vecs = bc.encode(texts2, is_tokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 25, 768)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.01175493, -0.1617977 ,  0.29648292, ...,  0.6362557 ,\n",
       "         -0.04364629, -0.27905396],\n",
       "        [ 0.47149712, -0.7719901 , -0.73191035, ...,  0.12688503,\n",
       "         -0.2823674 , -0.19017912],\n",
       "        [ 0.50740826, -0.44305798, -0.15977177, ...,  0.29794553,\n",
       "          0.02039958, -0.23108397],\n",
       "        ...,\n",
       "        [-0.        , -0.        , -0.        , ..., -0.        ,\n",
       "         -0.        , -0.        ],\n",
       "        [-0.        , -0.        ,  0.        , ..., -0.        ,\n",
       "         -0.        , -0.        ],\n",
       "        [ 0.        ,  0.        , -0.        , ..., -0.        ,\n",
       "         -0.        , -0.        ]],\n",
       "\n",
       "       [[-0.14682214,  0.4779205 ,  0.6809331 , ...,  0.5676709 ,\n",
       "         -0.0326115 , -0.93247014],\n",
       "        [ 0.5639583 ,  0.34819496,  1.2245796 , ...,  0.39399296,\n",
       "         -0.68489134, -0.94545054],\n",
       "        [-0.05322995, -0.2393229 ,  1.0470473 , ...,  0.7995612 ,\n",
       "         -0.11019681, -1.1336169 ],\n",
       "        ...,\n",
       "        [-0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         -0.        , -0.        ],\n",
       "        [-0.        ,  0.        ,  0.        , ..., -0.        ,\n",
       "         -0.        , -0.        ],\n",
       "        [-0.        ,  0.        ,  0.        , ..., -0.        ,\n",
       "         -0.        , -0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc.encode(['你好 世界!', '这是 它'], show_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.27740484,  0.7500669 ,  0.4284198 , ...,  0.6141869 ,\n",
       "         -0.20193532, -0.26091242],\n",
       "        [ 0.4758171 ,  0.0606777 ,  0.6419624 , ..., -0.52978677,\n",
       "          0.04691685, -0.18648772],\n",
       "        [ 0.7059512 ,  0.07687826, -0.04954352, ..., -0.12742816,\n",
       "         -0.35104132, -0.04731564],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.        ,\n",
       "         -0.        , -0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.        ,\n",
       "         -0.        , -0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.        ,\n",
       "         -0.        , -0.        ]],\n",
       "\n",
       "       [[-0.8403784 ,  1.0385634 , -0.34069625, ...,  0.66673994,\n",
       "         -0.3021288 , -0.5121763 ],\n",
       "        [-0.89954877,  1.0046595 , -0.46250865, ...,  0.18021399,\n",
       "         -0.2635792 , -0.8112389 ],\n",
       "        [-0.1540899 ,  0.5245218 , -0.5092169 , ...,  0.13944007,\n",
       "          0.28740644, -0.11593796],\n",
       "        ...,\n",
       "        [-0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         -0.        , -0.        ],\n",
       "        [-0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         -0.        , -0.        ],\n",
       "        [-0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         -0.        , -0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc.encode([['hello', 'world!'], ['thisis', 'it']], show_tokens=True, is_tokenized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the pretrained BERT Chinese from Google is character-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BertClient with tf.data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_fp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-c2539610531c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m ds = (tf.data.TextLineDataset(train_fp).batch(batch_size)\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpy_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_encodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'feature'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_fp' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "batch_size = 256\n",
    "num_parallel_calls = 4\n",
    "num_clients = num_parallel_calls * 2  # should be at least greater than `num_parallel_calls`\n",
    "\n",
    "# start a pool of clients\n",
    "bc_clients = [BertClient(show_server_config=False) for _ in range(num_clients)]\n",
    "\n",
    "\n",
    "def get_encodes(x):\n",
    "    # x is `batch_size` of lines, each of which is a json object\n",
    "    samples = [json.loads(l) for l in x]\n",
    "    text = [s['raw_text'] for s in samples]  # List[List[str]]\n",
    "    labels = [s['label'] for s in samples]  # List[str]\n",
    "    # get a client from available clients\n",
    "    bc_client = bc_clients.pop()\n",
    "    features = bc_client.encode(text)\n",
    "    # after use, put it back\n",
    "    bc_clients.append(bc_client)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "ds = (tf.data.TextLineDataset(train_fp).batch(batch_size)\n",
    "        .map(lambda x: tf.py_func(get_encodes, [x], [tf.float32, tf.string]),  num_parallel_calls=num_parallel_calls)\n",
    "        .map(lambda x, y: {'feature': x, 'label': y})\n",
    "        .make_one_shot_iterator().get_next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a text classifier using BERT features and tf.estimator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DNNClassifier(\n",
    "    hidden_units=[512],\n",
    "    feature_columns=[tf.feature_column.numeric_column('feature', shape=(768,))],\n",
    "    n_classes=len(laws),\n",
    "    config=run_config,\n",
    "    label_vocabulary=laws_str,\n",
    "    dropout=0.1)\n",
    "\n",
    "input_fn = lambda fp: (tf.data.TextLineDataset(fp)\n",
    "                       .apply(tf.contrib.data.shuffle_and_repeat(buffer_size=10000))\n",
    "                       .batch(batch_size)\n",
    "                       .map(lambda x: tf.py_func(get_encodes, [x], [tf.float32, tf.string]), num_parallel_calls=num_parallel_calls)\n",
    "                       .map(lambda x, y: ({'feature': x}, y))\n",
    "                       .prefetch(20))\n",
    "\n",
    "train_spec = TrainSpec(input_fn=lambda: input_fn(train_fp))\n",
    "eval_spec = EvalSpec(input_fn=lambda: input_fn(eval_fp), throttle_secs=0)\n",
    "train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
